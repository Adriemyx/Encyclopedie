<h1 align='center'> Machine learning - Processus Gaussiens üßÆ </h1>

Les **processus gaussiens (GP)** sont une g√©n√©ralisation infinie des distributions gaussiennes pour des ensembles de donn√©es continus.
Ils sont principalement utilis√©s en r√©gression et en optimisation de fonctions. 
Un GP d√©finit une distribution sur des fonctions, permettant de faire des pr√©dictions probabilistes dans des probl√®mes non param√©triques.

Un processus gaussien est sp√©cifi√© par:
- Une fonction moyenne: $\mu(x)$, qui donne l'esp√©rance des sorties en un point.
- Une fonction de covariance (ou noyau): $k(x, x')$, qui encode la d√©pendance entre les points $x$ et $x'$.

Formellement: $f(x) \sim \mathcal{GP}(\mu(x), k(x, x'))$
<br>

## **I. Principes Fondamentaux**

### 1. **D√©finition:** Tout ensemble de points $\{x_1, ..., x_n\}$ tir√© d'un processus gaussien suit une distribution multivari√©e gaussienne: $f(x) = \mathcal{N}(\mu, K)$
o√π:
   - $\mu$ est un vecteur de moyennes,
   - $K$ est la matrice de covariance d√©finie par le noyau.

### 2. **Noyaux Courants:**
   - **RBF (Radial Basis Function)** ou noyau gaussien:
   $k(x, x') = \sigma_f^2 \exp\left(-\frac{\|x - x'\|^2}{2\ell^2}\right)$
   - **Lin√©aire:**
   $k(x, x') = x^\top x'$
   - **P√©riodique:**
   $k(x, x') = \sigma_f^2 \exp\left(-\frac{2\sin^2(\pi |x - x'|/p)}{\ell^2}\right)$

### 3. **R√©gression Gaussienne:**
   √âtant donn√© des observations $X = \{x_1, ..., x_n\}$ et leurs sorties $y = \{y_1, ..., y_n\}$, 
   le GP utilise les relations probabilistes pour pr√©dire les valeurs en des points non observ√©s $X^*$.

<br>

## **II. R√©gression Gaussienne**

Soit:
- Les observations: $\mathbf{y} \sim \mathcal{N}(\mathbf{\mu}, \mathbf{K} + \sigma^2 \mathbf{I})$,
- Les points de pr√©diction: $\mathbf{f}^{*} \sim \mathcal{N}(\mu, K_{*})$.

Les pr√©dictions sont donn√©es par: $\mathbf{f}^{*} \mid \mathbf{X}, \mathbf{y}, \mathbf{X}^{*} \sim \mathcal{N}(\mathbf{\mu}^{*}, \mathbf{\Sigma}^{*})$
o√π:
- Moyenne pr√©dictive: $\mu_* = K(X_*, X) [K(X, X) + \sigma^2 I]^{-1} y$
- Covariance pr√©dictive: $\mathbf{\Sigma}^{*} = K(X_{*}, X_{*}) - K(X_{*}, X) \big[K(X, X) + \sigma^2 I\big]^{-1} K(X, X_{*})$

---

Voici une impl√©mentation simple d'une r√©gression gaussienne avec un noyau RBF.

```python
# Importation des Librairies
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import cholesky, solve


# D√©finition du Noyau RBF
def rbf_kernel(X1, X2, length_scale=1.0, sigma_f=1.0):
    """
    Noyau RBF (Radial Basis Function).
    """
    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)
    return sigma_f**2 * np.exp(-0.5 / length_scale**2 * sqdist)

# R√©gression Gaussienne
class GaussianProcessRegressor:
    def __init__(self, kernel, noise=1e-5):
        self.kernel = kernel
        self.noise = noise

    def fit(self, X_train, y_train):
        self.X_train = X_train
        self.y_train = y_train
        K = self.kernel(X_train, X_train) + self.noise * np.eye(len(X_train))
        self.L = cholesky(K, lower=True)
        self.alpha = solve(self.L.T, solve(self.L, y_train))

    def predict(self, X_test):
        K_s = self.kernel(self.X_train, X_test)
        K_ss = self.kernel(X_test, X_test) + 1e-8 * np.eye(len(X_test))
        mean = K_s.T @ self.alpha
        v = solve(self.L, K_s)
        covariance = K_ss - v.T @ v
        return mean, covariance


# Exemple d'Utilisation:

# Donn√©es simul√©es
X_train = np.array([[-4.0], [-3.0], [-2.0], [0.0], [2.0]])
y_train = np.sin(X_train).ravel()

X_test = np.linspace(-5, 5, 100).reshape(-1, 1)

# Mod√®le GP
gp = GaussianProcessRegressor(kernel=lambda X1, X2: rbf_kernel(X1, X2, length_scale=1.0, sigma_f=1.0))
gp.fit(X_train, y_train)

# Pr√©diction
mean, cov = gp.predict(X_test)

# Intervalle de confiance
std_dev = np.sqrt(np.diag(cov))
lower_bound = mean - 1.96 * std_dev
upper_bound = mean + 1.96 * std_dev

# Visualisation
plt.figure(figsize=(10, 6))
plt.plot(X_train, y_train, 'ro', label="Donn√©es d'entra√Ænement")
plt.plot(X_test, mean, 'b-', label="Pr√©diction moyenne")
plt.fill_between(X_test.ravel(), lower_bound, upper_bound, color='blue', alpha=0.2, label="95% IC")
plt.legend()
plt.show()
```

<br>

## **III. Applications des Processus Gaussiens**

1. **R√©gression Non-Param√©trique:** Les GP offrent une m√©thode flexible pour mod√©liser des fonctions complexes.
2. **Optimisation Bay√©sienne:** Utilis√©s pour optimiser des fonctions co√ªteuses (e.g., ajustement d'hyperparam√®tres en ML).
3. **Mod√®les Spatiaux:** Appropri√©s pour les probl√®mes o√π les donn√©es d√©pendent fortement de leur position g√©ographique.

<br>
<br>

## **IV. Optimisation Bay√©sienne**
L‚Äô**optimisation bay√©sienne** est une m√©thode puissante pour optimiser des fonctions co√ªteuses √† √©valuer. 
Elle est couramment utilis√©e en machine learning, notamment pour l‚Äôajustement des hyperparam√®tres. 
Contrairement aux m√©thodes classiques comme la descente de gradient, elle convient pour des fonctions non convexes, non d√©rivables et bruit√©es.

Le principe central repose sur deux composants principaux:
1. **Mod√®le probabiliste (ex.: Processus Gaussien)**: Il mod√©lise la fonction cible.
2. **Fonction d‚Äôacquisition**: Elle guide le choix des points √† √©valuer.

<br>

### **1. Processus de l‚ÄôOptimisation Bay√©sienne**

1. **D√©finir la fonction cible** $f(x)$, souvent co√ªteuse √† √©valuer.
2. **Construire un mod√®le probabiliste** (souvent un GP) pour approximer $f(x)$.
3. **Optimiser la fonction d‚Äôacquisition** (comme l‚Äô*Expected Improvement* ou l‚Äô*UCB*) pour s√©lectionner le prochain point √† √©valuer.
4. **√âvaluer $f(x)$ au nouveau point** et mettre √† jour le mod√®le.
5. **R√©p√©ter** jusqu‚Äô√† convergence ou jusqu‚Äô√† atteindre un budget donn√© (nombre maximal d‚Äô√©valuations).

<br>

### **2. Fonction d‚ÄôAcquisition**

Les fonctions d‚Äôacquisition √©quilibrent l‚Äô**exploitation** (explorer les points o√π le mod√®le pr√©dit de bonnes performances) 
et l‚Äô**exploration** (explorer des zones incertaines).

- **Expected Improvement (EI)**:
$EI(x) = \mathbb{E}[\max(0, f(x) - f_{\text{best}})]$
  O√π $f_{\text{best}}$ est la meilleure valeur observ√©e jusqu‚Äô√† pr√©sent.

- **Upper Confidence Bound (UCB)**:
$UCB(x) = \mu(x) + \kappa \sigma(x)$
  O√π $\mu(x)$ est la moyenne pr√©dite, $\sigma(x)$ est l‚Äô√©cart-type pr√©dite, et $\kappa$ contr√¥le l‚Äôexploration.

<br>

### **3. Impl√©mentation**
#### **a. Minimisation d'une fonction math√©matique**
Voici une impl√©mentation simple de l‚Äôoptimisation bay√©sienne pour minimiser une fonction $f(x)$:
```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize

# D√©finir la Fonction Cible (Co√ªteuse √† √âvaluer):

# Fonction cible √† minimiser (exemple: sin(x) avec un minimum global)
def objective_function(x):
    return np.sin(3 * x) + 0.5 * x**2 + 0.2 * x


#Noyau RBF
def rbf_kernel(X1, X2, length_scale=1.0, sigma_f=1.0):
    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)
    return sigma_f**2 * np.exp(-0.5 / length_scale**2 * sqdist)

# R√©gression Gaussienne
class GaussianProcess:
    def __init__(self, kernel, noise=1e-6):
        self.kernel = kernel
        self.noise = noise

    def fit(self, X_train, y_train):
        self.X_train = X_train
        self.y_train = y_train
        K = self.kernel(X_train, X_train) + self.noise * np.eye(len(X_train))
        self.L = np.linalg.cholesky(K)
        self.alpha = np.linalg.solve(self.L.T, np.linalg.solve(self.L, y_train))

    def predict(self, X_test):
        K_s = self.kernel(self.X_train, X_test)
        K_ss = self.kernel(X_test, X_test) + 1e-8 * np.eye(len(X_test))
        mean = K_s.T @ self.alpha
        v = np.linalg.solve(self.L, K_s)
        covariance = K_ss - v.T @ v
        return mean.ravel(), np.sqrt(np.diag(covariance))
        
# Fonction d‚ÄôAcquisition (Expected Improvement)
def expected_improvement(X, gp, y_max, xi=0.01):
    mu, sigma = gp.predict(X)
    sigma = sigma.reshape(-1, 1)
    with np.errstate(divide='ignore'):
        Z = (mu - y_max - xi) / sigma
        ei = (mu - y_max - xi) * norm.cdf(Z) + sigma * norm.pdf(Z)
    return ei.ravel()
    
# Optimisation Bay√©sienne
def bayesian_optimization(objective, bounds, n_iters=10, gp_params={}):
    # Initialisation
    X_train = np.random.uniform(bounds[0], bounds[1], size=(5, 1))
    y_train = np.array([objective(x) for x in X_train]).ravel()
    
    # Mod√®le GP
    gp = GaussianProcess(kernel=lambda X1, X2: rbf_kernel(X1, X2, **gp_params))
    gp.fit(X_train, y_train)
    
    for _ in range(n_iters):
        # Maximiser la fonction d'acquisition
        def acquisition(X):
            return -expected_improvement(X.reshape(-1, 1), gp, y_train.max())
        
        res = minimize(acquisition, x0=np.random.uniform(bounds[0], bounds[1]), bounds=[bounds])
        x_next = res.x
        
        # √âvaluer la fonction cible
        y_next = objective(x_next)
        
        # Ajouter la nouvelle observation
        X_train = np.vstack((X_train, [[x_next]]))
        y_train = np.append(y_train, y_next)
        
        # Mettre √† jour le mod√®le GP
        gp.fit(X_train, y_train)
        
    return X_train, y_train


# Exemple d‚ÄôApplication
from scipy.stats import norm

# Optimiser la fonction cible
bounds = (-2, 2)
X_train, y_train = bayesian_optimization(objective_function, bounds, n_iters=15, gp_params={"length_scale": 0.5, "sigma_f": 1.0})

# Visualiser les r√©sultats
X_test = np.linspace(bounds[0], bounds[1], 100).reshape(-1, 1)
y_test = objective_function(X_test)

plt.figure(figsize=(12, 6))
plt.plot(X_test, y_test, 'r--', label="Fonction cible")
plt.plot(X_train, y_train, 'bo', label="√âchantillons")
plt.legend()
plt.title("Optimisation Bay√©sienne")
plt.show()
```

<br>

#### **b. Optimisation des hyperparam√®tres**
Voici un exemple o√π l‚Äôoptimisation bay√©sienne est utilis√©e pour ajuster les hyperparam√®tres d‚Äôun mod√®le machine learning, comme une **r√©gression Ridge**.   
Exemple: Optimisation de l'Hyperparam√®tre $\alpha$ d'une R√©gression Ridge

```python
# Importation des Librairies
from sklearn.linear_model import Ridge
from sklearn.datasets import make_regression
from sklearn.model_selection import cross_val_score
import numpy as np

# Fonction Cible: Validation Crois√©e

# Donn√©es simul√©es
X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)

# Fonction cible: erreur quadratique moyenne en validation crois√©e
def objective_function(alpha):
    model = Ridge(alpha=alpha)
    # Validation crois√©e (Moyenne des erreurs n√©gatives pour avoir une "perte positive")
    score = -np.mean(cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error'))
    return score


# Application de l‚ÄôOptimisation Bay√©sienne
bounds = (1e-3, 10)  # Recherche d'alpha entre 0.001 et 10
X_train, y_train = bayesian_optimization(objective_function, bounds, n_iters=20, gp_params={"length_scale": 0.5, "sigma_f": 1.0})

# Meilleur hyperparam√®tre trouv√©
best_alpha = X_train[np.argmin(y_train)]
print(f"Meilleur alpha trouv√©: {best_alpha}")

# Visualisation des R√©sultats
plt.figure(figsize=(10, 6))
plt.plot(X_train, y_train, 'bo-', label="Erreurs observ√©es")
plt.xlabel("Valeurs d'alpha")
plt.ylabel("Erreur quadratique moyenne (MSE)")
plt.title("Optimisation Bay√©sienne des Hyperparam√®tres")
plt.legend()
plt.show()
```

En utilisant l‚Äôoptimisation bay√©sienne, la m√©thode explore intelligemment l‚Äôespace des hyperparam√®tres pour trouver la meilleure valeur de $\alpha$ 
pour la r√©gression Ridge, tout en minimisant le nombre d‚Äô√©valuations co√ªteuses via la validation crois√©e.


<br>
<br>

## **V. Applications de l‚ÄôOptimisation Bay√©sienne**

1. **Ajustement d‚ÄôHyperparam√®tres:**
   - Recherche d‚Äôhyperparam√®tres dans les mod√®les de machine learning (e.g., SVM, r√©seaux de neurones).
2. **Conception d‚ÄôExp√©riences:**
   - Optimiser les param√®tres dans des exp√©riences physiques ou biologiques.
3. **Optimisation de Fonctions Co√ªteuses:**
   - Exemples: simulations, algorithmes d‚ÄôIA, tests logiciels.
