<h1 align='center'> Machine learning - Classification üóÇÔ∏è</h1>

Dans ce document sera pr√©sent√© quelques bases de code, notamment avec la libraire `scikit-learn`, pour faire de la classification sur python.


## SVM
SVM est un algorithme de classification supervis√©e pour s√©parer deux classes en maximisant la **marge** entre les deux points les plus proches de l'hyperplan s√©parateur.


```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation du mod√®le SVM avec un noyau lin√©aire
svm_model = SVC(kernel='linear')  # noyaux: lin√©aire, polynomial, gaussien (RBF), sigmo√Ød...

# Entra√Ænement du mod√®le
svm_model.fit(X_train, y_train)

# Pr√©diction
y_pred = svm_model.predict(X_test)

# Affichage des r√©sultats
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, marker='o', edgecolor='k')
plt.title("SVM Classification")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```


## Classificateur Bayesien na√Øf 
La classification na√Øve bay√©sienne est un type de classification bay√©sienne probabiliste simple bas√©e sur le th√©or√®me de Bayes avec une forte **ind√©pendance** des hypoth√®ses. Elle met en ≈ìuvre un classifieur bay√©sien na√Øf, ou classifieur na√Øf de Bayes, appartenant √† la famille des classifieurs lin√©aires.


```python
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import numpy as np

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation du mod√®le Bayes na√Øf
gnb = GaussianNB()

# Entra√Ænement du mod√®le
gnb.fit(X_train, y_train)

# Pr√©diction
y_pred = gnb.predict(X_test)

# Affichage des r√©sultats
print(f"Prediction: {y_pred})")

# Affiche les probabilit√©s pr√©dites par le mod√®le pour chaque classe pour les donn√©es de test
print(f"Probas: {gnb.predict_proba(X_test)}")

# Affiche les log-probabilit√©s (log vraisemblance) pour chaque classe.
print(f"Log probas: {gnb.predict_log_proba(X_test)}")


# Affichage des performances
print(f"Generalization error: {np.sum(np.not_equal(y_pred, y_test))/len(y_test)}")
print(f"Generalization score: {digits_nbc.score(X_test, y_test)}")
print(f"Confusion matrix: {confusion_matrix(y_test, y_pred)}")
```

<br>
<br>

## Arbres de d√©cision

Un arbre de d√©cisions est un algorithme d'apprentissage supervis√© non param√©trique, utilis√© √† la fois pour les t√¢ches de classification et de r√©gression. Il poss√®de une structure hi√©rarchique et arborescente, qui se compose d'un n≈ìud racine, de branches, de n≈ìuds internes et de n≈ìuds feuille.


```python
from sklearn import tree
from sklearn.model_selection import train_test_split

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation de l'arbre
dt = tree.DecisionTreeClassifier(criterion='entropy',max_depth=10, min_samples_leaf=10)

# Entra√Ænement du mod√®le
dt.fit(X_train, y_train)

# Affichage de l'arbre
tree.plot_tree(dt)
```

Il est √† noter que dans l'instanciation de l'arbre plusieurs param√®tres sont donn√©s:
1. **`criterion`**: C'est le **crit√®re de division** qui sp√©cifie la fonction de mesure de la qualit√© d'un d√©coupage dans l'arbre de d√©cision.   
   Lorsque `criterion='entropy'`, cela signifie que l'arbre utilisera l'**entropie** comme crit√®re pour d√©cider comment diviser les donn√©es √† chaque n≈ìud: L'entropie est une mesure de l'incertitude ou du d√©sordre dans les donn√©es. Plus pr√©cis√©ment, elle est utilis√©e pour calculer la *gain d'information* √† chaque division, et l'arbre choisit la division qui maximise ce gain d'information.   
   L'autre option courante pour `criterion` est `gini`, qui utilise l'**indice de Gini** comme crit√®re de d√©cision. 

2. **`max_depth`**: C'est la **Profondeur maximale de l'arbre** qui d√©finit le nombre maximal de niveaux de n≈ìuds dans l'arbre, √† partir de la racine jusqu'aux feuilles (les n≈ìuds finaux o√π des pr√©dictions sont faites).   
   Limiter la profondeur d'un arbre est une mani√®re courante de pr√©venir le **sur-apprentissage** (overfitting) en r√©duisant la complexit√© du mod√®le. Un arbre trop profond pourrait m√©moriser trop les donn√©es d'entra√Ænement, ce qui peut nuire √† sa capacit√© √† g√©n√©raliser √† de nouvelles donn√©es.

3. **`min_samples_leaf`**: C'est le **nombre minimum d'√©chantillons par feuille**. Ce param√®tre permet de contr√¥ler la taille des feuilles de l'arbre et peut aider √† √©viter des feuilles trop petites, ce qui peut r√©duire le sur-apprentissage.   
   Ce param√®tre a une influence sur la structure de l'arbre. Si cette valeur est augment√©e, un arbre plus "large" et moins "profond" sera obtenu. Cela peut √©galement rendre le mod√®le plus robuste et moins susceptible de s'adapter aux bruits ou aux variations sp√©cifiques dans les donn√©es d'entra√Ænement.




Pour afficher les fronti√®res de d√©cision:
```python
def plot_decision_boundary_tree(t, X, y, fig_size=(22, 8)):
    plot_step = 0.02
    x0_min, x0_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, plot_step), np.arange(x1_min, x1_max, plot_step))
    yypred = t.predict(np.c_[xx0.ravel(),xx1.ravel()])
    yypred = yypred.reshape(xx0.shape)
    plt.figure(figsize=fig_size)
    plt.contourf(xx0, xx1, yypred, cmap=plt.cm.Paired)
    y_pred = t.predict(X)
    Xblue_good = X[np.equal(y,-1)*np.equal(y,y_pred)]
    Xblue_bad  = X[np.equal(y,-1)*np.not_equal(y,y_pred)]
    Xred_good  = X[np.equal(y,1)*np.equal(y,y_pred)]
    Xred_bad   = X[np.equal(y,1)*np.not_equal(y,y_pred)]
    plt.scatter(Xblue_good[:,0],Xblue_good[:,1],c='b')
    plt.scatter(Xblue_bad[:,0],Xblue_bad[:,1],c='c',marker='v')
    plt.scatter(Xred_good[:,0],Xred_good[:,1],c='r')
    plt.scatter(Xred_bad[:,0],Xred_bad[:,1],c='m',marker='v')
    plt.show()

plot_decision_boundary_tree(dt, X, y)
```

## For√™ts al√©atoires 

Les **for√™ts al√©atoires** (*Random Forest*) sont un algorithme d'**ensemble learning** dont l'id√©e est de combiner plusieurs arbres de d√©cision pour obtenir une pr√©diction plus robuste et plus pr√©cise. En particulier, l'algorithme construit une **for√™t** d'arbres de d√©cision en introduisant de l'**al√©atoire** √† deux niveaux:
1. **√âchantillonnage bootstrap** des donn√©es (sub-sampling des donn√©es d'entra√Ænement).
2. **S√©lection al√©atoire des caract√©ristiques** √† chaque division (split) dans chaque arbre.

L'algorithme des for√™ts al√©atoires fonctionne en plusieurs √©tapes:

#### 1. **Cr√©ation d'arbres de d√©cision al√©atoires**
   - L'id√©e de base est de cr√©er un ensemble (ou une "for√™t") de plusieurs **arbres de d√©cision**, chacun √©tant construit √† partir d'un sous-ensemble al√©atoire des donn√©es d'entra√Ænement.
   - L'**√©chantillonnage bootstrap** est utilis√© pour cr√©er des sous-ensembles de donn√©es. Cela signifie que pour chaque arbre, on tire al√©atoirement des √©chantillons de l'ensemble d'entra√Ænement, avec remplacement (certains exemples peuvent √™tre r√©p√©t√©s, d'autres non).
   
#### 2. **S√©lection al√©atoire des caract√©ristiques**
   - √Ä chaque n≈ìud d'un arbre de d√©cision, au lieu d'examiner toutes les caract√©ristiques pour d√©cider de la meilleure division, un sous-ensemble al√©atoire des caract√©ristiques est choisi et utilis√© pour effectuer la division √† ce n≈ìud.
   - Cela permet d'introduire encore plus de diversit√© entre les arbres et de r√©duire la corr√©lation entre eux, ce qui am√©liore la performance du mod√®le global.

#### 3. **Entra√Ænement des arbres**  
   - Chaque arbre est entra√Æn√© sur un sous-ensemble diff√©rent de donn√©es (gr√¢ce au bootstrap) et en utilisant un sous-ensemble al√©atoire de caract√©ristiques √† chaque division. 
   - Cela signifie que chaque arbre peut avoir des structures l√©g√®rement diff√©rentes, ce qui aide √† r√©duire le sur-apprentissage (overfitting) lorsque ces arbres sont combin√©s.

#### 4. **Pr√©diction finale**
   - Une fois que tous les arbres ont √©t√© entra√Æn√©s, la pr√©diction de la for√™t est effectu√©e par **vote majoritaire** (pour la classification) ou **moyenne** (pour la r√©gression).   
  Chaque arbre "vote" pour une classe, et la classe ayant le plus grand nombre de votes devient la pr√©diction finale.

### Avantages des For√™ts Al√©atoires:
1. **Robustesse**
2. **R√©duction du sur-apprentissage**
3. **Capacit√© √† g√©rer des donn√©es complexes**

### Inconv√©nients des For√™ts Al√©atoires:
1. **Complexit√© computationnelle**
2. **Moins interpr√©table qu'un arbre de d√©cision simple**


```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©er une for√™t al√©atoire
rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

# Entra√Æner la for√™t al√©atoire
rf.fit(X_train, y_train)

# Pr√©dire les labels pour les donn√©es de test
y_pred = rf.predict(X_test)

# Calculer l'exactitude
accuracy = (y_pred == y_test).mean()
print(f"Accuracy: {accuracy}")

# Importance des features
importances = rf.feature_importances_
std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(22, 8))
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices], color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()
```

Voici quelques hyperparam√®tres importants que √† ajuster pour am√©liorer la performance du mod√®le de for√™t al√©atoire:
- **`n_estimators`**: Le nombre d'arbres de d√©cision dans la for√™t. Plus il y a d'arbres, plus la pr√©diction sera robuste, mais cela augmente √©galement le co√ªt computationnel.
- **`max_features`**: Le nombre maximal de caract√©ristiques √† consid√©rer pour chaque division. Cela contr√¥le le degr√© de diversit√© entre les arbres de la for√™t.
- **`bootstrap`**: Si l'√©chantillonnage bootstrap est utilis√© pour la cr√©ation des sous-ensembles de donn√©es (par d√©faut, c'est vrai).
- **`oob_score`**: Le score Out-Of-Bag (OOB), qui permet d'estimer la performance du mod√®le sans utiliser un ensemble de validation s√©par√©.


<br>
<br>


## Boosting
### AdaBoost 
AdaBoost (pour Adaptive Boosting) est un algorithme d'ensemble learning qui combine plusieurs classificateurs faibles (c'est-√†-dire des mod√®les qui, seuls, n'ont pas une grande performance) pour cr√©er un classificateur plus puissant. Il fonctionne en "boostant" progressivement la performance des mod√®les en les combinant de mani√®re adaptative.

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation du mod√®le
boosted_forest = AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy',max_depth=3), n_estimators=100)

# Entra√Ænement du mod√®le
boosted_forest.fit(X_train, y_train)

# Affichage des fronti√®res de d√©cision
plot_decision_boundary(boosted_forest, X_train,  y_train)

# Affichage des m√©triques
print(f"Training score: {boosted_forest.score(X_train, y_train)}")
print(f"Testing score: {boosted_forest.score(X_test, y_test)}")
```

*<u>Remarque</u>: Le param√®tre **`n_estimators`** fait r√©f√©rence au **nombre d'estimateurs faibles** (classificateurs) que l'algorithme va entra√Æner. Il sp√©cifie combien de classificateurs faibles l'algorithme AdaBoost va entra√Æner dans le cadre de la proc√©dure de boosting.*


### Gradient Boost
Le **Gradient Boosting** est une technique d'apprentissage supervis√© utilis√©e pour construire des mod√®les pr√©dictifs en combinant plusieurs mod√®les faibles (souvent des arbres de d√©cision). Elle repose sur l'id√©e de corriger, √† chaque √©tape, les erreurs r√©siduelles du mod√®le pr√©c√©dent. Ces corrections s'appuient sur le gradient de la fonction de perte, qui indique la direction dans laquelle am√©liorer le mod√®le. Le Gradient Boosting peut utiliser n'importe quelle fonction de perte diff√©rentiable, comme l'erreur quadratique (pour la r√©gression) ou l'entropie crois√©e (pour la classification). Au fil des it√©rations, le mod√®le devient de plus en plus pr√©cis.:


- On cherche une fonction $\hat{f}$ dans un espace de fonctions $\mathcal{H}$ qui minimise une fonction de perte $L(f(x), y)$. Cette fonction de perte mesure √† quel point notre mod√®le $f$ pr√©dit correctement $y$ √† partir de $x$.
- Formul√© math√©matiquement:
  $\hat{f} = \arg\min_{f \in \mathcal{H}} \mathbb{E}_{x, y} \left[ L(f(x), y) \right]$
- $\mathbb{E}_{x, y}$ repr√©sente l'esp√©rance par rapport √† la distribution des donn√©es $(x, y)$, ce qui revient √† minimiser l'erreur moyenne sur les donn√©es.

<br>

#### **1. Approche par √©tapes (it√©rative)**
- Il n'est pas possible de trouver $\hat{f}$ directement, car l'espace des fonctions $\mathcal{H}$ est tr√®s vaste et la solution analytique est souvent impossible.
- L'id√©e est de construire $\hat{f}$ progressivement, par √©tapes, en ajoutant petit √† petit des "corrections" √† une fonction initiale $f_0$. 
- √Ä l'√©tape $k$, le mod√®le est:
  $f_k = f_{k-1} + \alpha_k h_k$,
  o√π:
  - $f_{k-1}$ est le mod√®le courant.
  - $h_k$ est une fonction "correction" qui doit r√©duire l'erreur.
  - $\alpha_k$ est un facteur d'√©chelle trouv√© par optimisation.

<br>

#### **2. Direction de la correction**
- La fonction $h_k$ est choisie pour **pointer dans la direction qui r√©duit le plus rapidement la perte**. Cela revient √† suivre le **gradient de la perte** par rapport au mod√®le $f$:   
$h_k = \mathbb{E}_{x, y} \left[ \nabla_f L(f_{k-1}(x), y) \right]$
- Intuitivement, $h_k$ "montre" la direction dans laquelle on doit ajuster le mod√®le $f_{k-1}$ pour r√©duire l'erreur.

<br>

#### **3. R√¥le de $ \alpha_k $**
- Une fois $h_k$ trouv√©, on doit d√©terminer combien de cette correction ajouter. C'est fait via une recherche lin√©aire:
  $\alpha_k = \arg\min_\alpha \mathbb{E}_{x, y} \left[ L(f_{k-1}(x) + \alpha h_k(x), y) \right]$
- Cela garantit qu'on ajoute $h_k$ avec la bonne "intensit√©" pour minimiser la perte.

<br>

```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation du mod√®le
gtb = GradientBoostingClassifier(n_estimators=100)

# Entra√Ænement du mod√®le
gtb.fit(X_train, y_train)

# Affichage des fronti√®res de d√©cision
plot_decision_boundary(gtb, X_train,  y_train)

# Affichage des m√©triques
print(f"Training score: {gtb.score(X_train, y_train)}")
print(f"Testing score: {gtb.score(X_test, y_test)}")
```

<br>

#### **Applications:**
- **XGBoost**, **LightGBM**, et **CatBoost** sont des impl√©mentations tr√®s performantes du Gradient Boosting. Elles sont largement utilis√©es pour des probl√®mes de r√©gression, de classification et de ranking.



<br>

### XGBoost
**XGBoost** (Extreme Gradient Boosting) est une impl√©mentation avanc√©e de l'algorithme de **Gradient Boosting**. Il est con√ßu pour √™tre rapide, efficace et hautement performant.

#### **Avantages de XGBoost:**
1. **Optimisation du calcul**: Utilise la parall√©lisation et des techniques comme la r√©gularisation.
2. **Flexibilit√©**: Permet de travailler avec diff√©rentes fonctions de perte (log-loss, erreur quadratique, etc.).
3. **Gestion des donn√©es manquantes**: Prend en charge automatiquement les donn√©es manquantes.
4. **R√©gularisation int√©gr√©e**: Inclut $\mathcal{L}_1$ et $\mathcal{L}_2$ pour √©viter le surapprentissage.
5. **Support pour les grandes donn√©es**: Efficace avec des ensembles de donn√©es volumineux.

---

Lorsqu'il est appliqu√© √† un probl√®me de classification, XGBoost utilise une **fonction de perte logarithmique** pour √©valuer les pr√©dictions:

```python
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Chargement des donn√©es
X, y = data.data, data.target

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation de la structure DMatrix pour XGBoost
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# D√©finition des hyperparam√®tres pour un probl√®me de classification binaire
params = {
    'objective': 'binary:logistic',  # Fonction de perte pour la classification binaire
    'max_depth': 4,                 # Profondeur maximale des arbres
    'eta': 0.1,                     # Taux d'apprentissage
    'eval_metric': 'logloss',       # M√©trique √† optimiser
    'seed': 42                      # Pour la reproductibilit√©
}

# Entra√Ænement du mod√®le
evallist = [(dtrain, 'train'), (dtest, 'eval')]
model = xgb.train(params, dtrain, num_boost_round=100, evals=evallist, early_stopping_rounds=10)

# Pr√©dictions sur les donn√©es de test
y_pred_proba = model.predict(dtest)
y_pred = (y_pred_proba > 0.5).astype(int)  # Conversion des probabilit√©s en classes

# √âvaluation du mod√®le
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))
```


#### **Param√®tres importants:**
   - **`objective`**: Sp√©cifie la t√¢che √† effectuer. Ici, c'est une classification binaire (`binary:logistic`).
   - **`max_depth`**: Contr√¥le la complexit√© des arbres (√©vite le surapprentissage).
   - **`eta`**: Le taux d'apprentissage ($\eta$) r√©gule la contribution de chaque arbre.
   - **`eval_metric`**: La m√©trique √† optimiser. Pour la classification, c'est souvent la `logloss` ou l'`error`.

3. **`early_stopping_rounds`**:
   - Stoppe l'entra√Ænement si la performance ne s'am√©liore plus apr√®s un certain nombre de rounds.


#### **Tuning des hyperparam√®tres**
Pour am√©liorer les performances, il est possible d'optimiser les hyperparam√®tres de XGBoost, comme:
- **`n_estimators`**: Nombre d'arbres.
- **`learning_rate`** (alias `eta`): R√©duit le taux d'apprentissage.
- **`subsample`**: Fraction des donn√©es utilis√©es pour chaque arbre (r√©duction de la variance).
- **`colsample_bytree`**: Fraction des features utilis√©es pour chaque arbre.

Il est possible d'utiliser **GridSearchCV** ou **Optuna** pour optimiser ces param√®tres.

---

#### **Affichage des arbres de d√©cision**
XGBoost permet √©galement de visualiser les arbres g√©n√©r√©s:

```python
import matplotlib.pyplot as plt
xgb.plot_tree(model, num_trees=0)  # Visualiser le premier arbre
plt.show()
```