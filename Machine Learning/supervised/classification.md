<h1 align='center'> Machine learning - Classification üóÇÔ∏è</h1>

Dans ce document sera pr√©sent√© quelques bases de code, notamment avec la libraire `scikit-learn`, pour faire de la classification sur python.


## K-Means
K-means est un algorithme de clustering non supervis√© utilis√© pour partitionner un ensemble de donn√©es en **K** groupes (ou clusters) bas√©s sur des caract√©ristiques similaires. 

```python
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split

# S√©paration 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Choix du nombre de classes pour s√©parer les donn√©es
nb_clusters = ...

# Cr√©ation d'un objet KMeans avec nb_clusters clusters
kmeans = KMeans(n_clusters=nb_clusters)  

# Ajustement du mod√®le sur les donn√©es d'entra√Ænement
kmeans.fit(X_train)  

# Pr√©diction des clusters pour chaque point de X_train
y_pred = kmeans.predict(X_train)

```

*<u>Remarque:</u> Ici $\mathcal{X}$ peut √™tre une matrice ou un vecteur*.



## SVM
SVM est un algorithme de classification supervis√©e pour s√©parer deux classes en maximisant la **marge** entre les deux points les plus proches de l'hyperplan s√©parateur.


```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation du mod√®le SVM avec un noyau lin√©aire
svm_model = SVC(kernel='linear')  # noyaux: lin√©aire, polynomial, gaussien (RBF), sigmo√Ød...

# Entra√Ænement du mod√®le
svm_model.fit(X_train, y_train)

# Pr√©diction
y_pred = svm_model.predict(X_test)

# Affichage des r√©sultats
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred, marker='o', edgecolor='k')
plt.title("SVM Classification")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()
```


## Classificateur Bayesien na√Øf 
La classification na√Øve bay√©sienne est un type de classification bay√©sienne probabiliste simple bas√©e sur le th√©or√®me de Bayes avec une forte **ind√©pendance** des hypoth√®ses. Elle met en ≈ìuvre un classifieur bay√©sien na√Øf, ou classifieur na√Øf de Bayes, appartenant √† la famille des classifieurs lin√©aires.


```python
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import numpy as np

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation du mod√®le Bayes na√Øf
gnb = GaussianNB()

# Entra√Ænement du mod√®le
gnb.fit(X_train, y_train)

# Pr√©diction
y_pred = gnb.predict(X_test)

# Affichage des r√©sultats
print(f"Prediction: {y_pred})")

# Affiche les probabilit√©s pr√©dites par le mod√®le pour chaque classe pour les donn√©es de test
print(f"Probas: {gnb.predict_proba(X_test)}")

# Affiche les log-probabilit√©s (log vraisemblance) pour chaque classe.
print(f"Log probas: {gnb.predict_log_proba(X_test)}")


# Affichage des performances
print(f"Generalization error: {np.sum(np.not_equal(y_pred, y_test))/len(y_test)}")
print(f"Generalization score: {digits_nbc.score(X_test, y_test)}")
print(f"Confusion matrix: {confusion_matrix(y_test, y_pred)}")
```

<br>
<br>

## Arbres de d√©cision

Un arbre de d√©cisions est un algorithme d'apprentissage supervis√© non param√©trique, utilis√© √† la fois pour les t√¢ches de classification et de r√©gression. Il poss√®de une structure hi√©rarchique et arborescente, qui se compose d'un n≈ìud racine, de branches, de n≈ìuds internes et de n≈ìuds feuille.


```python
from sklearn import tree
from sklearn.model_selection import train_test_split

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation de l'arbre
dt = tree.DecisionTreeClassifier(criterion='entropy',max_depth=10, min_samples_leaf=10)

# Entra√Ænement du mod√®le
dt.fit(X_train, y_train)

# Affichage de l'arbre
tree.plot_tree(dt)
```

Il est √† noter que dans l'instanciation de l'arbre plusieurs param√®tres sont donn√©s:
1. **`criterion`**: C'est le **crit√®re de division** qui sp√©cifie la fonction de mesure de la qualit√© d'un d√©coupage dans l'arbre de d√©cision.   
   Lorsque `criterion='entropy'`, cela signifie que l'arbre utilisera l'**entropie** comme crit√®re pour d√©cider comment diviser les donn√©es √† chaque n≈ìud: L'entropie est une mesure de l'incertitude ou du d√©sordre dans les donn√©es. Plus pr√©cis√©ment, elle est utilis√©e pour calculer la *gain d'information* √† chaque division, et l'arbre choisit la division qui maximise ce gain d'information.   
   L'autre option courante pour `criterion` est `gini`, qui utilise l'**indice de Gini** comme crit√®re de d√©cision. 

2. **`max_depth`**: C'est la **Profondeur maximale de l'arbre** qui d√©finit le nombre maximal de niveaux de n≈ìuds dans l'arbre, √† partir de la racine jusqu'aux feuilles (les n≈ìuds finaux o√π des pr√©dictions sont faites).   
   Limiter la profondeur d'un arbre est une mani√®re courante de pr√©venir le **sur-apprentissage** (overfitting) en r√©duisant la complexit√© du mod√®le. Un arbre trop profond pourrait m√©moriser trop les donn√©es d'entra√Ænement, ce qui peut nuire √† sa capacit√© √† g√©n√©raliser √† de nouvelles donn√©es.

3. **`min_samples_leaf`**: C'est le **nombre minimum d'√©chantillons par feuille**. Ce param√®tre permet de contr√¥ler la taille des feuilles de l'arbre et peut aider √† √©viter des feuilles trop petites, ce qui peut r√©duire le sur-apprentissage.   
   Ce param√®tre a une influence sur la structure de l'arbre. Si cette valeur est augment√©e, un arbre plus "large" et moins "profond" sera obtenu. Cela peut √©galement rendre le mod√®le plus robuste et moins susceptible de s'adapter aux bruits ou aux variations sp√©cifiques dans les donn√©es d'entra√Ænement.




Pour afficher les fronti√®res de d√©cision:
```python
def plot_decision_boundary_tree(t, X, y, fig_size=(22, 8)):
    plot_step = 0.02
    x0_min, x0_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, plot_step), np.arange(x1_min, x1_max, plot_step))
    yypred = t.predict(np.c_[xx0.ravel(),xx1.ravel()])
    yypred = yypred.reshape(xx0.shape)
    plt.figure(figsize=fig_size)
    plt.contourf(xx0, xx1, yypred, cmap=plt.cm.Paired)
    y_pred = t.predict(X)
    Xblue_good = X[np.equal(y,-1)*np.equal(y,y_pred)]
    Xblue_bad  = X[np.equal(y,-1)*np.not_equal(y,y_pred)]
    Xred_good  = X[np.equal(y,1)*np.equal(y,y_pred)]
    Xred_bad   = X[np.equal(y,1)*np.not_equal(y,y_pred)]
    plt.scatter(Xblue_good[:,0],Xblue_good[:,1],c='b')
    plt.scatter(Xblue_bad[:,0],Xblue_bad[:,1],c='c',marker='v')
    plt.scatter(Xred_good[:,0],Xred_good[:,1],c='r')
    plt.scatter(Xred_bad[:,0],Xred_bad[:,1],c='m',marker='v')
    plt.show()

plot_decision_boundary_tree(dt, X, y)
```

## For√™ts al√©atoires 

Les **for√™ts al√©atoires** (*Random Forest*) sont un algorithme d'**ensemble learning** dont l'id√©e est de combiner plusieurs arbres de d√©cision pour obtenir une pr√©diction plus robuste et plus pr√©cise. En particulier, l'algorithme construit une **for√™t** d'arbres de d√©cision en introduisant de l'**al√©atoire** √† deux niveaux:
1. **√âchantillonnage bootstrap** des donn√©es (sub-sampling des donn√©es d'entra√Ænement).
2. **S√©lection al√©atoire des caract√©ristiques** √† chaque division (split) dans chaque arbre.

L'algorithme des for√™ts al√©atoires fonctionne en plusieurs √©tapes:

#### 1. **Cr√©ation d'arbres de d√©cision al√©atoires**
   - L'id√©e de base est de cr√©er un ensemble (ou une "for√™t") de plusieurs **arbres de d√©cision**, chacun √©tant construit √† partir d'un sous-ensemble al√©atoire des donn√©es d'entra√Ænement.
   - L'**√©chantillonnage bootstrap** est utilis√© pour cr√©er des sous-ensembles de donn√©es. Cela signifie que pour chaque arbre, on tire al√©atoirement des √©chantillons de l'ensemble d'entra√Ænement, avec remplacement (certains exemples peuvent √™tre r√©p√©t√©s, d'autres non).
   
#### 2. **S√©lection al√©atoire des caract√©ristiques**
   - √Ä chaque n≈ìud d'un arbre de d√©cision, au lieu d'examiner toutes les caract√©ristiques pour d√©cider de la meilleure division, un sous-ensemble al√©atoire des caract√©ristiques est choisi et utilis√© pour effectuer la division √† ce n≈ìud.
   - Cela permet d'introduire encore plus de diversit√© entre les arbres et de r√©duire la corr√©lation entre eux, ce qui am√©liore la performance du mod√®le global.

#### 3. **Entra√Ænement des arbres**  
   - Chaque arbre est entra√Æn√© sur un sous-ensemble diff√©rent de donn√©es (gr√¢ce au bootstrap) et en utilisant un sous-ensemble al√©atoire de caract√©ristiques √† chaque division. 
   - Cela signifie que chaque arbre peut avoir des structures l√©g√®rement diff√©rentes, ce qui aide √† r√©duire le sur-apprentissage (overfitting) lorsque ces arbres sont combin√©s.

#### 4. **Pr√©diction finale**
   - Une fois que tous les arbres ont √©t√© entra√Æn√©s, la pr√©diction de la for√™t est effectu√©e par **vote majoritaire** (pour la classification) ou **moyenne** (pour la r√©gression).   
  Chaque arbre "vote" pour une classe, et la classe ayant le plus grand nombre de votes devient la pr√©diction finale.

### Avantages des For√™ts Al√©atoires:
1. **Robustesse**
2. **R√©duction du sur-apprentissage**
3. **Capacit√© √† g√©rer des donn√©es complexes**

### Inconv√©nients des For√™ts Al√©atoires:
1. **Complexit√© computationnelle**
2. **Moins interpr√©table qu'un arbre de d√©cision simple**


```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©er une for√™t al√©atoire
rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

# Entra√Æner la for√™t al√©atoire
rf.fit(X_train, y_train)

# Pr√©dire les labels pour les donn√©es de test
y_pred = rf.predict(X_test)

# Calculer l'exactitude
accuracy = (y_pred == y_test).mean()
print(f"Accuracy: {accuracy}")

# Importance des features
importances = rf.feature_importances_
std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(22, 8))
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices], color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()
```

Voici quelques hyperparam√®tres importants que √† ajuster pour am√©liorer la performance du mod√®le de for√™t al√©atoire:
- **`n_estimators`**: Le nombre d'arbres de d√©cision dans la for√™t. Plus il y a d'arbres, plus la pr√©diction sera robuste, mais cela augmente √©galement le co√ªt computationnel.
- **`max_features`**: Le nombre maximal de caract√©ristiques √† consid√©rer pour chaque division. Cela contr√¥le le degr√© de diversit√© entre les arbres de la for√™t.
- **`bootstrap`**: Si l'√©chantillonnage bootstrap est utilis√© pour la cr√©ation des sous-ensembles de donn√©es (par d√©faut, c'est vrai).
- **`oob_score`**: Le score Out-Of-Bag (OOB), qui permet d'estimer la performance du mod√®le sans utiliser un ensemble de validation s√©par√©.


<br>
<br>


## Boosting
### AdaBoost 
AdaBoost (pour Adaptive Boosting) est un algorithme d'ensemble learning qui combine plusieurs classificateurs faibles (c'est-√†-dire des mod√®les qui, seuls, n'ont pas une grande performance) pour cr√©er un classificateur plus puissant. Il fonctionne en "boostant" progressivement la performance des mod√®les en les combinant de mani√®re adaptative.

```python
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation du mod√®le
boosted_forest = AdaBoostClassifier(tree.DecisionTreeClassifier(criterion='entropy',max_depth=3), n_estimators=100)

# Entra√Ænement du mod√®le
boosted_forest.fit(X_train, y_train)

# Affichage des fronti√®res de d√©cision
plot_decision_boundary(boosted_forest, X_train,  y_train)

# Affichage des m√©triques
print(f"Training score: {boosted_forest.score(X_train, y_train)}")
print(f"Testing score: {boosted_forest.score(X_test, y_test)}")
```

*<u>Remarque</u>: Le param√®tre **`n_estimators`** fait r√©f√©rence au **nombre d'estimateurs faibles** (classificateurs) que l'algorithme va entra√Æner. Il sp√©cifie combien de classificateurs faibles l'algorithme AdaBoost va entra√Æner dans le cadre de la proc√©dure de boosting.*


### Gradient Boost

```python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation du mod√®le
gtb = GradientBoostingClassifier(n_estimators=100)

# Entra√Ænement du mod√®le
gtb.fit(X_train, y_train)

# Affichage des fronti√®res de d√©cision
plot_decision_boundary(gtb, X_train,  y_train)

# Affichage des m√©triques
print(f"Training score: {gtb.score(X_train, y_train)}")
print(f"Testing score: {gtb.score(X_test, y_test)}")
```