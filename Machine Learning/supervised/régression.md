<h1 align='center'> Machine learning - R√©gression üìà</h1>

Dans ce document sera pr√©sent√© quelques bases de code, notamment avec la libraire `scikit-learn`, pour faire de la r√©gression sur python.

## R√©gression lin√©aire 
A utiliser dans le cas d'un probl√®me **supervis√©** avec un label **quantitatif** et dont la relation entre la cible et l'entr√©e (**une variable**) semble √™tre lin√©aire $y = \alpha \times x + \beta$.

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Instanciation de sklearn.linear_model.LinearRegression
lr = LinearRegression()

# Entra√Ænement du mod√®le de r√©gression lin√©aire sur les donn√©es d'entra√Ænement
lr.fit(X_train, y_train)

# Pr√©diction sur les donn√©es de test
y_pred = lr.predict(X_test)
```

### M√©triques de test
Pour tester de la pertinence de la r√©gression, il faut pouvoir l'√©avluer. Pour √©valuer les performances d'un mod√®le de r√©gression lin√©aire avec `scikit-learn`, il existe plusieurs m√©triques de test pr√©d√©finies:

1. **MSE (Mean Squared Error)**: C'est la moyenne des carr√©s des erreurs. Il mesure la dispersion des erreurs.

   ```python
   from sklearn.metrics import mean_squared_error
   mse = mean_squared_error(y_true, y_pred)
   ```

2. **RMSE (Root Mean Squared Error)**: C'est la racine carr√©e du MSE. Il est dans la m√™me unit√© que la variable cible, ce qui le rend plus interpr√©table.

   ```python
   rmse = mean_squared_error(y_true, y_pred, squared=False)
   ```

3. **MAE (Mean Absolute Error)**: C'est la moyenne des erreurs absolues. Cela donne une id√©e de la taille des erreurs en unit√©s de la variable cible.

   ```python
   from sklearn.metrics import mean_absolute_error
   mae = mean_absolute_error(y_true, y_pred)
   ```

4. **R¬≤ (Coefficient de d√©termination)**: Cette m√©trique indique la proportion de la variance dans la variable cible qui est pr√©dit par le mod√®le. Elle varie entre 0 et 1, avec des valeurs plus √©lev√©es indiquant un meilleur ajustement.

   ```python
   from sklearn.metrics import r2_score
   r2 = r2_score(y_true, y_pred)
   ```



### Methodes de d√©tection d'*outliers*
Certains mod√®les peuvent pr√©senter des m√©triques assez mauvaises √† cause de certaines valeurs ab√©rantes qui induisent le mod√®le en erreur. Pour d√©tecter ces *"outliers"*, il existe plusieurs m√©thodes:


1. **Bo√Æte √† Moustaches (*Boxplot*)**: Un boxplot visualise la distribution des donn√©es et identifie les outliers comme des points situ√©s en dehors des moustaches ($1,5$ fois l'intervalle interquartile). Cela permet d'identifier **visuellement** les outliers.

```python
import matplotlib.pyplot as plt

plt.boxplot(data)
plt.title('Boxplot des donn√©es')
plt.show()
```


2. **Distance de Cook**: La distance de Cook mesure l'influence d'un point de donn√©es sur les coefficients du mod√®le de r√©gression. Elle √©value l'impact d'une observation sur les valeurs ajust√©es: Un point avec une distance de Cook sup√©rieure √† $1$ ou √† $\frac{4}{n}$, o√π $n$ est le nombre d'observations, peut √™tre consid√©r√© comme influent.   

La distance de cook peut se calculer manuellement ou via statsmodels:
  
```python
import statsmodels.api as sm

model = sm.OLS(y, X).fit()
influence = model.get_influence()
cooks_d = influence.cooks_distance
```


3. **R√©sidus Studentis√©s**: Les r√©sidus studentis√©s sont des r√©sidus standardis√©s qui tiennent compte de la variance des r√©sidus, permettant d'identifier les observations atypiques: Des r√©sidus studentis√©s sup√©rieurs √† $3$ ou inf√©rieurs √† $-3$ indiquent des points qui s'√©cartent de la tendance g√©n√©rale.

```python
studentized_residuals = influence.resid_studentized
```


Il est possible √©galement de faire des tests statistiques.


<br>
<br>
<br>

## R√©gression lin√©aire multiple
A utiliser dans le cas d'un probl√®me **supervis√©** avec un label **quantitatif** et dont la cible semblerait √™tre proche d'une **combinaision lin√©aire des entr√©es** (**plusieurs variables**) semble √™tre lin√©aire $y = \sum_{i} \alpha_{i} \times x + \beta_{0}$.

Le code pour effectuer une r√©gression lin√©aire multiple est le m√™me que celui pour cr√©er une r√©gression lin√©aire sauf que maintenant, $\mathcal{X}$ est une matrice o√π chaque colonne repr√©sente une variable ind√©pendante (ou pr√©dicteur), et chaque ligne repr√©sente une observation. 

### S√©lection des variables
Toutes les variables √† disposition dans $\mathcal{X}$ ne sont pas forc√©ment n√©cessaire pour la pr√©diction de la sortie. Il faut alors proc√©der √† la s√©lection des variables. Il existe plusieurs m√©thodes (cf. [Pre-processing doc.](pre-processing.md#s√©lection-des-variables)
).    
Une fois les variables pertienentes s√©lectionn√©es, une r√©gression lin√©aire multiple peut √™tre r√©alis√©e. 

<br>
<br>

### R√©gression avec r√©gularisation
#### R√©gression lin√©aire avec r√©gularisation Lasso
La r√©gression lasso pr√©sente plusieurs int√©r√™ts, notamment:

1. **S√©lection de variables**: Lasso p√©nalise les coefficients des variables, ce qui peut conduire √† mettre certains d'entre eux √† z√©ro. Cela permet d'identifier et de conserver uniquement les variables les plus pertinentes, simplifiant ainsi le mod√®le.

2. **R√©duction du surapprentissage**: En ajoutant une p√©nalit√© sur la complexit√© du mod√®le, la r√©gression lasso aide √† r√©duire le risque de surapprentissage, ce qui peut am√©liorer la capacit√© de g√©n√©ralisation du mod√®le sur des donn√©es non vues.

3. **Robustesse face √† la multicolin√©arit√©**: Lorsque des variables sont corr√©l√©es, lasso peut choisir l'une d'elles et ignorer les autres, offrant ainsi une solution stable m√™me en pr√©sence de multicolin√©arit√©.

4. **Interpr√©tabilit√©**: En r√©duisant le nombre de variables, le mod√®le devient plus interpr√©table. Cela facilite l'analyse et la compr√©hension des relations entre les variables et la variable cible.

5. **Flexibilit√©**: Lasso peut √™tre utilis√© dans de nombreux contextes, qu'il s'agisse de r√©gression lin√©aire, de classification, ou d'autres types de probl√®mes de mod√©lisation.

En somme, la r√©gression lasso est un outil puissant pour g√©rer la complexit√© des mod√®les, am√©liorer leur performance et faciliter l'interpr√©tation des r√©sultats.

Voici le code d'impl√©mentation d'une r√©gression Lasso simple:
```python
from sklearn.linear_model import Lasso
from sklearn.model_selection import train_test_split

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Coefficient de contr√¥le de l'ampleur de la p√©nalisation (L1) appliqu√©e aux coefficients de la r√©gression
# Plus il est √©lev√©, plus la p√©nalisation est grande
alpha = 0.5

# Instanciation de sklearn.linear_model.Lasso
lasso_regressor = Lasso(alpha=alpha)

# Entra√Ænement du mod√®le de r√©gression Lasso sur les donn√©es d'entra√Ænement
lasso_regressor.fit(X_train, y_train)

# Pr√©diction sur les donn√©es de test
y_pred_lasso = lasso_regressor.predict(X_test)

# Affichage des coefficients du mod√®le
coefficients = pd.Series(lasso_regressor.coef_.flatten(), index=X.columns)
print("\nCoefficients du mod√®le Lasso:")
print(coefficients)
```


#### R√©gression lin√©aire avec r√©gularisation Ridge
La r√©gression **Ridge** est id√©ale pour les situations avec multicolin√©arit√© et quand on veut des coefficients plus stables sans n√©cessairement r√©duire le nombre de variables.

1. **Gestion de la multicolin√©arit√©**: Ridge est particuli√®rement efficace lorsque les variables d'entr√©e sont fortement corr√©l√©es. En ajoutant une p√©nalit√© sur la somme des carr√©s des coefficients (p√©nalit√© l2), elle stabilise les estimations et r√©duit la variance.

2. **Am√©lioration de la pr√©diction**: En contraignant les coefficients, Ridge aide √† √©viter le surapprentissage, ce qui peut am√©liorer les performances pr√©dictives sur des donn√©es non vues, surtout lorsque le mod√®le est complexe.

3. **Aucune s√©lection de variables**: Bien que Ridge ne r√©alise pas de s√©lection de variables (tous les coefficients restent non nuls), il fournit une estimation plus stable des coefficients, ce qui peut √™tre souhaitable dans certains cas.


Voici le code d'impl√©mentation d'une r√©gression Ridge simple:
```python
from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Coefficient de contr√¥le de l'ampleur de la p√©nalisation (L2) appliqu√©e aux coefficients de la r√©gression
# Plus il est √©lev√©, plus la p√©nalisation est grande
alpha = 0.5

# Instanciation de sklearn.linear_model.Ridge
ridge_regressor = Ridge(alpha=alpha)

# Entra√Ænement du mod√®le de r√©gression Ridge sur les donn√©es d'entra√Ænement
ridge_regressor.fit(X_train, y_train)

# Pr√©diction sur les donn√©es de test
y_pred_ridge = ridge_regressor.predict(X_test)

# Affichage des coefficients du mod√®le
coefficients = pd.Series(ridge_regressor.coef_.flatten(), index=X.columns)
print("\nCoefficients du mod√®le Ridge:")
print(coefficients)
```



#### R√©gression Elastic Net

La r√©gression **Elastic Net** est pr√©f√©rable quand on a de nombreuses variables corr√©l√©es et qu'on souhaite r√©aliser une s√©lection de variables tout en gardant une certaine robustesse.

1. **Combinaison des avantages de Lasso et Ridge**: Elastic Net combine les p√©nalit√©s l1 et l2, ce qui permet √† la fois la s√©lection de variables (comme avec Lasso) et la gestion de la multicolin√©arit√© (comme avec Ridge).

2. **Robustesse en cas de nombreuses variables**: Elastic Net est particuli√®rement utile lorsque le nombre de variables pr√©dictives est sup√©rieur au nombre d'observations ou en pr√©sence de variables corr√©l√©es. Il peut s√©lectionner plusieurs variables corr√©l√©es tout en maintenant la stabilit√© des estimations.

3. **Flexibilit√©**: En ajustant le param√®tre \( l1\_ratio \), tu peux contr√¥ler la balance entre la s√©lection de variables et la r√©gularisation, permettant une personnalisation selon le probl√®me.


Voici le code d'impl√©mentation d'une r√©gression Elastic Net simple:
```python
from sklearn.linear_model import ElasticNet
from sklearn.model_selection import train_test_split

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Coefficient de contr√¥le de l'ampleur de la p√©nalisation appliqu√©e aux coefficients de la r√©gression
# alpha contr√¥le la force de la p√©nalisation, l1_ratio contr√¥le la combinaison entre l1 et l2
alpha = 0.5
l1_ratio = 0.5  # 0.5 pour un m√©lange √©gal de Lasso et Ridge

# Instanciation de sklearn.linear_model.ElasticNet
elastic_net_regressor = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)

# Entra√Ænement du mod√®le de r√©gression Elastic Net sur les donn√©es d'entra√Ænement
elastic_net_regressor.fit(X_train, y_train)

# Pr√©diction sur les donn√©es de test
y_pred_elastic_net = elastic_net_regressor.predict(X_test)

# Affichage des coefficients du mod√®le
coefficients = pd.Series(elastic_net_regressor.coef_.flatten(), index=X.columns)
print("\nCoefficients du mod√®le ElasticNet:")
print(coefficients)
```




### R√©gression PLS (Partial Least Squares)

La r√©gression PLS est une m√©thode statistique utilis√©e principalement pour des situations o√π **le nombre de variables pr√©dictives est √©lev√© par rapport au nombre d'observations, ou lorsque les variables pr√©dictives sont corr√©l√©es**. Voici les principaux int√©r√™ts et applications de la r√©gression PLS:

1. **R√©duction de la dimensionnalit√©**: PLS combine la r√©duction de dimensionnalit√© et la mod√©lisation pr√©dictive. Elle projette les donn√©es dans un espace de dimensions inf√©rieures, facilitant ainsi l'analyse sans perdre trop d'information.

2. **Gestion de la multicolin√©arit√©**: Lorsque les variables pr√©dictives sont corr√©l√©es, PLS peut √™tre plus efficace que d'autres m√©thodes, comme la r√©gression lin√©aire ordinaire, qui peuvent donner des estimations instables en raison de la multicolin√©arit√©.

3. **Optimisation de la pr√©diction**: PLS maximise la covariance entre les variables pr√©dictives et la variable cible. Cela permet de construire des mod√®les pr√©dictifs qui capturent mieux les relations entre les variables.

4. **Adaptabilit√©**: PLS est flexible et peut √™tre utilis√© dans divers contextes, que ce soit pour des donn√©es exp√©rimentales, des donn√©es spectrales, ou d'autres types de donn√©es complexes.


Voici le code d'impl√©mentation d'une r√©gression PLS simple:
```python
from sklearn.cross_decomposition import PLSRegression
from sklearn.model_selection import train_test_split
import pandas as pd

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Instanciation du mod√®le PLS avec un nombre de composantes latentes
n_components = 5  # Choisir le nombre de composantes latentes
pls = PLSRegression(n_components=n_components)

# Entra√Ænement du mod√®le
pls.fit(X_train, y_train)

# Pr√©diction sur les donn√©es de test
y_pred_pls = pls.predict(X_test)

# Affichage des coefficients du mod√®le
coefficients = pd.Series(pls.coef_.flatten(), index=X.columns)
print("\nCoefficients du mod√®le PLS:")
print(coefficients)
```




### R√©gression Logistique

La r√©gression logistique est une m√©thode statistique utilis√©e pour mod√©liser la relation entre une variable d√©pendante binaire (ou cat√©gorique) et une ou plusieurs variables ind√©pendantes. Elle est couramment utilis√©e pour des probl√®mes de classification o√π l'objectif est de pr√©dire l'appartenance √† l'une des deux cat√©gories.    
Voici quelques caract√©ristiques cl√©s de la r√©gression logistique:

1. **Variable d√©pendante binaire**: La r√©gression logistique est principalement utilis√©e lorsque la variable cible est binaire (par exemple, succ√®s/√©chec, oui/non, 0/1).

2. **Fonction logistique**: La r√©gression logistique utilise la fonction logistique (ou sigmo√Øde) pour mod√©liser la probabilit√© que la variable d√©pendante prenne la valeur 1. La fonction logistique est d√©finie comme: $P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k)}}$ o√π \( P(Y=1|X) \) est la probabilit√© que \( Y \) soit √©gal √† 1, \( \beta_0 \) est l'ordonn√©e √† l'origine, et \( \beta_1, \beta_2, \ldots, \beta_k \) sont les coefficients des variables ind√©pendantes.

3. **Estimation des param√®tres**: Les coefficients du mod√®le sont g√©n√©ralement estim√©s par la m√©thode de maximum de vraisemblance, qui cherche √† maximiser la probabilit√© d'observer les donn√©es donn√©es les param√®tres du mod√®le.

4. **Interpr√©tation des coefficients**: Les coefficients dans une r√©gression logistique peuvent √™tre interpr√©t√©s en termes d'odds (cotes). Par exemple, un coefficient positif indique qu'une augmentation de la variable ind√©pendante augmente les chances que la variable d√©pendante soit √©gale √† 1.

5. **Extensions**: Bien qu'elle soit principalement utilis√©e pour des probl√®mes de classification binaire, la r√©gression logistique peut √©galement √™tre √©tendue √† des cas multiclasse √† l'aide de techniques telles que la r√©gression logistique multinomiale.


Voici le code d'impl√©mentation d'une r√©gression logistique simple:
```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import pandas as pd

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Instanciation du mod√®le r√©gression logistique
clf = LogisticRegression(random_state=0)

# Entra√Ænement du mod√®le
clf.fit(X_train, y_train)

# Pr√©diction sur les donn√©es de test
clf.predict(X_test)
clf.predict_proba(X_test)
clf_score =  clf.score(X_test, y_test)

# Affichage du score du mod√®le
print(f"Score clf: {clf_score}")
print(f"Pourcentage d'erreur sur les tests: {round((1-clf_score)*100, 2)}%")

# Affichage des coefficients du mod√®le
coefficients = pd.Series(clf.coef_.flatten(), index=X.columns)
print("\nCoefficients du mod√®le logistique:")
print(coefficients)
```

<br>
<br>

## R√©gression Gaussienne

```python
from smt.surrogate_models import KRG

# Instanciation du mod√®le
gpr = KRG()

# Entra√Ænement du mod√®le
gpr.set_training_values(X_data, y_data)
gpr.train()

print('Theta optimal', gpr.optimal_theta)
```


## Forets al√©atoires

### For√™ts al√©atoires pour la r√©gression

Les **for√™ts al√©atoires** (*Random Forest*) sont un algorithme d'**ensemble learning** dont l'id√©e est de combiner plusieurs arbres de d√©cision pour obtenir une pr√©diction plus robuste et plus pr√©cise. En particulier, l'algorithme construit une **for√™t** d'arbres de d√©cision en introduisant de l'**al√©atoire** √† deux niveaux:
1. **√âchantillonnage bootstrap** des donn√©es (sub-sampling des donn√©es d'entra√Ænement).
2. **S√©lection al√©atoire des caract√©ristiques** √† chaque division (split) dans chaque arbre.

L'algorithme des for√™ts al√©atoires fonctionne en plusieurs √©tapes:

#### 1. **Cr√©ation d'arbres de d√©cision al√©atoires**
   - Comme dans la version pour la classification, on cr√©e plusieurs **arbres de d√©cision** en √©chantillonnant al√©atoirement les donn√©es d'entra√Ænement √† chaque arbre, √† l'aide de l'**√©chantillonnage bootstrap**.
   - Chaque arbre est donc construit sur un sous-ensemble diff√©rent des donn√©es, ce qui introduit de la diversit√© et am√©liore la g√©n√©ralisation du mod√®le.

#### 2. **S√©lection al√©atoire des caract√©ristiques**
   - √Ä chaque n≈ìud de l'arbre, au lieu d'examiner toutes les caract√©ristiques pour choisir la meilleure coupure, un sous-ensemble al√©atoire des caract√©ristiques est choisi. Cela contribue √† la diversit√© des arbres et √† la r√©duction de la corr√©lation entre eux.

#### 3. **Entra√Ænement des arbres**
   - Chaque arbre est entra√Æn√© ind√©pendamment sur un sous-ensemble des donn√©es, avec une s√©lection al√©atoire des caract√©ristiques √† chaque n≈ìud, ce qui permet de cr√©er des arbres avec des structures diff√©rentes.

#### 4. **Pr√©diction finale**
   - La pr√©diction finale de la for√™t al√©atoire est la **moyenne** des pr√©dictions de tous les arbres. Cela permet d'obtenir une estimation plus pr√©cise que celle d'un seul arbre de d√©cision.

---

### Avantages des For√™ts Al√©atoires:
1. **Robustesse**
2. **Pr√©cision**
3. **Adaptabilit√©**

### Inconv√©nients des For√™ts Al√©atoires pour la r√©gression:
1. **Complexit√© computationnelle**
2. **Moins interpr√©table**
3. **Risque d'overfitting avec des arbres trop profonds**


```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt


# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©er une for√™t al√©atoire pour la r√©gression
rf_regressor = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)

# Entra√Æner la for√™t al√©atoire
rf_regressor.fit(X_train, y_train)

# Pr√©dire les valeurs pour les donn√©es de test
y_pred = rf_regressor.predict(X_test)

# Calculer l'erreur absolue moyenne (MAE) ou l'erreur quadratique moyenne (RMSE)
from sklearn.metrics import mean_absolute_error, mean_squared_error
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"Mean Absolute Error (MAE): {mae}")
print(f"Root Mean Squared Error (RMSE): {rmse}")

# Visualiser les r√©sultats
plt.scatter(y_test, y_pred)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.xlabel('Vrai y')
plt.ylabel('Pr√©diction y')
plt.title('Pr√©dictions vs R√©el')
plt.show()

# Importance des features
importances = rf_regressor.feature_importances_
std = np.std([tree.feature_importances_ for tree in rf_regressor.estimators_], axis=0)
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(12, 8))
plt.title("Importance des caract√©ristiques")
plt.bar(range(X.shape[1]), importances[indices], color="r", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), indices)
plt.xlim([-1, X.shape[1]])
plt.show()
```

### Hyperparam√®tres importants √† ajuster pour la r√©gression:
1. **`n_estimators`**: Le nombre d'arbres dans la for√™t. Un nombre plus √©lev√© peut am√©liorer la pr√©cision, mais augmente aussi le temps de calcul.
2. **`max_features`**: Le nombre maximal de caract√©ristiques √† consid√©rer pour chaque division d'arbre. Cela augmente la diversit√© des arbres et r√©duit la corr√©lation entre eux.
3. **`bootstrap`**: Si l'√©chantillonnage bootstrap est utilis√© pour l'√©chantillon des donn√©es (par d√©faut, c'est `True`).
4. **`oob_score`**: Le score Out-Of-Bag, qui permet d'estimer la performance du mod√®le sans un ensemble de validation s√©par√©.


<br>
<br>
<br>
<br>

## Affichage 

Pour tracer une r√©gression, il est possible d'utiliser `plotly.express` ou `plotly.graph_objects` pour cr√©er des visualisations interactives:
```python
import numpy as np
import pandas as pd
import plotly.express as px


# Si X est une matrice (n_samples, n_features): Utiliser une seule colonne pour le trac√©
data = pd.DataFrame({'Feature': X[:, 0], 'Target': y})  # Prendre la premi√®re caract√©ristique

# Si X est un vecteur (n_samples):
data = pd.DataFrame({'Feature': X.flatten(), 'Target': y})

# Instanciation et entra√Ænement du mod√®le de r√©gression choisi
model = #RegressionModelFunction()
model.fit(X, y)

# Cr√©er un espace de valeurs pour la pr√©diction
X_range = np.linspace(X.min(), X.max(), 300).reshape(-1, 1)
y_pred = model.predict_proba(X_range)[:, 1]  # Probabilit√©s de la classe positive

# Tracer les donn√©es et la courbe de r√©gression logistique avec Plotly
fig = px.scatter(data, x='Feature', y='Target', title='R√©gression Logistique', labels={'Target': 'Classe', 'Feature': 'Feature'})
fig.add_scatter(x=X_range.flatten(), y=y_pred, mode='lines', name='Courbe de r√©gression', line=dict(color='red'))

# Afficher la figure
fig.show()
```