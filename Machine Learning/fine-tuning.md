<h1 align='center'> Machine Learning - Fine tuning üöÄ</h1>

## **I. Diff√©rence entre param√®tres et hyperparam√®tres**

- **Param√®tres**: Ce sont les valeurs apprises par le mod√®le pendant l'entra√Ænement, comme les poids d'un r√©seau neuronal ou les coefficients d'une r√©gression lin√©aire.  
  Exemple: $\mathbf{w}$ et $b$ dans $y = \mathbf{w}^T\mathbf{x} + b$.

- **Hyperparam√®tres**: Ce sont les param√®tres d√©finis avant l'entra√Ænement et non appris directement √† partir des donn√©es. Ils influencent la structure ou le comportement de l'apprentissage.  
  Exemple: le taux d'apprentissage, la profondeur des arbres, le nombre de neurones dans une couche cach√©e.

L'optimisation des hyperparam√®tres permet de:
- R√©duire le **sous-ajustement** (mod√®le trop simple, faible performance).
- R√©duire le **surajustement** (mod√®le trop complexe, mauvais sur les nouvelles donn√©es).
- Maximiser les performances sur un ensemble de validation ou en cross-validation.

<br>

## **II. M√©thodes d‚Äôoptimisation des hyperparam√®tres**

### **3.1. Recherche en grille (Grid Search)**

La recherche en grille explore toutes les combinaisons possibles d‚Äôhyperparam√®tres dans un espace d√©fini.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# Chargement des donn√©es
data = load_iris()
X, y = data.data, data.target

# D√©finir les hyperparam√®tres √† explorer
param_grid = {
    'n_estimators': [10, 50, 100],  # Nombre d'arbres
    'max_depth': [3, 5, 10],        # Profondeur maximale
    'min_samples_split': [2, 5],    # Minimum d'√©chantillons pour diviser un n≈ìud
}

# Cr√©ation du mod√®le et de la recherche en grille
model = RandomForestClassifier()
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')

# Entra√Ænement
grid_search.fit(X, y)

# R√©sultats
print("Meilleurs hyperparam√®tres:", grid_search.best_params_)
print("Meilleure pr√©cision:", grid_search.best_score_)
```

#### **Avantages**:
- Facile √† comprendre et √† impl√©menter.

#### **Inconv√©nients**:
- Co√ªt computationnel √©lev√©, surtout si l‚Äôespace d‚Äôhyperparam√®tres est grand.

---

### **3.2. Recherche al√©atoire (Random Search)**

La recherche al√©atoire explore des combinaisons al√©atoires d‚Äôhyperparam√®tres dans l‚Äôespace d√©fini.

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(10, 200),  # Distribution uniforme pour le nombre d'arbres
    'max_depth': randint(3, 15),       # Distribution uniforme pour la profondeur
}

# Cr√©ation du mod√®le et de la recherche al√©atoire
random_search = RandomizedSearchCV(model, param_dist, n_iter=20, cv=5, scoring='accuracy', random_state=42)
random_search.fit(X, y)

print("Meilleurs hyperparam√®tres:", random_search.best_params_)
print("Meilleure pr√©cision:", random_search.best_score_)
```

#### **Avantages**:
- Plus rapide que Grid Search.
- Utile si certains hyperparam√®tres sont moins importants.

#### **Inconv√©nients**:
- Ne garantit pas d‚Äôexplorer toutes les bonnes combinaisons.

---

### **3.3. Optimisation bay√©sienne**

L‚Äôoptimisation bay√©sienne construit un mod√®le probabiliste de la fonction d‚Äôobjectif et choisit les hyperparam√®tres en fonction des r√©sultats pr√©c√©dents.

**Exemple avec Optuna:**
```python
import optuna
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score

# Fonction d'optimisation
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 50, 200),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
    }
    model = GradientBoostingClassifier(**params)
    score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()
    return score

# Ex√©cution de l'optimisation
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

print("Meilleurs hyperparam√®tres:", study.best_params)
print("Meilleure pr√©cision:", study.best_value)
```

#### **Avantages**:
- Plus efficace pour les espaces complexes.
- Explore les hyperparam√®tres en fonction des performances pass√©es.

#### **Inconv√©nients**:
- Plus complexe √† mettre en ≈ìuvre.

(cf. [processus_gaussien](supervised/processus_gaussien.md))

---

### **3.4. Algorithmes √©volutionnaires**

Ces techniques, comme l‚Äôalgorithme g√©n√©tique, utilisent des principes d‚Äô√©volution pour explorer l‚Äôespace des hyperparam√®tres.

Exemple: **TPOT** (Tool for Optimized Pipeline).
TPOT (**Tree-based Pipeline Optimization Tool**) est une biblioth√®que Python qui utilise des algorithmes g√©n√©tiques pour automatiser la s√©lection des mod√®les et l'optimisation des hyperparam√®tres. Voici comment utiliser TPOT pour un probl√®me de classification:

```python
from tpot import TPOTClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

# Chargement des donn√©es
data = load_iris()
X, y = data.data, data.target

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation de l'instance TPOT
tpot = TPOTClassifier(
    generations=5,            # Nombre de g√©n√©rations pour l'√©volution
    population_size=20,       # Taille de la population √† chaque g√©n√©ration
    verbosity=2,              # Niveau de d√©tails des logs
    random_state=42,          # Reproductibilit√©
    scoring='accuracy',       # M√©trique d'√©valuation
    cv=5                      # Validation crois√©e
)

# Entra√Ænement
tpot.fit(X_train, y_train)

# Pr√©dictions et √©valuation
print(f"Score sur les donn√©es de test: {tpot.score(X_test, y_test):.4f}")

# Exporter le meilleur pipeline
tpot.export('best_pipeline.py')  # Sauvegarde le pipeline en Python
```

<br>

Voici comment utiliser TPOT pour un probl√®me de r√©gression:

```python
from tpot import TPOTRegressor
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_california_housing

# Chargement des donn√©es
data = fetch_california_housing()
X, y = data.data, data.target

# Division des donn√©es en ensembles d'entra√Ænement et de test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Cr√©ation de l'instance TPOT
tpot = TPOTRegressor(
    generations=5,             # Nombre de g√©n√©rations pour l'√©volution
    population_size=20,        # Taille de la population √† chaque g√©n√©ration
    verbosity=2,               # Niveau de d√©tails des logs
    random_state=42,           # Reproductibilit√©
    scoring='r2',              # M√©trique d'√©valuation
    cv=5                       # Validation crois√©e
)

# Entra√Ænement
tpot.fit(X_train, y_train)

# Pr√©dictions et √©valuation
print(f"Score R¬≤ sur les donn√©es de test: {tpot.score(X_test, y_test):.4f}")

# Exporter le meilleur pipeline
tpot.export('best_pipeline_regression.py')
```



### **Interpr√©tation des r√©sultats**
1. **Generations et population_size** :
   - `generations` : Le nombre d'it√©rations pour l'optimisation.
   - `population_size` : Le nombre de pipelines diff√©rents test√©s par g√©n√©ration.
   
2. **Exportation du pipeline** :
   - Le fichier export√© (`best_pipeline.py`) contient le meilleur pipeline trouv√© par TPOT. Vous pouvez l'utiliser sans r√©ex√©cuter TPOT.

---

*<u>Remarques:</u>*
- **TPOT est gourmand en ressources** : Pour les grands ensembles de donn√©es ou de nombreuses g√©n√©rations, utilisez des machines puissantes ou r√©duisez les tailles de population/g√©n√©rations.
- Si votre temps est limit√©, vous pouvez :
  - R√©duire `population_size` et `generations`.
  - Ajouter une contrainte de temps avec `max_time_mins` (temps maximal pour l'ex√©cution totale).



<br>
<br>

### **III. Bonnes pratiques pour le fine-tuning**

#### **1. D√©finir un espace d‚Äôhyperparam√®tres pertinent**
- En se basant sur les **caract√©ristiques des donn√©es** et les **limites du mod√®le**.
- Exemple: Pour un arbre de d√©cision, une tr√®s grande profondeur peut entra√Æner du surajustement.

#### **4.2. Utiliser la validation crois√©e**
- Toujours √©valuer les performances sur un ensemble de validation pour √©viter le surajustement.

#### **4.3. Surveiller le temps de calcul**
- √âquilibrer la pr√©cision souhait√©e avec le co√ªt computationnel.

#### **4.4. √âviter les pi√®ges courants**
- Ne pas tester trop de combinaisons sans justification.
- Ne pas optimiser sur l‚Äôensemble de test (r√©server un ensemble pour l‚Äô√©valuation finale).

<br>
<br>

### **IV. Importance des hyperparam√®tres courants**

| Mod√®le                    | Hyperparam√®tres cl√©s                               |
|---------------------------|---------------------------------------------------|
| Arbres de d√©cision         | `max_depth`, `min_samples_split`, `min_samples_leaf` |
| For√™t al√©atoire            | `n_estimators`, `max_depth`, `max_features`       |
| Gradient Boosting          | `n_estimators`, `learning_rate`, `subsample`      |
| R√©seaux neuronaux          | `learning_rate`, `batch_size`, `epochs`, `layers` |
| KNN                        | `n_neighbors`, `metric`                           |


<br>
<br>


### **V. Structure, r√©tr√©cissement et randomisation**
Pour contr√¥ler le **surajustement** dans les arbres √† gradient boost√©, trois principaux leviers peuvent √™tre utilis√©s: la **structure de l'arbre**, le **r√©tr√©cissement** (learning rate), et la **randomisation** (sous-√©chantillonnage des donn√©es ou des features).


#### **1. Contr√¥ler la structure de l‚Äôarbre**
La structure de l‚Äôarbre d√©termine la complexit√© des mod√®les individuels (arbres). Une complexit√© excessive peut entra√Æner un surajustement.

- **`max_depth`**: Limite la profondeur des arbres pour r√©duire leur complexit√©.
- **`min_child_weight`**: Requiert un nombre minimum d‚Äô√©chantillons dans une feuille pour qu‚Äôelle soit cr√©√©e.
- **`gamma`**: Repr√©sente le gain minimum requis pour diviser un noeud. Plus il est grand, plus le mod√®le sera r√©gulier.

```python
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Chargement des donn√©es California Housing
from sklearn.datasets import fetch_california_housing
data = fetch_california_housing()
X, y = data.data, data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Contr√¥le de la structure de l'arbre
params = {
    'objective': 'reg:squarederror',
    'max_depth': 4,                # Limite la profondeur des arbres
    'min_child_weight': 10,        # Minimum d'√©chantillons par feuille
    'gamma': 0.2,                  # Gain minimal pour diviser un n≈ìud
    'eta': 0.1,                    # Taux d'apprentissage (r√©tr√©cissement)
    'subsample': 0.8,              # Randomisation (voir plus bas)
    'colsample_bytree': 0.8        # Randomisation (voir plus bas)
}

dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dtest, 'test')], early_stopping_rounds=10)

# Pr√©dictions
y_pred = model.predict(dtest)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print(f"RMSE: {rmse:.2f}")
```

<br>

#### **2. R√©tr√©cissement (Learning Rate ou `eta`)**
Le **learning rate** r√©duit l‚Äôimpact de chaque arbre individuel sur le mod√®le final. Cela emp√™che les ajustements excessifs √† une it√©ration donn√©e. Cependant, cela n√©cessite g√©n√©ralement un plus grand nombre d‚Äôarbres pour converger.

- **`eta`**: Contr√¥le la contribution de chaque arbre (valeurs typiques: 0.01 √† 0.3).

```python
params = {
    'objective': 'reg:squarederror',
    'max_depth': 6,
    'eta': 0.05,                  # Learning rate faible pour plus de r√©gularisation
    'subsample': 0.8,
    'colsample_bytree': 0.8
}

# Entra√Æner avec un taux d'apprentissage faible
model = xgb.train(params, dtrain, num_boost_round=300, evals=[(dtest, 'test')], early_stopping_rounds=10)

# Pr√©dictions
y_pred = model.predict(dtest)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print(f"RMSE avec learning rate faible: {rmse:.2f}")
```

<br>

#### **3. Randomisation (Sous-√©chantillonnage)**
La randomisation ajoute de la diversit√© entre les arbres, r√©duisant le risque de surajustement tout en maintenant une bonne performance globale.

- **`subsample`**: Fraction des donn√©es utilis√©es pour chaque arbre.
- **`colsample_bytree`**: Fraction des features utilis√©es pour construire chaque arbre.
- **`colsample_bylevel`**: Fraction des features √† chaque niveau de l‚Äôarbre.
- **`colsample_bynode`**: Fraction des features √† chaque noeud.

```python
params = {
    'objective': 'reg:squarederror',
    'max_depth': 4,
    'eta': 0.1,
    'subsample': 0.7,             # Utiliser 70% des √©chantillons par arbre
    'colsample_bytree': 0.6,      # Utiliser 60% des features par arbre
}

# Entra√Æner avec randomisation
model = xgb.train(params, dtrain, num_boost_round=100, evals=[(dtest, 'test')], early_stopping_rounds=10)

# Pr√©dictions
y_pred = model.predict(dtest)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print(f"RMSE avec randomisation: {rmse:.2f}")
```

---

#### **R√©sum√©: Combiner les 3 leviers**
Pour un contr√¥le optimal du surajustement, combiner ces trois techniques:
- Ajuster la **structure de l‚Äôarbre** pour √©viter des mod√®les trop complexes.
- R√©duiser l‚Äôimpact de chaque arbre avec un **learning rate faible**.
- Utiliser la **randomisation** pour introduire de la diversit√© entre les arbres.

#### **Exemple combin√©:**
```python
params = {
    'objective': 'reg:squarederror',
    'max_depth': 4,
    'min_child_weight': 5,
    'gamma': 0.3,
    'eta': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'lambda': 1.0,                # R√©gularisation L2 pour davantage de contr√¥le
    'alpha': 0.5                  # R√©gularisation L1
}

model = xgb.train(params, dtrain, num_boost_round=200, evals=[(dtest, 'test')], early_stopping_rounds=15)

# √âvaluation
y_pred = model.predict(dtest)
rmse = mean_squared_error(y_test, y_pred, squared=False)
print(f"RMSE final: {rmse:.2f}")
```