<h1 align='center'> Machine learning - D√©tection d'anomalies üîé</h1>

### Qu'est-ce qu'une anomalie?
* G√©n√©ralement: un individu (ligne) rare dans un ensemble de donn√©es qui diff√®re significativement de la majorit√© des donn√©es.
* Parfois: les anomalies ne sont pas si rares, et peuvent ne pas √™tre si diff√©rentes de la majorit√© des donn√©es...


Pour faire de la d√©tection d'anomalies, l'apprentissage supervis√© avec un ensemble de donn√©es √©tiquet√©es ne peut pas √™tre utilis√© car:
- L'ensemble de donn√©es serait tr√®s d√©s√©quilibr√©: Par d√©finition les anomalies sont **rares**, si alors il y a 5 anomalies pour 100 000 points normaux, il est difficile de faire un bon mod√®le.
- Il est impossible de couvrir tous les types d'anomalies √©tant donn√© qu'une anomalie est quelque chose d'inattendu. Alors, si un nouveau type d'anomalie apparait, il serait class√© comme normal, alors que non!

<br>

Il faut alors utiliser d'autres approches:
- D√©tection des valeurs aberrantes: l'ensemble de donn√©es contient des anomalies au sens de raret√© et statistiquement diff√©rent. D√©tecter les √©l√©ments de ce m√™me ensemble de donn√©es qui diff√®rent de la majorit√© des donn√©es
- D√©tection de nouveaut√©: Soit un ensemble de donn√©es propre sans anomalies, il faut apprendre le comportement normal, afin de pouvoir v√©rifier si un nouvel √©l√©ment est normal ou une anomalie





## D√©tection des valeurs aberrantes
D√©tecter dans un ensemble de donn√©es les √©l√©ments qui diff√®rent de la majorit√© des donn√©es.

En 1D:
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def plotAnomalies1D(s, anomalies, threshold1, threshold2):
   """
      s: Pandas Series containing all the points to plot
      anomalies: Pandas Series containing all the points which are anomalies
      threshold1: Float value - minimum threshold to be normal
      threshold2: Float value - maximum threshold to be normal
   """
   fig = plt.figure(figsize=(22,8))
   plt.plot(s, [0]*len(s), 'bo')
   plt.plot(anomalies, [0]*len(anomalies), 'ro')
   plt.plot([threshold1]*2, [-1,1], 'g--')
   plt.plot([threshold2]*2, [-1,1], 'g--')
   plt.show()

   return 0



med = df["col_name"].mean()
mad = (df["col_name"] - med).abs().mean()

threshold1 = med - 3*mad
threshold2 = med + 3*mad
anomalies = df["col_name"][(df["col_name"] - med).abs() > 3*mad].copy()

plotAnomalies1D(df["col_name"], anomalies, threshold1, threshold2)
```
La moyenne et l'√©cart-type ne sont pas toujours fiables. En effet, elles quantifient la densit√© des donn√©es dans le cas d'une distribution normale, mais ce n'est pas toujours le cas. Ces valeurs sont donc tr√®s sensibles aux valeurs aberrantes: S'il y a trop ou beaucoup de valeurs aberrantes, alors l'estimation sera fauss√©e.   
Comme alternative la **MAD** ($\text{Median Absolute Deviation} = \text{median}( | x - \text{median}(x) | )$) est utilis√©e.


<br>

En nD:
Il est possible de **r√©duire la dimension** du probl√®me en un probl√®me 2D, avec une ACP par exemple (cf [[pre-processing](../pre-processing.md)]). Ensuite, une fois que le probl√®me est en 2D, il sera possible de visualiser la s√©paration via ces fonctions:

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def plotAnomalies2D(df, clf_name, clf):
   """
      df: Pandas DataFrame containing all the points to plot (for features X1 and X2)
      clf_name: String value - name of the outlier detection model
      clf: Scikit Learn model instance - the trained outlier detection model
   """
   fig = plt.figure(figsize=(22,8))
   plt.plot(df['X1'],df['X2'], 'o')
   plt.xlabel('X1')
   plt.ylabel('X2')
   plt.xlim([df['X1'].min()-3,df['X1'].max()+3])
   plt.ylim([df['X2'].min()-3,df['X2'].max()+3])
   plt.title(clf_name)
   
   if clf_name == 'LOF':
      ypred = clf.fit_predict(df[['X1','X2']])
      plt.plot(df['X1'][ypred==-1],df['X2'][ypred==-1],'ro')
   else:
      xx = np.meshgrid(np.linspace(df['X1'].min()-3,df['X1'].max()+3, 500)
      yy = np.linspace(df['X2'].min()-3,df['X2'].max()+3, 500))
      Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
      Z = Z.reshape(xx.shape)
      plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='r')

   plt.show()

   return 0



def plotAnomalyScore2D(df, clf_name, clf):
   """
      df: Pandas DataFrame containing all the points to plot (for features X1 and X2)
      clf_name: String value - name of the outlier detection model
      clf: Scikit Learn model instance - the trained outlier detection model
   """
   if clf_name == 'LOF':
      score = clf.negative_outlier_factor_
   else:
      score = clf.decision_function(df[['X1','X2']])
   
   fig = plt.figure(figsize=(22,8))
   sc = plt.scatter(x=df['X1'],y=df['X2'], c=-score, cmap='Reds')
   plt.colorbar(sc, label='anomaly score')
   plt.xlabel('X1')
   plt.ylabel('X2')
   plt.title(clf_name)
   plt.show()

   return 0
```

<br>

Il existe plusieurs m√©thodes pour le traiter:

### 1. Elliptic envelope 
L‚Äôenveloppe elliptique est une m√©thode statistique classique pour la d√©tection des anomalies dans des ensembles de donn√©es multivari√©s. Elle repose sur l‚Äôhypoth√®se que les donn√©es suivent une distribution normale multivari√©e. Les points situ√©s loin de l‚Äôenveloppe d√©finie par cette distribution sont consid√©r√©s comme des anomalies:

L'enveloppe elliptique correspond √† une r√©gion d√©finie par l'√©quation de la distance de Mahalanobis:
$D^2(x) = (x - \mu)^T \Sigma^{-1} (x - \mu)$ o√π $D^2(x)$ est la distance de Mahalanobis d‚Äôun point $x$ par rapport √† la distribution, $\mu$ la moyenne (qui est le centre de l‚Äôenveloppe) et $\Sigma$ la matrice de covariance (qui d√©finit la forme et l‚Äô√©tendue de l‚Äôenveloppe).   
Les points dont $D^2(x)$ d√©passe un certain seuil sont consid√©r√©s comme des anomalies:

#### a. **Estimation de la moyenne et de la covariance:**
   - Les param√®tres $\mu$ et $\Sigma$ sont estim√©s √† partir des donn√©es en supposant une distribution normale multivari√©e.
   - Pour rendre la m√©thode plus robuste aux anomalies, des techniques comme le **Minimum Covariance Determinant (MCD)** peuvent √™tre utilis√©es pour initialiser ces param√®tres.

#### b. **Calcul des distances de Mahalanobis:**
   - Une fois $\mu$ et $\Sigma$ estim√©s, la distance de Mahalanobis est calcul√©e pour chaque point.

#### c. **Fixation d‚Äôun seuil:**
   - Le seuil est souvent choisi en fonction du degr√© de confiance $95\%$, $99\%$ dans une distribution $\chi^2$ √† $d$ degr√©s de libert√© $d$ est le nombre de dimensions des donn√©es.

#### d. **Identification des anomalies:**
   - Les points situ√©s en dehors de l‚Äôenveloppe sont marqu√©s comme anomalies.

```python
from sklearn.covariance import EllipticEnvelope
from sklearn.model_selection import GridSearchCV

clf_name = 'Elliptic Envelope'
clf = EllipticEnvelope()

param_grid = {
    'support_fraction': [0.8, 0.9, 1.0],
    'contamination': [0.05, 0.1, 0.2],   
    'assume_centered': [True, False]  
}

grid_search = GridSearchCV(
    estimator=clf,
    param_grid=param_grid,
    scoring=custom_scorer,  
    cv=3,
    verbose=True,
    n_jobs=-1
)

grid_search.fit(df_2d[['X1', 'X2']])

best_params = grid_search.best_params_
print(f"Meilleurs hyperparam√®tres: {best_params}")

best_clf = EllipticEnvelope(**best_params)

y_pred_elliptic_envelope = best_clf.fit_predict(df_2d[['X1', 'X2']])

plotAnomalies2D(df_2d, clf_name, best_clf)
```
Il est √† noter qu'une recherche des meilleurs hyper-param√®tres est effectu√© via une *grid search*, ce qui √©vite de tester les param√®tres "√† la main".

#### **Param√®tres importants:**

1. **`support_fraction`:**
   - Fraction minimale des points consid√©r√©s comme non-anormaux lors de l‚Äôestimation de la covariance.
   - Plus faible, elle am√©liore la robustesse mais peut ignorer plus de points normaux.

2. **`contamination`:**
   - Fraction estim√©e d‚Äôanomalies dans les donn√©es.

3. **`random_state`:**
   - G√®re la reproductibilit√© des r√©sultats.


#### **Avantages:**

1. **Simplicit√© math√©matique:**
   - Bas√©e sur des concepts statistiques solides et largement utilis√©s (distribution normale multivari√©e, distance de Mahalanobis).

2. **Efficace pour des donn√©es gaussiennes:**
   - Si les donn√©es suivent effectivement une distribution gaussienne multivari√©e, cette m√©thode est optimale.

3. **Robustesse avec le MCD:**
   - L‚Äôutilisation du MCD pour initialiser $\mu$ et $\Sigma$ rend la m√©thode robuste face √† un petit nombre d‚Äôanomalies.


#### **Limitations:**

1. **Hypoth√®se de normalit√©:**
   - L‚Äôefficacit√© de l‚Äôenveloppe elliptique repose sur l‚Äôhypoth√®se que les donn√©es normales suivent une distribution gaussienne multivari√©e.
   - Si les donn√©es ne suivent pas cette hypoth√®se, la m√©thode peut √™tre moins performante.

2. **Moins efficace dans les hautes dimensions:**
   - Les estimations de la covariance deviennent moins fiables avec des dimensions √©lev√©es (probl√®me de la mal√©diction de la dimensionnalit√©).

3. **Risque de sur-apprentissage:**
   - Si les donn√©es contiennent trop d‚Äôanomalies ou d‚Äôoutliers, elles peuvent influencer les estimations de $\mu$ et $\Sigma$.



<br>
<br>

---

### 2. Isolation Forest
L'*isolation forest* est une m√©thode efficace pour la d√©tection d'anomalies qui repose sur l'id√©e que les anomalies ou points atypiques sont plus faciles √† isoler que les points normaux dans un ensemble de donn√©es car **les anomalies sont rares et diff√©rentes**.   
L'isolation forest exploite ces deux caract√©ristiques en construisant plusieurs arbres de s√©paration (arbres d'isolement) et en mesurant combien de coupures sont n√©cessaires pour isoler chaque point:

#### a. **Construction des arbres d'isolement:**
   - Chaque arbre est construit en divisant r√©cursivement l'ensemble de donn√©es le long de dimensions choisies al√©atoirement.
   - √Ä chaque division, un seuil est choisi al√©atoirement dans la plage de valeurs de la dimension s√©lectionn√©e.
   - Un arbre est construit jusqu'√† ce que:
     - Chaque point soit isol√© dans une feuille, **ou**
     - Une profondeur maximale soit atteinte.

#### b. **R√©p√©tition avec plusieurs arbres:**
   - Pour obtenir des r√©sultats robustes, on construit plusieurs arbres (g√©n√©ralement entre 100 et 1000).

#### c. **Calcul de la profondeur moyenne:**
   - Pour chaque point, on mesure la profondeur moyenne √† laquelle il est isol√© dans tous les arbres.
   - Un point facilement isol√© (faible profondeur) est consid√©r√© comme une anomalie.
   - Les points qui n√©cessitent beaucoup de divisions (profondeur √©lev√©e) sont consid√©r√©s comme normaux.

#### d. **Score d'anomalie:**
   - Le score est une fonction inverse de la profondeur moyenne.
   - Un score √©lev√© indique un point probablement anormal.

#### **Avantages de l'isolation forest:**
1. **Efficacit√©:**
   - Complexit√© lin√©aire (O(n)) en termes de taille des donn√©es, ce qui en fait une m√©thode adapt√©e aux grands ensembles de donn√©es.
2. **Peu d'hyperparam√®tres:**
   - Nombre d‚Äôarbres ($n_{\text{trees}}$) et sous-√©chantillon (subsample) sont les principaux param√®tres.
   - Pas besoin de normaliser ou standardiser les donn√©es.
3. **Flexibilit√©:**
   - Fonctionne bien pour les ensembles de donn√©es de grande dimension et h√©t√©rog√®nes.
4. **D√©tection automatique:**
   - L‚Äôalgorithme ne n√©cessite pas de mod√©lisation explicite des donn√©es normales ou anormales.


<br>



```python
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import GridSearchCV

clf_name='Isolation Forest'
clf = IsolationForest()

param_grid = {
    'n_estimators': [100, 200, 300, 500],
    'max_samples': [0.1, 0.5, 1.0],
    'contamination': [0.05, 0.1, 0.2, 'auto'],
    'max_features': [1, 2]
}

grid_search = GridSearchCV(
    estimator=clf,
    param_grid=param_grid,
    scoring=custom_scorer,
    cv=3,
    verbose=True,
    n_jobs=-1
)

grid_search.fit(df_2d[['X1', 'X2']])

best_params = grid_search.best_params_
print(f"Meilleurs hyperparam√®tres: {best_params}")

best_clf = IsolationForest(**best_params)

y_pred_isolation_forest = best_clf.fit_predict(df_2d[['X1', 'X2']])

plotAnomalies2D(df_2d, clf_name, best_clf)
```

#### **Param√®tres cl√©s dans `IsolationForest`:**
1. **`n_estimators`**: Nombre d‚Äôarbres dans la for√™t. Un plus grand nombre am√©liore la robustesse, mais augmente le temps de calcul.
2. **`max_samples`**: Taille du sous-√©chantillon utilis√© pour construire chaque arbre. Par d√©faut, utilise \( \min(256, \text{taille totale des donn√©es}) .
3. **`contamination`**: Proportion attendue d‚Äôanomalies dans les donn√©es. Si inconnue, choisissez une valeur comme 0.05.
4. **`random_state`**: Pour la reproductibilit√© des r√©sultats.


#### **Limitations:**
1. **Sensibilit√© au param√®tre de contamination:**
   - Si la proportion d'anomalies est mal estim√©e, cela peut affecter les r√©sultats.
2. **Risque d'erreurs pour des donn√©es tr√®s complexes:**
   - Les anomalies proches des clusters normaux peuvent passer inaper√ßues.
3. **Moins efficace pour des distributions non tabulaires:**
   - Peut avoir du mal √† traiter des relations tr√®s non lin√©aires entre les variables.


<br>
<br>

---

### 3. Local Outlier Factor 
Le *Local Outlier Factor* (LOF) est une m√©thode de d√©tection d'anomalies bas√©e sur une analyse locale de la densit√©. Contrairement aux m√©thodes globales, qui comparent chaque point √† l'ensemble des donn√©es, le LOF compare la densit√© locale d'un point √† celle de ses voisins proches. Un point est consid√©r√© comme une anomalie si sa densit√© locale est significativement plus faible que celle de ses voisins:

#### 1. **Distance $k$-plus-proche-voisin ($k$-NN):**
   - La distance $k$-NN d'un point est la distance au $k$-i√®me plus proche voisin. Cette distance est utilis√©e pour d√©finir la "localit√©".

#### 2. **Distance atteignable:**
   - La distance atteignable entre deux points $A$ et $B$ est d√©finie comme:
    $ \text{distance atteignable}(A, B) = \max(\text{distance k-NN de B}, \text{distance euclidienne}(A, B))$ 
   - Cela permet d'att√©nuer l'effet des points tr√®s proches dans des zones de forte densit√©.

#### 3. **Densit√© locale atteignable:**
   - La densit√© locale atteignable d‚Äôun point $A$ est inversement proportionnelle √† la distance moyenne atteignable entre $A$ et ses $k$-plus-proches-voisins:
    $\text{densit√© locale atteignable}(A) = \frac{k}{\sum_{B \in \text{voisins de } A} \text{distance atteignable}(A, B)}$

#### 4. **Facteur de d√©tection locale (LOF):**
   - Le LOF d‚Äôun point $A$ est le rapport entre la densit√© locale moyenne de ses $k$-plus-proches-voisins et sa propre densit√© locale atteignable:
    $\text{LOF}(A) = \frac{\text{moyenne des densit√©s locales des voisins de } A}{\text{densit√© locale atteignable}(A)}$
   - Si $ \text{LOF}(A) \approx 1 $, le point est normal.
   - Si $ \text{LOF}(A) > 1 $, le point est un outlier. Plus le LOF est grand, plus le point est isol√©.

```python
from sklearn.neighbors import LocalOutlierFactor

clf_name = 'Local Outlier Factor'
clf = LocalOutlierFactor(n_neighbors=5, contamination=0.5, novelty=True)
clf.fit(df[['X1','X2']])

plotAnomalies2D(df_2d, clf_name, clf)
```

#### **Param√®tres importants:**

1. **`n_neighbors`:**
   - Nombre de voisins √† consid√©rer. Influence directement la "localit√©". Un $k$ plus grand favorise une √©valuation plus globale, un $k$ plus petit favorise une d√©tection plus locale.

2. **`contamination`:**
   - Fraction estim√©e des anomalies dans les donn√©es. Permet de calibrer le seuil de d√©tection.

3. **`metric`:**
   - M√©thode utilis√©e pour calculer les distances (euclidienne par d√©faut, mais d‚Äôautres m√©triques comme Minkowski ou Mahalanobis sont possibles).

#### **Avantages du LOF**

1. **Analyse locale:**
   - Contrairement aux m√©thodes globales (comme l‚ÄôIsolation Forest), le LOF d√©tecte des anomalies qui peuvent √™tre "normales" √† l'√©chelle globale, mais inhabituelles dans leur voisinage local.

2. **Non-param√©trique:**
   - Pas besoin de supposer une distribution particuli√®re des donn√©es.

3. **Adaptabilit√©:**
   - Fonctionne bien avec des donn√©es h√©t√©rog√®nes ou ayant des densit√©s variables.


#### **Inconv√©nients du LOF**

1. **Sensibilit√© au choix de $k$:**
   - Le nombre de voisins $k$ influence directement les r√©sultats. Une mauvaise s√©lection peut entra√Æner des erreurs.

2. **Complexit√©:**
   - La complexit√© temporelle est quadratique ($O(n^2)$) pour le calcul des distances, bien que des optimisations ($k$-d arbres, approximations) r√©duisent ce co√ªt.

3. **Difficult√© en haute dimension:**
   - Comme beaucoup d'algorithmes bas√©s sur les distances, le LOF souffre de la "mal√©diction de la dimensionnalit√©".



<br>
<br>

---

### 4. One Class SVM
Le *One-Class SVM* est l'extension des machines √† vecteurs de support (SVM) adapt√©e aux cas o√π l'on dispose uniquement de donn√©es normales pour l'entra√Ænement, sans exemples d'anomalies:

#### 1. **Projection dans un espace √† haute dimension:**
   - Les donn√©es sont transform√©es via une fonction noyau pour mieux capturer des structures complexes.

#### 2. **Optimisation:**
   - Le mod√®le optimise un hyperplan ou une hypersph√®re qui s√©pare les donn√©es normales du reste, en minimisant une fonction objectif qui √©quilibre la marge et le nombre de points rejet√©s.

#### 3. **Pr√©diction:**
   - Apr√®s l'entra√Ænement, le mod√®le attribue une √©tiquette √† chaque point:
    - $+1$: Donn√©es normales (inliers).
    - $-1$: Anomalies (outliers).
    
```python
from sklearn.svm import OneClassSVM

clf_name = 'One Class SVM'
clf = OneClassSVM(nu=0.15, kernel="rbf", gamma=0.33)
clf.fit(df_2d[['X1','X2']])

y_pred_one_class_svm = clf.fit_predict(df_2d[['X1','X2']])

plotAnomalies2D(df_2d, clf_name, clf)
```

#### **Param√®tres importants:**

1. **`kernel`:**
   - D√©finit le type de noyau utilis√© pour capturer la fronti√®re:
     - `"linear"`: Hyperplan lin√©aire.
     - `"rbf"` (par d√©faut): Non-lin√©aire, adapt√© aux distributions complexes.

2. **`nu`:**
   - Fraction des points rejet√©s comme anomalies. Correspond √† la contamination attendue dans les donn√©es.

3. **`gamma`:**
   - Param√®tre de forme du noyau radial. Contr√¥le la courbure de la fronti√®re.

4. **`contamination` (post-analyse):**
   - Si besoin, ajustez la proportion des anomalies apr√®s entra√Ænement pour optimiser les pr√©dictions.

#### **Avantages du One-Class SVM**

1. **Capacit√© √† mod√©liser des distributions complexes:**
   - Gr√¢ce √† l‚Äôutilisation de noyaux, il peut capturer des fronti√®res non lin√©aires.

2. **Apprentissage non supervis√©:**
   - Ne n√©cessite que des donn√©es normales pour s‚Äôentra√Æner.

3. **Robustesse:**
   - Fonctionne bien avec des distributions complexes et des dimensions √©lev√©es.


#### **Limitations du One-Class SVM**

1. **Sensibilit√© au choix des hyperparam√®tres:**
   - Les performances d√©pendent fortement de la s√©lection du noyau, du param√®tre $ \nu $ (contamination) et de $ \gamma $ (influence du noyau).

2. **Complexit√©:**
   - Sa complexit√© temporelle est quadratique √† cubique $ O(n^2) $ √† $ O(n^3) $, ce qui le rend co√ªteux pour de grands ensembles de donn√©es.

3. **Difficult√© avec des donn√©es d√©s√©quilibr√©es:**
   - Si une grande proportion d‚Äôanomalies est pr√©sente dans les donn√©es d‚Äôentra√Ænement, cela peut nuire aux performances.

   
<br>
<br>

---

### 5. Minimum covariance determinent
La m√©thode du **D√©terminant de Covariance Minimale (MCD)** est une technique robuste utilis√©e en statistique, notamment pour l'estimation de param√®tres dans des distributions multivari√©es et pour la d√©tection de valeurs aberrantes:

#### **a. S√©lection al√©atoire d'un sous-ensemble de points de donn√©es**
- On commence par choisir al√©atoirement un sous-ensemble $\mathcal{h}$ de points de donn√©es parmi l'ensemble total $\mathcal{n}$.
- Typiquement, $\mathcal{h}$ est choisi pour √™tre suffisamment grand pour permettre une estimation robuste, mais plus petit que $\mathcal{n}$, afin de limiter l'influence des valeurs aberrantes.


#### **b. Calculer la matrice de covariance et son d√©terminant**
- Pour chaque sous-ensemble s√©lectionn√©, on calcule:
  - La **moyenne** vectorielle $\mu_h$ des points du sous-ensemble.
  - La **matrice de covariance**  $S_h$  du sous-ensemble.
  - Le **d√©terminant de la matrice de covariance**  $\det(S_h)$ , qui refl√®te le volume de la distribution estim√©e √† partir du sous-ensemble.

> **Pourquoi le d√©terminant?**  
Le d√©terminant de la matrice de covariance mesure l'√©tendue ou la "dispersion" des points dans l'espace. **Plus ce d√©terminant est petit, plus la distribution estim√©e est compacte.**


#### **c. R√©p√©ter plusieurs fois et conserver le d√©terminant le plus petit**
- On r√©p√®te les √©tapes 1 et 2 un grand nombre de fois avec diff√©rents sous-ensembles al√©atoires.
- Finalement, on conserve le sous-ensemble  $\mathcal{h}$  qui donne le **plus petit d√©terminant**  $\det(S_h)$.     
Cela permet de s√©lectionner un sous-ensemble qui capture le c≈ìur des donn√©es, en minimisant l'effet des valeurs aberrantes.


#### **d. Calculer la distance de Mahalanobis**
- √Ä partir de la moyenne $\mu_h$ et de la matrice de covariance $S_h$ obtenues √† l'√©tape pr√©c√©dente, on peut calculer la **distance de Mahalanobis**  $d_M$  pour chaque observation $x$ de l'ensemble initial.

La distance de Mahalanobis est d√©finie comme: $d_M(x) = \sqrt{(x - \mu_h)^T S_h^{-1} (x - \mu_h)}$ 


> **Interpr√©tation**: Cette distance mesure la "proximit√©" d'une observation par rapport √† la distribution estim√©e. Elle tient compte de la corr√©lation entre les variables, contrairement √† une simple distance euclidienne.


#### **e. D√©finir un seuil pour identifier les valeurs aberrantes**
- Une fois les distances de Mahalanobis calcul√©es pour toutes les observations, il faut d√©finir un seuil pour distinguer les observations normales des valeurs aberrantes.
- Les observations dont la distance d√©passe ce seuil (souvent bas√© sur une distribution  $\chi^2$  avec  $p$  degr√©s de libert√©, o√π $p$ est le nombre de variables) sont consid√©r√©es comme des **outliers**.


```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import chi2

def covariance_matrix(X):
   return np.cov(X.T)  

def mahalanobis_distance(X, mean, cov_matrix):
   inv_cov = np.linalg.inv(cov_matrix + np.eye(cov_matrix.shape[0]) * 1e-6)
   diffs = X - mean
   # Calling diagonal to take only relative measure of distance with regards to the mean of distribution, not comparing to others
   return np.sqrt(np.diag(np.dot(np.dot(diffs, inv_cov), diffs.T)))


# MCD implementation
def mcd(X, h, n_iterations=100):
   np.random.seed(42)
   best_det = float('inf')
   best_subset = None
   
   # Random selection of subsets and optimisation
   for _ in range(n_iterations):
      subset = X[np.random.choice(X.shape[0], h, replace=False)]
      cov_matrix = covariance_matrix(subset)
      det = np.linalg.det(cov_matrix)
      if det < best_det:
         best_det = det
         best_subset = subset
   
   # Calculation of the mean and covariance matrix for the best subset
   best_mean = np.mean(best_subset, axis=0)
   best_cov = covariance_matrix(best_subset)
   
   # Calculation of Mahalanobis distances for all observations
   distances = mahalanobis_distance(X, best_mean, best_cov)
   return distances


data = np.array([X1, X2]).T

# Standardisation of data to avoid scale effects
data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)

# Threshold based on the œá¬≤ law:
threshold = chi2.ppf(0.775, df=data.shape[1])

# Threshold 85·µâ centile:
threshold = np.percentile(distances, 85)


# Test with different values of h
h_values = [int(data.shape[0] * frac) for frac in [0.5, 0.6, 0.7]]

fig, axs = plt.subplots(1, len(h_values), figsize=(15, 5), sharex=True, sharey=True)

for i, h in enumerate(h_values):
   distances = mcd(data, h)
   outliers_pred = distances > threshold
   print(f"h={h}, nombre d'outliers: {np.sum(outliers_pred)}")
   
   # Display results
   axs[i].scatter(data[:, 0], data[:, 1], c=outliers_pred, cmap='coolwarm')
   axs[i].set_title(f"h={h}, Outliers={np.sum(outliers_pred)}")
   axs[i].set_xlabel('X1')
   axs[i].set_ylabel('X2')

plt.tight_layout()
plt.show()

# Histogram of Mahalanobis distances
for h in h_values:
   distances = mcd(data, h)
   plt.figure(figsize=(22, 8))
   plt.hist(distances, bins=30, color='skyblue', alpha=0.7, label=f"h={h}")
   plt.axvline(threshold, color='red', linestyle='--', label=f'Seuil={threshold:.2f}')
   plt.title(f"Histogramme des distances Mahalanobis (h={h})")
   plt.xlabel("Distance")
   plt.ylabel("Fr√©quence")
   plt.legend()
   plt.show()
```

Le MCD est particuli√®rement adapt√© dans les contextes suivants:
1. **Robustesse**: Il r√©duit l'impact des valeurs aberrantes sur les estimations.
2. **Fiabilit√©**: L'utilisation du d√©terminant garantit que l'estimation est fond√©e sur le c≈ìur dense des donn√©es.
3. **Applications pratiques**: Il est souvent utilis√© dans l'identification d'anomalies, la classification robuste, et la d√©tection d'observations atypiques dans les ensembles de donn√©es multivari√©es.


### **Comparaison des m√©thodes:**

| M√©thode               | Nature               | Avantages                              | Limites                              |
|-----------------------|----------------------|---------------------------------------|--------------------------------------|
| **One-Class SVM**     | Globale, Kernel      | Mod√©lise des fronti√®res complexes     | Sensible aux hyperparam√®tres         |
| **Isolation Forest**  | Globale             | Rapide, robuste, non param√©trique     | Moins efficace sur de petites donn√©es|
| **Local Outlier Factor** | Locale              | D√©tecte des anomalies locales         | Sensible √† $k$, co√ªteux en calcul  |
| **Elliptic Envelope** | Globale             | Simple, efficace si donn√©es gaussiennes | Inefficace pour des distributions non-gaussiennes |


<br>
<br>
<br>

## D√©tection de nouveaut√©
