<h1 align='center'> Deep Learning - AE & VAE  ü§≥ </h1>

Les autoencodeurs sont un type particulier de r√©seau neuronal non supervis√© qui apprend √† repr√©senter les donn√©es d'entr√©e dans un espace de dimension r√©duite (appel√© **espace latent**) avant de les reconstruire. 


## **I. Introduction aux Autoencodeurs (AE)**

Il est possible de consid√©rer que les autoencodeurs sont compos√©s de deux r√©seaux, un **encodeur** $e$ et un **d√©codeur** $d$.

L'encodeur apprend une transformation non lin√©aire $e: X$ √† $Z$ qui projette les donn√©es de l'espace d'entr√©e original √† haute dimension $X$ vers un **espace latent** √† plus basse dimension $Z$. Soit $z = e(x)$ un **vecteur latent**. Un vecteur latent est une repr√©sentation √† faible dimension d'un point de donn√©es qui contient des informations sur $x$. La transformation $e$ doit avoir certaines propri√©t√©s, comme des valeurs similaires de $x$ doivent avoir des vecteurs latents similaires (et des valeurs dissemblables de $x$ doivent avoir des vecteurs latents dissemblables).

Un d√©codeur apprend une transformation non lin√©aire $d: Z$ √† $X$ qui projette les vecteurs latents dans l'espace d'entr√©e original √† haute dimension $X$. Cette transformation doit prendre le vecteur latent $z = e(x)$ et reconstruire les donn√©es d'entr√©e originales $\hat{x} = d(z) = d(e(x))$.

Un autoencodeur est simplement la composition du codeur et du d√©codeur $f(x) = d(e(x))$. L'autoencodeur est entra√Æn√© pour minimiser la diff√©rence entre l'entr√©e $x$ et la reconstruction $\hat{x}$ en utilisant une sorte de **perte de reconstruction**. Comme l'autoencodeur est entra√Æn√© dans son ensemble (entra√Æn√© "de bout en bout"), l'optimisation se fait simultan√©ment sur l'encodeur et le d√©codeur.


<h3 align='center'>
    <img src="img/autoencoder.png">
</h3>

<br>

### 1. **Architecture des Autoencodeurs**

Un autoencodeur se compose de deux parties principales:
1. **Encodeur**: 
   - R√©duit les donn√©es d'entr√©e dans un espace latent de dimension inf√©rieure.
   - $z = e(x)$, o√π $z$ est la repr√©sentation latente.

2. **D√©codeur**:
   - Reconstruit les donn√©es d'entr√©e √† partir de la repr√©sentation latente.
   - $\hat{x} = d(z)$, o√π $\hat{x}$ est la reconstruction de $x$.

La t√¢che de l'autoencodeur est d'optimiser la reconstruction, c'est-√†-dire de minimiser la perte entre $x$ et $\hat{x}$.

---

### **Fonction de Perte**

L'objectif est de minimiser la distance entre les donn√©es originales $x$ et la reconstruction $\hat{x}$. Typiquement, via la **Mean Squared Error (MSE)** ou la **Binary Cross-Entropy (BCE)** selon les donn√©es:
$$\mathcal{L} = \| x - \hat{x} \|^2 \quad \text{(MSE)}$$


<br>


### 2. **Code en PyTorch pour un Autoencodeur**

Voici un exemple d'autoencodeur simple pour les images MNIST:

```python
import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Hyperparam√®tres
input_dim = 28 * 28
hidden_dim = 128
latent_dim = 32
batch_size = 64
epochs = 10
learning_rate = 1e-3
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Mod√®le
class Autoencoder(nn.Module):
    def __init__(self):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()  # Pour des donn√©es normalis√©es entre 0 et 1
        )

    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return reconstructed

# Charger les donn√©es MNIST
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: x.view(-1))  # Aplatir les images
])
dataloader = DataLoader(datasets.MNIST('./data', download=True, transform=transform), batch_size=batch_size, shuffle=True)

# Initialisation
model = Autoencoder().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.MSELoss()

# Entra√Ænement
for epoch in range(epochs):
    for imgs, _ in dataloader:
        imgs = imgs.to(device)
        outputs = model(imgs)
        loss = criterion(outputs, imgs)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}")
```

<br>

### 3. **Applications des Autoencodeurs**

1. **R√©duction de dimension**:
   - Similaire √† PCA, mais **non lin√©aire**.
2. **D√©noising**:
   - Reconstruction des donn√©es propres √† partir de donn√©es bruit√©es (*Denoising Autoencoder*).
3. **Compression d'images**:
   - Codage dans un espace latent pour des repr√©sentations plus compactes.
4. **Pr√©-entrainement des r√©seaux**:
   - Apprentissage des repr√©sentations utiles.




<br>
<br>

## **II. Introduction aux Variational Autoencoders (VAEs)**

Un **Variational Autoencoder (VAE)** est une extension probabiliste des autoencodeurs. Il ne se contente pas de compresser les donn√©es,  il impose une structure probabiliste √† l'espace latent.

La seule contrainte sur la repr√©sentation du vecteur latent pour les autoencodeurs traditionnels est que les vecteurs latents doivent √™tre facilement d√©codables dans l'image originale. En cons√©quence, l'espace latent $Z$ peut devenir disjoint et non continu. Les autoencodeurs variationnels tentent de r√©soudre ce probl√®me.

Dans les autoencodeurs traditionnels, les entr√©es sont mises en correspondance de mani√®re d√©terministe avec un vecteur latent $z = e(x)$. Dans les autoencodeurs variationnels, les entr√©es sont mises en correspondance avec une distribution de probabilit√© sur les vecteurs latents, et un vecteur latent est ensuite √©chantillonn√© √† partir de cette distribution. Le d√©codeur devient ainsi plus robuste pour d√©coder les vecteurs latents. 

Plus pr√©cis√©ment, au lieu de mettre en correspondance l'entr√©e $x$ avec un vecteur latent $z = e(x)$, elle est mise en correspondance avec une distribution de probabilit√©s $p(z|x)$. Pour caract√©riser cette distribution, une distribution normale est utilis√©e et le vecteur moyen $\mu(x)$ et un vecteur d'√©carts types $\sigma(x)$ sont calcul√©s. Ceux-ci param√®trent une distribution gaussienne diagonale $p(z|x) = \mathcal{N}(\mu_x, \sigma_x)$, √† partir de laquelle un vecteur latent $z \sim \mathcal{N}(\mu_x, \sigma_x)$ sera √©chantillon√©.

Pour ce faire, il faut modifier la partie codeur de l'autoencodeur pour qu'elle produise $\mu(x)$ et $\sigma(x)$ dans deux couches distinctes. Une activation exponentielle est souvent ajout√©e √† $\sigma(x)$ pour s'assurer que le r√©sultat est positif.

<h3 align='center'>
    <img src="img/variational-autoencoder.png">
</h3>


### 1. **Architecture des VAE**

1. **Encodeur**:
   - G√©n√®re deux sorties: la **moyenne** ($\mu$) et la **variance** ($\sigma^2$) d'une distribution latente.
   - $z \sim \mathcal{N}(\mu, \sigma^2)$.

2. **D√©codeur**:
   - Reconstruit les donn√©es √† partir de $z$, √©chantillonn√© depuis la distribution latente: $$\hat{x} = d(z)$$


<br>


Toutefois, cela ne r√©sout pas compl√®tement le probl√®me. Il peut toujours y avoir des lacunes dans l'espace latent car les moyennes produites peuvent √™tre significativement diff√©rentes et les √©carts-types peuvent √™tre faibles. Pour r√©duire ce probl√®me, il faut ajouter un terme de perte de r√©gularisation qui p√©nalise la distribution $p(z \mid x)$ pour √™tre trop √©loign√©e de la distribution normale standard $\mathcal{N}(0, 1)$. 

Ce terme de p√©nalisation est la [divergence de KL](https://fr.wikipedia.org/wiki/Divergence_de_Kullback-Leibler) entre $p(z \mid x) = \mathcal{N}(\mu_x, \sigma_x)$ et $\mathcal{N}(0, 1)$, qui est donn√©e par
$$\mathcal{L}_{KL} = D_{KL}(q(z|x) \| p(z)) = \frac{1}{2} \sum \left( 1 + \log(\sigma^2) - \mu^2 - \sigma^2 \right)$$
Cela permet de **contraindre la distribution latente √† √™tre proche d'une gaussienne standard**. En substance, il faut forcer le codeur √† trouver des vecteurs latents qui suivent approximativement une distribution gaussienne standard que le d√©codeur peut alors d√©coder efficacement.

Il faut de plus, ajouter une **erreur de reconstruction** √† la perte. Soit la similarit√© entre $x$ et $\hat{x}$:
   $$\mathcal{L}_{reconstruction} = \| x - \hat{x} \|^2$$
   

#### La perte totale est donc:
$$\mathcal{L} = \mathcal{L}_{reconstruction} + \mathcal{L}_{KL}$$
$$\mathcal{L} = \| x - \hat{x} \|^2 + \frac{1}{2} \sum \left( 1 + \log(\sigma^2) - \mu^2 - \sigma^2 \right)$$


<br>

### 2. **Code en PyTorch pour un VAE**


```python
class VAE(nn.Module):
    def __init__(self, input_dim, hidden_dim, latent_dim):
        super(VAE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        self.mu = nn.Linear(hidden_dim, latent_dim)
        self.log_var = nn.Linear(hidden_dim, latent_dim)
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )

    def reparameterize(self, mu, log_var):
        std = torch.exp(0.5 * log_var)
        eps = torch.randn_like(std)
        return mu + eps * std

    def forward(self, x):
        h = self.encoder(x)
        mu = self.mu(h)
        log_var = self.log_var(h)
        z = self.reparameterize(mu, log_var)
        reconstructed = self.decoder(z)
        return reconstructed, mu, log_var

# Initialisation
vae = VAE(input_dim, hidden_dim, latent_dim).to(device)
optimizer = torch.optim.Adam(vae.parameters(), lr=learning_rate)

# Entra√Ænement
for epoch in range(epochs):
    for imgs, _ in dataloader:
        imgs = imgs.to(device)
        reconstructed, mu, log_var = vae(imgs)

        # Pertes
        reconstruction_loss = nn.MSELoss()(reconstructed, imgs)
        kl_divergence = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
        loss = reconstruction_loss + kl_divergence

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}")
```

Les autoencodeurs variationnels produisent un espace latent $Z$ qui est r√©gularis√© pour √™tre plus compact et plus lisse que celui appris par les autoencodeurs traditionnels. Cela permet d'√©chantillonner au hasard des points $z \sim Z$ et de produire des reconstructions correspondantes $\hat{x} = d(z)$ qui forment des chiffres r√©alistes, contrairement aux autoencodeurs traditionnels.

---

### 3. **Applications des VAE**

1. **G√©n√©ration de donn√©es**: 
   - Produire des images ou des √©chantillons similaires aux donn√©es d'entr√©e.
2. **Mod√©lisation de la distribution latente**:
   - Capturer une distribution probabiliste sur les donn√©es.
3. **Interpolations**:
   - Cr√©er des transitions fluides entre des points dans l'espace latent.
4. **R√©duction de dimension**:
   - Identifier des structures sous-jacentes dans les donn√©es.


<br>
<br>

## III. R√©sum√© des Diff√©rences entre AE et VAE

| **Autoencodeurs (AE)** | **Variational Autoencoders (VAE)** |
|-------------------------|-----------------------------------|
| Latent space d√©terministe | Latent space probabiliste |
| Optimise uniquement la reconstruction | Ajoute une r√©gularisation via divergence KL |
| Non probabiliste | Probabiliste, capable de g√©n√©rer des donn√©es nouvelles |