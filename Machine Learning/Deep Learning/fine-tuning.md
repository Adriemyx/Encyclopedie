<h1 align='center'> Fine-Tuning pour le DeepLearning </h1>

Le **Fine-Tuning** est une technique d'apprentissage profond qui consiste √† ajuster un mod√®le pr√©-entra√Æn√© sur une nouvelle t√¢che sp√©cifique.  


## 1. Freezing & Unfreezing Progressif
- **Phase 1**: Geler toutes les couches du mod√®le sauf les couches de classification.  
- **Phase 2**: D√©bloquer progressivement certaines couches du r√©seau et fine-tuner avec un LR plus faible.  

Cela permet d'emp√™cher d'apprendre des poids inutiles au d√©but et de conserver les features pr√©-entra√Æn√©es.  

### **Impl√©mentation (TensorFlow/Keras)**
```python
# Phase 1: Geler toutes les couches du mod√®le
base_model.trainable = False
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_generator, epochs=5)

# Phase 2: D√©bloquer certaines couches et fine-tuner
base_model.trainable = True
for layer in base_model.layers[:100]:  # Geler les 100 premi√®res couches
    layer.trainable = False

# Diminuer le LR pour √©viter d'endommager les poids pr√©-entra√Æn√©s
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_generator, epochs=5)
```

<br>


## 2. Ajustement du Learning Rate (LR) et Schedulers
Le **learning rate** (taux d‚Äôapprentissage) d√©termine √† quelle vitesse le mod√®le ajuste ses poids.  
- **Trop grand** üèÉüí® ‚Üí Convergence instable, risque d‚Äôoscillations.  
- **Trop petit** üê¢ ‚Üí Convergence tr√®s lente, risque de rester bloqu√© dans un minimum local.  

Dans le **fine-tuning**, il est commun d'utiliser un **LR plus faible** que celui utilis√© lors du pr√©-entra√Ænement.    
Un *scheduler* ajuste **dynamiquement** le LR pendant l'entra√Ænement pour am√©liorer la convergence.  

###  **2.1. ReduceLROnPlateau (Adaptatif)**
- R√©duit le LR lorsque la validation loss **cesse de diminuer**.  
- √âvite un LR trop √©lev√© lorsque le mod√®le stagne.  

**üîπ Impl√©mentation (TensorFlow/Keras):**  
```python
from tensorflow.keras.callbacks import ReduceLROnPlateau

# R√©duit le LR si la validation loss ne diminue pas apr√®s 3 epochs
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)

# Ajout dans model.fit()
model.fit(train_generator, validation_data=val_generator, epochs=20, callbacks=[lr_scheduler])
```

**üîπ Impl√©mentation (PyTorch):**  
```python
import torch.optim.lr_scheduler as lr_scheduler

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)

for epoch in range(20):
    train_loss = train_one_epoch()  # Fonction d'entra√Ænement
    val_loss = validate()  # Fonction de validation
    
    scheduler.step(val_loss)  # Ajuste le LR si val_loss stagne
```

---

### **2.2. StepLR (D√©croissance en paliers)**
- Diminue le LR apr√®s un nombre d'epochs fixe (ex. chaque 10 epochs).  
- Simple mais efficace !  

**üîπ Impl√©mentation (PyTorch):**  
```python
scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

for epoch in range(30):
    train_one_epoch()
    scheduler.step()  # Diminue le LR toutes les 10 epochs
```

---

### **3. Cosine Annealing (Scheduler avanc√©)**
- **Diminution progressive** du LR en suivant une courbe cosinus.  
- Utilis√© souvent avec **SGD + Momentum**.  

**üîπ Impl√©mentation (PyTorch):**  
```python
scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)

for epoch in range(50):
    train_one_epoch()
    scheduler.step()  # LR suit une courbe cosinus
```

<br>

## **3. Utilisation du Dropout**
- Technique qui **d√©sactive al√©atoirement** des neurones pendant l'entra√Ænement.  
- Emp√™che le r√©seau de **trop s‚Äôadapter** aux donn√©es d'entra√Ænement (**overfitting**).  
- G√©n√©ralement utilis√© **avant les couches fully connected**.

**Dans le fine-tuning, on peut augmenter le dropout pour compenser le risque d‚Äôoverfitting sur peu de donn√©es.**  

**Impl√©mentation (TensorFlow/Keras)**
```python
from tensorflow.keras.layers import Dropout

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)  # D√©sactive 50% des neurones
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)  # D√©sactive 30% des neurones
output = Dense(2, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)
```

**Impl√©mentation (PyTorch)**
```python
import torch.nn as nn

class CustomModel(nn.Module):
    def __init__(self, base_model):
        super(CustomModel, self).__init__()
        self.base = base_model
        self.dropout = nn.Dropout(0.5)  # Dropout de 50%
        self.fc = nn.Linear(512, 2)  # Derni√®re couche pour classification

    def forward(self, x):
        x = self.base(x)
        x = self.dropout(x)  # Appliquer le dropout
        x = self.fc(x)
        return x

model = CustomModel(models.resnet18(pretrained=True))
```


<br>

## 4. Label Smoothing
- **Emp√™che le mod√®le d‚Äô√™tre trop s√ªr de ses pr√©dictions.**  
- Modifie l√©g√®rement les labels:  
  - Au lieu d‚Äôavoir **1** pour la classe correcte, on met **0.9**.  
  - Au lieu d‚Äôavoir **0** pour les autres classes, on met **0.1 / (nombre de classes - 1)**.  

**Impl√©mentation (PyTorch)**
```python
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
```

<br>

## 5. Data augmentation
La *data augmentation* (augmentation des donn√©es) est une technique utilis√©e pour am√©liorer la performance des mod√®les d'apprentissage automatique en augmentant artificiellement la taille et la diversit√© du jeu de donn√©es d‚Äôentra√Ænement. Elle consiste √† appliquer diverses transformations sur les images originales, ce qui permet d'√©viter le surapprentissage (overfitting) et de rendre le mod√®le plus robuste face aux variations des donn√©es r√©elles.

Les transformations classiques incluent:
- **Rotation**: Faire tourner l'image d'un certain angle.
- **Zoom**: Appliquer un zoom avant ou arri√®re sur l'image.
- **Translation**: D√©placer l'image horizontalement ou verticalement.
- **Miroir**: Retourner l'image horizontalement ou verticalement.
- **Perturbation de couleur**: Modifier la luminosit√©, le contraste ou la saturation des images.

Ces transformations permettent au mod√®le d'apprendre √† reconna√Ætre les objets sous diff√©rentes perspectives et conditions, rendant ainsi le mod√®le plus g√©n√©raliste et mieux adapt√© √† des donn√©es r√©elles. L'utilisation de la data augmentation est particuli√®rement b√©n√©fique lorsqu'il y a peu de donn√©es d‚Äôentra√Ænement disponibles.


### Impl√©mentation (PyTorch)
```python
import torch
from torchvision import transforms
from PIL import Image

# D√©finition des transformations
data_augmentation = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # Retourner l'image horizontalement
    transforms.RandomRotation(30),      # Rotation al√©atoire de l'image entre -30 et 30 degr√©s
    transforms.RandomResizedCrop(224),  # Recadrage al√©atoire de l'image avec mise √† l'√©chelle
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Perturbation des couleurs
    transforms.ToTensor(),              # Conversion de l'image en tensor
])

# Chargement de l'image
image = Image.open('image.jpg')

# Application des transformations
augmented_image = data_augmentation(image)

# Affichage de l'image transform√©e (optionnel)
import matplotlib.pyplot as plt
plt.imshow(augmented_image.permute(1, 2, 0))  # Permute les dimensions pour l'affichage
plt.show()
```

### Explication des transformations:
1. **RandomHorizontalFlip()**: Retourne l'image horizontalement avec une probabilit√© de 50%.
2. **RandomRotation(30)**: Fait une rotation al√©atoire de l'image entre -30 et +30 degr√©s.
3. **RandomResizedCrop(224)**: Recadre l'image de mani√®re al√©atoire tout en conservant une taille de 224x224 pixels apr√®s le recadrage.
4. **ColorJitter()**: Modifie de mani√®re al√©atoire la luminosit√©, le contraste, la saturation et la teinte de l'image pour diversifier les couleurs.
5. **ToTensor()**: Convertit l'image PIL en un tensor PyTorch pour l'entra√Ænement.


<br>

## 6. Mixup & CutMix (Augmentation Avanc√©e)
- **Mixup**: M√©lange deux images et leurs labels correspondants.  
- **CutMix**: Remplace une partie d‚Äôune image par une autre image du dataset.  

### **Impl√©mentation (TensorFlow/Keras)**
```python
import tensorflow as tf

def mixup(image, label, alpha=0.2):
    batch_size = tf.shape(image)[0]
    weights = tfp.distributions.Beta(alpha, alpha).sample([batch_size, 1, 1, 1])
    image_shuffled = tf.random.shuffle(image)
    label_shuffled = tf.random.shuffle(label)
    
    mixed_image = weights * image + (1 - weights) * image_shuffled
    mixed_label = weights * label + (1 - weights) * label_shuffled
    return mixed_image, mixed_label
```

<br>

## 7. Stochastic Weight Averaging (SWA)
- Au lieu de prendre les poids du dernier epoch, **moyenner plusieurs mod√®les entra√Æn√©s**.  
- Donne de meilleures performances et une meilleure g√©n√©ralisation.  

### **Impl√©mentation (PyTorch)**
```python
from torch.optim.swa_utils import AveragedModel

swa_model = AveragedModel(model)
```

<br>

## 8. Knowledge Distillation
- **Un gros mod√®le (teacher)** entra√Æne un **mod√®le plus petit (student)** en lui transf√©rant son "savoir".  
- Permet d‚Äôutiliser un mod√®le **l√©ger** avec des performances comparables.  

### **Impl√©mentation**
```python
# Loss avec la sortie du teacher et celle du student
loss = alpha * student_loss + (1 - alpha) * distillation_loss
```


<br>

## 9. **Utilisation de l'Ensemble Learning**
Combiner plusieurs mod√®les pour am√©liorer la robustesse des pr√©dictions, car en combinant les pr√©dictions de plusieurs mod√®les (par exemple, via un *ensemble voting* ou une *moyenne* des pr√©dictions), on peut r√©duire la variance et √©viter le sur-apprentissage sur des donn√©es sp√©cifiques.

**Impl√©mentation (PyTorch)**: 
```python
class EnsembleModel(nn.Module):
    def __init__(self, model_list):
        super(EnsembleModel, self).__init__()
        self.models = nn.ModuleList(model_list)

    def forward(self, x):
        outputs = [model(x) for model in self.models]
        return torch.mean(torch.stack(outputs), dim=0)

# Usage avec plusieurs mod√®les
ensemble_model = EnsembleModel([model1, model2, model3])
```

<br>

## 10. **Early Stopping**
Arr√™ter l‚Äôentra√Ænement si la performance sur les donn√©es de validation cesse de s‚Äôam√©liorer pendant plusieurs epochs. Cela permet d'√©viter l'overfitting en arr√™tant l'entra√Ænement avant que le mod√®le ne commence √† surajuster les donn√©es d'entra√Ænement.

**Impl√©mentation (TensorFlow/Keras)**:
```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)

model.fit(train_data, validation_data=val_data, epochs=50, callbacks=[early_stopping])
```

## 11. **Batch Normalization (BN)**
Ajouter la **normalisation par lots** pour am√©liorer la stabilit√© et acc√©l√©rer la convergence. BN aide √† maintenir la variance des activations dans un certain intervalle, ce qui peut am√©liorer la vitesse de convergence et la stabilit√© de l'entra√Ænement.

**Impl√©mentation (TensorFlow/Keras)**:
```python
from tensorflow.keras.layers import BatchNormalization

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = BatchNormalization()(x)  # Ajout de la normalisation par lots
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
output = Dense(2, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)
```

## 12. **Adversarial Training**
Introduire des **exemples adversariaux** dans l'entra√Ænement pour rendre le mod√®le plus robuste. Cela permet d'am√©liorer la capacit√© du mod√®le √† g√©n√©raliser en l'exposant √† des exemples l√©g√®rement modifi√©s qui peuvent tromper un mod√®le sans d√©fense.

**Impl√©mentation (PyTorch)**:
```python
def adversarial_training(model, inputs, labels, epsilon=0.1):
    inputs.requires_grad = True
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    model.zero_grad()
    loss.backward()
    data_grad = inputs.grad.data
    perturbed_data = inputs + epsilon * data_grad.sign()
    return perturbed_data
```

## 13. **Progressive Resizing (Redimensionnement Progressif)**
Commencer l‚Äôentra√Ænement avec des images de faible r√©solution et augmenter progressivement la taille des images pendant l'entra√Ænement. Cela peut am√©liorer l'efficacit√© de l'entra√Ænement en r√©duisant le co√ªt computationnel au d√©but, puis en affinant progressivement les d√©tails √† des r√©solutions plus √©lev√©es.

**Impl√©mentation (Keras)**:
```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Phase 1: Images de basse r√©solution
datagen = ImageDataGenerator(rescale=1./255)
train_data = datagen.flow_from_directory('data/train', target_size=(128, 128))

# Phase 2: Images de haute r√©solution
datagen = ImageDataGenerator(rescale=1./255)
train_data = datagen.flow_from_directory('data/train', target_size=(224, 224))
```

## 14. **Self-ensembling (Self-ensemblage)**
Cr√©er plusieurs versions d‚Äôun mod√®le via des techniques comme la *dropout* pendant l'inf√©rence, puis agr√©ger les pr√©dictions. Cela permet de tirer parti de la variance des pr√©dictions et de r√©duire l'overfitting.

**Impl√©mentation (PyTorch)**:
```python
model.eval()  # Passage en mode √©valuation pour utiliser dropout durant l'inf√©rence
outputs = []
for _ in range(5):  # Pr√©dictions multiples
    outputs.append(model(inputs))
final_prediction = torch.mean(torch.stack(outputs), dim=0)
```

### 15. **Hyperparameter Optimization (Optimisation des hyperparam√®tres)**
Utiliser des techniques comme la **recherche al√©atoire** ou les **algorithmes bay√©siens** pour optimiser les hyperparam√®tres du mod√®le (par exemple, le taux d'apprentissage, le nombre de couches, etc.). L'optimisation des hyperparam√®tres peut am√©liorer de mani√®re significative les performances en ajustant les param√®tres du mod√®le √† leurs valeurs optimales.

**Impl√©mentation (Hyperopt avec PyTorch)**:
```python
from hyperopt import fmin, tpe, hp, Trials

def objective(params):
    model = YourModel(params['learning_rate'])
    train_loss = train(model)  # Fonction d'entra√Ænement
    return train_loss

space = {
    'learning_rate': hp.loguniform('learning_rate', -5, 0),
    'batch_size': hp.choice('batch_size', [16, 32, 64])
}

best = fmin(objective, space, algo=tpe.suggest, max_evals=50)
print(best)
```

### 16. **Weighted Loss Function**
Attribuer des poids plus √©lev√©s aux classes sous-repr√©sent√©es dans le jeu de donn√©es. Cela peut √™tre tr√®s utile lorsque les donn√©es sont d√©s√©quilibr√©es (par exemple, classification binaire avec un d√©s√©quilibre entre les classes).

   **Impl√©mentation (PyTorch)**:
   ```python
   class_weights = torch.tensor([1.0, 5.0])  # Poids plus √©lev√©s pour la classe minoritaire
   criterion = nn.CrossEntropyLoss(weight=class_weights)
   ```