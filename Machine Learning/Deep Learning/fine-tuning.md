<h1 align='center'> Fine-Tuning pour le DeepLearning </h1>

Le **Fine-Tuning** est une technique d'apprentissage profond qui consiste √† ajuster un mod√®le pr√©-entra√Æn√© sur une nouvelle t√¢che sp√©cifique.  


## 1. Freezing & Unfreezing Progressif
- **Phase 1** : Geler toutes les couches du mod√®le sauf les couches de classification.  
- **Phase 2** : D√©bloquer progressivement certaines couches du r√©seau et fine-tuner avec un LR plus faible.  

Cela permet d'emp√™cher d'apprendre des poids inutiles au d√©but et de conserver les features pr√©-entra√Æn√©es.  

### **Impl√©mentation (TensorFlow/Keras)**
```python
# Phase 1 : Geler toutes les couches du mod√®le
base_model.trainable = False
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_generator, epochs=5)

# Phase 2 : D√©bloquer certaines couches et fine-tuner
base_model.trainable = True
for layer in base_model.layers[:100]:  # Geler les 100 premi√®res couches
    layer.trainable = False

# Diminuer le LR pour √©viter d'endommager les poids pr√©-entra√Æn√©s
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_generator, epochs=5)
```

<br>


## 2. Ajustement du Learning Rate (LR) et Schedulers
Le **learning rate** (taux d‚Äôapprentissage) d√©termine √† quelle vitesse le mod√®le ajuste ses poids.  
- **Trop grand** üèÉüí® ‚Üí Convergence instable, risque d‚Äôoscillations.  
- **Trop petit** üê¢ ‚Üí Convergence tr√®s lente, risque de rester bloqu√© dans un minimum local.  

Dans le **fine-tuning**, il est commun d'utiliser un **LR plus faible** que celui utilis√© lors du pr√©-entra√Ænement.    
Un *scheduler* ajuste **dynamiquement** le LR pendant l'entra√Ænement pour am√©liorer la convergence.  

###  **2.1. ReduceLROnPlateau (Adaptatif)**
- R√©duit le LR lorsque la validation loss **cesse de diminuer**.  
- √âvite un LR trop √©lev√© lorsque le mod√®le stagne.  

**üîπ Impl√©mentation (TensorFlow/Keras) :**  
```python
from tensorflow.keras.callbacks import ReduceLROnPlateau

# R√©duit le LR si la validation loss ne diminue pas apr√®s 3 epochs
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)

# Ajout dans model.fit()
model.fit(train_generator, validation_data=val_generator, epochs=20, callbacks=[lr_scheduler])
```

**üîπ Impl√©mentation (PyTorch) :**  
```python
import torch.optim.lr_scheduler as lr_scheduler

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)

for epoch in range(20):
    train_loss = train_one_epoch()  # Fonction d'entra√Ænement
    val_loss = validate()  # Fonction de validation
    
    scheduler.step(val_loss)  # Ajuste le LR si val_loss stagne
```

---

### **2.2. StepLR (D√©croissance en paliers)**
- Diminue le LR apr√®s un nombre d'epochs fixe (ex. chaque 10 epochs).  
- Simple mais efficace !  

**üîπ Impl√©mentation (PyTorch) :**  
```python
scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

for epoch in range(30):
    train_one_epoch()
    scheduler.step()  # Diminue le LR toutes les 10 epochs
```

---

### **3. Cosine Annealing (Scheduler avanc√©)**
- **Diminution progressive** du LR en suivant une courbe cosinus.  
- Utilis√© souvent avec **SGD + Momentum**.  

**üîπ Impl√©mentation (PyTorch) :**  
```python
scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)

for epoch in range(50):
    train_one_epoch()
    scheduler.step()  # LR suit une courbe cosinus
```

<br>

## **3. Utilisation du Dropout**
- Technique qui **d√©sactive al√©atoirement** des neurones pendant l'entra√Ænement.  
- Emp√™che le r√©seau de **trop s‚Äôadapter** aux donn√©es d'entra√Ænement (**overfitting**).  
- G√©n√©ralement utilis√© **avant les couches fully connected**.

**Dans le fine-tuning, on peut augmenter le dropout pour compenser le risque d‚Äôoverfitting sur peu de donn√©es.**  

**Impl√©mentation (TensorFlow/Keras)**
```python
from tensorflow.keras.layers import Dropout

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)  # D√©sactive 50% des neurones
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)  # D√©sactive 30% des neurones
output = Dense(2, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=output)
```

**Impl√©mentation (PyTorch)**
```python
import torch.nn as nn

class CustomModel(nn.Module):
    def __init__(self, base_model):
        super(CustomModel, self).__init__()
        self.base = base_model
        self.dropout = nn.Dropout(0.5)  # Dropout de 50%
        self.fc = nn.Linear(512, 2)  # Derni√®re couche pour classification

    def forward(self, x):
        x = self.base(x)
        x = self.dropout(x)  # Appliquer le dropout
        x = self.fc(x)
        return x

model = CustomModel(models.resnet18(pretrained=True))
```


<br>

## 4. Label Smoothing
- **Emp√™che le mod√®le d‚Äô√™tre trop s√ªr de ses pr√©dictions.**  
- Modifie l√©g√®rement les labels :  
  - Au lieu d‚Äôavoir **1** pour la classe correcte, on met **0.9**.  
  - Au lieu d‚Äôavoir **0** pour les autres classes, on met **0.1 / (nombre de classes - 1)**.  

**Impl√©mentation (PyTorch)**
```python
criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
```

<br>

## 5. Data augmentation
La *data augmentation* (augmentation des donn√©es) est une technique utilis√©e pour am√©liorer la performance des mod√®les d'apprentissage automatique en augmentant artificiellement la taille et la diversit√© du jeu de donn√©es d‚Äôentra√Ænement. Elle consiste √† appliquer diverses transformations sur les images originales, ce qui permet d'√©viter le surapprentissage (overfitting) et de rendre le mod√®le plus robuste face aux variations des donn√©es r√©elles.

Les transformations classiques incluent :
- **Rotation** : Faire tourner l'image d'un certain angle.
- **Zoom** : Appliquer un zoom avant ou arri√®re sur l'image.
- **Translation** : D√©placer l'image horizontalement ou verticalement.
- **Miroir** : Retourner l'image horizontalement ou verticalement.
- **Perturbation de couleur** : Modifier la luminosit√©, le contraste ou la saturation des images.

Ces transformations permettent au mod√®le d'apprendre √† reconna√Ætre les objets sous diff√©rentes perspectives et conditions, rendant ainsi le mod√®le plus g√©n√©raliste et mieux adapt√© √† des donn√©es r√©elles. L'utilisation de la data augmentation est particuli√®rement b√©n√©fique lorsqu'il y a peu de donn√©es d‚Äôentra√Ænement disponibles.


### Impl√©mentation (PyTorch)
```python
import torch
from torchvision import transforms
from PIL import Image

# D√©finition des transformations
data_augmentation = transforms.Compose([
    transforms.RandomHorizontalFlip(),  # Retourner l'image horizontalement
    transforms.RandomRotation(30),      # Rotation al√©atoire de l'image entre -30 et 30 degr√©s
    transforms.RandomResizedCrop(224),  # Recadrage al√©atoire de l'image avec mise √† l'√©chelle
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Perturbation des couleurs
    transforms.ToTensor(),              # Conversion de l'image en tensor
])

# Chargement de l'image
image = Image.open('image.jpg')

# Application des transformations
augmented_image = data_augmentation(image)

# Affichage de l'image transform√©e (optionnel)
import matplotlib.pyplot as plt
plt.imshow(augmented_image.permute(1, 2, 0))  # Permute les dimensions pour l'affichage
plt.show()
```

### Explication des transformations :
1. **RandomHorizontalFlip()** : Retourne l'image horizontalement avec une probabilit√© de 50%.
2. **RandomRotation(30)** : Fait une rotation al√©atoire de l'image entre -30 et +30 degr√©s.
3. **RandomResizedCrop(224)** : Recadre l'image de mani√®re al√©atoire tout en conservant une taille de 224x224 pixels apr√®s le recadrage.
4. **ColorJitter()** : Modifie de mani√®re al√©atoire la luminosit√©, le contraste, la saturation et la teinte de l'image pour diversifier les couleurs.
5. **ToTensor()** : Convertit l'image PIL en un tensor PyTorch pour l'entra√Ænement.


<br>

## 6. Mixup & CutMix (Augmentation Avanc√©e)
- **Mixup** : M√©lange deux images et leurs labels correspondants.  
- **CutMix** : Remplace une partie d‚Äôune image par une autre image du dataset.  

### **Impl√©mentation (TensorFlow/Keras)**
```python
import tensorflow as tf

def mixup(image, label, alpha=0.2):
    batch_size = tf.shape(image)[0]
    weights = tfp.distributions.Beta(alpha, alpha).sample([batch_size, 1, 1, 1])
    image_shuffled = tf.random.shuffle(image)
    label_shuffled = tf.random.shuffle(label)
    
    mixed_image = weights * image + (1 - weights) * image_shuffled
    mixed_label = weights * label + (1 - weights) * label_shuffled
    return mixed_image, mixed_label
```

<br>

## 7. Stochastic Weight Averaging (SWA)
- Au lieu de prendre les poids du dernier epoch, **moyenner plusieurs mod√®les entra√Æn√©s**.  
- Donne de meilleures performances et une meilleure g√©n√©ralisation.  

### **Impl√©mentation (PyTorch)**
```python
from torch.optim.swa_utils import AveragedModel

swa_model = AveragedModel(model)
```

<br>

## 8. Knowledge Distillation
- **Un gros mod√®le (teacher)** entra√Æne un **mod√®le plus petit (student)** en lui transf√©rant son "savoir".  
- Permet d‚Äôutiliser un mod√®le **l√©ger** avec des performances comparables.  

### **Impl√©mentation**
```python
# Loss avec la sortie du teacher et celle du student
loss = alpha * student_loss + (1 - alpha) * distillation_loss
```