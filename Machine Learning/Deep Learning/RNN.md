<h1 align='center'> Deep learning - Recurrent Neural Networks üîÅ</h1>

Ce document traite d'un nouveau type de couche de r√©seau neuronal profond: **les couches r√©currentes**. Ces types de couches ont pour caract√©ristique commune d'avoir un √©tat interne ou **cach√©**. Cela les rend bien adapt√©s aux donn√©es s√©quentielles, telles que **les s√©ries temporelles** ou **les donn√©es textuelles**. Ils sont tr√®s utilis√©s dans le traitement du langage naturel.

## I. Introduction: *Finding Structure in Time* (Elman, 1990)

Dans son travail fondamental sur les R√©seaux de Neurones R√©currents, Elman a examin√© une vari√©t√© de probl√®mes qui ne pouvaient pas √™tre r√©solus avec des "r√©seaux de neurones standard". Le trait commun de ces probl√®mes, pr√©sent√©s dans son article intitul√© *Finding Structure in Time* (publi√© en 1990), est que les donn√©es sous-jacentes ont une structure temporelle (par exemple, elles pr√©sentent une certaine p√©riodicit√© ou sont organis√©es sous forme de s√©quence).

Ici, une adaptation de l'un de ces probl√®mes sera reproduit en √©tudiant **l'apprentissage des fronti√®res de mots**.

### 1. Probl√®me: *Le H√©ron* (La Fontaine, 1678)

Dans son article, Elman travaillait avec un lexique compos√© de 15 mots en anglais. Dans cet exercice, des mots fran√ßais seront utilis√©s √† la place, ainsi que les 16 premiers mots rencontr√©s dans *Le H√©ron* (La Fontaine, 1678).

<div align="center"><b>Un jour sur ses longs pieds, allait, je ne sais o√π,<br/>
Le H√©ron au long bec </b> emmanch√© d'un long cou.<br/>
Il c√¥toyait une rivi√®re.</div>

√Ä partir de ces 16 mots, de nombreuses phrases seront g√©n√©r√©es en √©chantillonnant al√©atoirement dans le lexique.


### 2. Pr√©paration des donn√©es et d√©finition de la t√¢che

Pour travailler avec PyTorch, il faut organiser les donn√©es en tenseurs. Chaque caract√®re dans le texte est l'une des 26 lettres de l'alphabet. Par cons√©quent, *5 bits* suffisent pour encoder chaque caract√®re.

La t√¢che que propose Elman pour ce probl√®me est **de pr√©dire le prochain caract√®re de la s√©quence**. Bien performer √† cette t√¢che signifierait que le r√©seau a r√©ussi √† comprendre que les s√©quences de caract√®res forment des mots et sont donc pr√©visibles. Pour impl√©menter la t√¢che dans PyTorch, il faut cr√©er 2 tenseurs: 
- l'un d√©finissant l'*entr√©e* (**input**) qui sera donn√© au r√©seau (c'est-√†-dire le caract√®re actuel)
- l'autre √©tant la *cible* (**target**) (c'est-√†-dire le caract√®re suivant).

```python
# Convert a character into a 5 bits encoding
def encodeCharacter(char):
    index = string.ascii_lowercase.index(char) #Index in the alphabet 
    output = [float(x) for x in '{:05b}'.format(index)] #convert to 5 bits
    return output 

# Encode sequence
input = []
target = []

for i in range(len(text)-1):
    input.append(encodeCharacter(text[i]))
    target.append(encodeCharacter(text[i + 1]))

# Convert to tensors
input = torch.from_numpy(np.array(input))
target = torch.from_numpy(np.array(target))

print("tensors size:", input.size())
print("first input: ", input[0])
print("first target: ", target[0])
```

### 3. M√©thode simple: ANN avec une seule couche cach√©e

En tant que premi√®re approche et de r√©f√©rence, il est n√©cessaire de voir comment un r√©seau avec une seule couche cach√©e pourrait se comporter.   
Pour entra√Æner le r√©seau, les m√™mes param√®tres que ceux propos√©s par Elman dans son article seront utilis√©s: 
- Les couches d'entr√©e et de sortie de taille 5 (correspondant √† la taille de codage des lettres)
- Une couche cach√©e de taille 20
- Un entra√Ænement consistant en 10 it√©rations compl√®tes sur la s√©quence.

```python
class LinearNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(LinearNN, self).__init__()
        self.hidden_size = hidden_size
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)
        self.sig=nn.Sigmoid()

    def forward(self, input):
        hidden = self.linear1(input)
        output = self.sig(self.linear2(hidden))
        return output


n_hidden = 20
#network initialization
linearnn = LinearNN(5, n_hidden, 5).double()

#criterion and optimizer for training
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(linearnn.parameters(), lr=0.1, momentum=0.9)

#training iterations
for iter in range(10):
    running_loss=0
    for char in range(input.size(0)):
        optimizer.zero_grad()
        output = linearnn(input[char].reshape(1,5))
        loss = criterion(output, target[char].reshape(1,5))
        loss.backward(retain_graph=True)
        running_loss+=loss.item()
        optimizer.step()
    print("iter ",str(iter)," average loss on iteration:", str(running_loss/input.size(0)))
```

En consid√©rant la perte, le r√©seau ne semble pas avoir appris grand-chose. Cependant, il est clair que le r√©seau n'est pas tr√®s performant lorsqu'il s'agit d'apprendre √† pr√©dire la lettre suivante √† partir de la lettre actuelle. L'interpr√©tation d'Elman de ce r√©sultat est qu'il manque un aspect fondamental dans l'impl√©mentation: **la r√©currence**.

En effet, lors de la pr√©diction de la lettre suivante √† partir de la lettre actuelle, il peut √™tre utile de garder en m√©moire ce que le r√©seau a pr√©dit auparavant. Cette m√©moire permet au r√©seau de tenir compte non seulement de son √©tat actuel lorsqu'il pr√©dit le r√©sultat, mais aussi de l'√©tat pr√©c√©dent.
> Pour les donn√©es organis√©es en s√©quences logiquement structur√©es (par exemple les mots), le concept de r√©currence est crucial pour obtenir de bons r√©sultats.

### 4. Introduction √† la r√©currence

La proposition d'Elman est d'ajouter une **couche r√©currente**. √Ä chaque √©tape, la couche cach√©e re√ßoit non seulement l'entr√©e de la couche d'entr√©e, mais aussi de la couche r√©currente. L'√©tat de la couche cach√©e est ensuite copi√© dans la couche r√©currente pour l'√©tape suivante. 

Cela permet au r√©seau de conserver une certaine m√©moire de l'√©tat pr√©c√©dent.

<h3 align='center'>
    <img src="img/elman.png">
</h3>


En pyTorch, cela peut se faire en utilisant la couche **RNNCell**.


```python
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn1 = nn.RNNCell(input_size, hidden_size)
        self.linear = nn.Linear(hidden_size, output_size)
        self.sig = nn.Sigmoid()

    def forward(self, input, hidden):
        hidden = self.rnn1(input, hidden)
        output = self.sig(self.linear(hidden))
        return output, hidden #we return both output and hidden state, as both will be needed for the next step
    
    def init_hidden(self):
        return torch.zeros(1, self.hidden_size, dtype=torch.double)


n_hidden = 20
#network initialization
rnn = RNN(5, n_hidden, 5).double()

#criterion and optimizer for training
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(rnn.parameters(), lr=0.1, momentum=0.9)

#training iterations
for iter in range(10):
    running_loss=0
    hidden = rnn.init_hidden() #initialize hidden state
    for i in range(input.size(0)):
        optimizer.zero_grad()
        output, hidden = rnn(input[i].reshape(1,5),hidden.detach())
        loss = criterion(output, target[i].reshape(1,5))
        loss.backward(retain_graph=True)
        running_loss += loss.item()
        optimizer.step()
    print("iter ",str(iter)," average loss on iteration:", str(running_loss/input.size(0)))
```

Une certaine convergence peut √™tre constat√©e, m√™me si elle n'est pas tr√®s convaincante (car la t√¢che reste difficile).

En analysant l'erreur faite sur chaque lettre, un sch√©ma appara√Æt tr√®s clairement: l'erreur est √©lev√©e au d√©but des mots, puis diminue au fur et √† mesure que le mot devient plus clair. 

Cela correspond tout √† fait √† la mani√®re dont un humain r√©soudrait ce probl√®me: *pr√©dire la lettre suivante √† partir de la lettre actuelle est impossible, √† moins qu'il n'y ait une structure (impos√©e par les mots) dans cette s√©quence*.
Par exemple, avec uniquement la lettre ¬´ h ¬ª, il n'y a aucun moyen de pr√©dire efficacement la lettre suivante. Cependant, en continuant √† lire et en gardant en m√©moire que les premi√®res lettres sont "*hero*", il est beaucoup plus probable que la pr√©diction de la lettre suivante retourne  "n" (pour former le mot "*heron*").


<br>
<br>

## II. Pr√©vision des s√©ries temporelles

### 1. Pre-processing
La premi√®re √©tape de traitement des donn√©es est de **normaliser les donn√©es**. Cela est b√©n√©fique pour l'entra√Ænement des r√©seaux de neurones r√©currents, mais n√©cessite une connaissance du domaine. (*Ex*: connaissance des limites physiques des capteurs)  

Une autre pr√©paration souvent effectu√©e avec des donn√©es temporelles est de v√©rifier si elles sont **stationnaires**, c'est-√†-dire si la moyenne et la variance changent au fil du temps. Pour ce faire, il est possible d'utiliser le [test de Dickey-Fuller](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test). Ce test peut √™tre tr√®s co√ªteux √† calculer, alors il faut sous-√©chantillonner les donn√©es. Une petite valeur signifie que la s√©rie temporelle **est stationnaire**. Ce n'est pas une condition n√©cessaire pour les LSTM, mais cela facilitera l'entra√Ænement. Lorsque la s√©rie temporelle n'est pas stationnaire (`p_value > 0.05`), il est normal de pr√©dire **la diff√©rence** entre les pas de temps, c'est-√†-dire $$y_t - y_{t-1},$$ qui peut √™tre calcul√©e en utilisant `diff` dans `NumPy` et `shift` dans `pandas`.

### 2. Pr√©diction univari√©e et 1-√©tape

Dans un permier temps, les pr√©dictions seront:
+ Univari√©es: Un signal pour pr√©dire l'avenir de ce signal sans tenir compte des autres caract√©ristiques. L'inverse est multivari√©, o√π plusieurs caract√©ristiques sont utilis√©es dans la pr√©diction (et plusieurs caract√©ristiques peuvent √™tre pr√©dites simultan√©ment).
+ 1 √©tape: Une √©tape dans le futur sera pr√©dite, ce qui peut √©galement √™tre consid√©r√© comme ayant un horizon d'un √©chantillon.


Pour un horizon de $1$, la ligne de base na√Øve fonctionne bien et si l'horizon d√©passe les $1 \text{ms}$, les performances se d√©gradent.   
Une autre approche consiste alors √† diminuer de fa√ßon exponentielle la d√©pendance √† l'√©gard des pr√©dictions pass√©es: le *lissage exponentiel*. Le taux de diminution des pr√©dictions pass√©es est le param√®tre $\alpha$.   
Bien que les valeurs alpha inf√©rieures aident √† pr√©dire la tendance g√©n√©rale de nos donn√©es, leur RMSE est pire que l'utilisation d'un historique tr√®s court. Ainsi, pour des donn√©es EEG, le mod√®le a encore du mal √† faire bon usage des donn√©es historiques pour pr√©dire les donn√©es futures. 


Alors, au lieu d'un param√®tre unique pour la d√©composition de l'historique, il faudra d√©sormais un r√©seau neuronal r√©current pour informer la d√©pendance √† la m√©moire pour la pr√©diction, et les param√®tres du r√©seau seront optimis√©s √† l'aide de la descente de gradient stochastique.

<br>
<br>

### 3. RNN
Une couche de r√©seau de neurones r√©currents (RNN) est tr√®s similaire √† une couche de r√©seau de neurones enti√®rement connect√©e (*feed-forward*): elle poss√®de une matrice de poids $W_x$ reliant la couche pr√©c√©dente $x$ √† chaque neurone de la couche r√©currente, un terme de biais pour chaque neurone, et une fonction d'activation.   
Cependant, un RNN poss√®de √©galement un **√©tat**: sp√©cifiquement, chaque neurone est connect√© √† tous les autres neurones de la m√™me couche avec un d√©lai de temps de $1 \text{ms}$. Cela signifie qu'une couche de RNN dispose d'une seconde matrice de poids $W_s$ de taille $n \times n$, o√π $n$ est le nombre de neurones dans la couche r√©currente. 

L'√©tat peut √™tre calcul√© comme suit:
$$s_t = \tanh(W_{x} x + b_{x} + W_{s} s_{t-1} + b_{s})$$

Une mani√®re d'interpr√©ter ces connexions r√©currentes est de consid√©rer les activations pr√©c√©dentes de la couche r√©currente comme un **√©tat cach√©**, et d'utiliser cet √©tat cach√© comme entr√©e pour le r√©seau:

<h3 align='center'>
    <img src="img/rnn.png">
</h3>

La principale diff√©rence avec les r√©seaux de neurones r√©currents (RNN) est qu'ils d√©pendent de l'√©tat pr√©c√©dent pour calculer l'√©tat actuel. Au lieu de simplement pr√©dire $Y = f(x)$ comme dans les r√©seaux de neurones feed-forward, les r√©seaux r√©currents effectuent une pr√©diction du type $Y_1 = f(x_1, f(x_0))$.



```python
class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.rnn1 = nn.RNNCell(input_size, hidden_size)
        self.linear = nn.Linear(hidden_size, output_size)

    def forward(self, inp, hidden, future=0):
        outputs = []
        for signal in inp.split(1, dim=1):  # Process input sequence
            hidden = self.rnn1(signal, hidden)
            output = self.linear(hidden)
            outputs.append(output)
        for _ in range(future):  # Predict future timesteps
            hidden = self.rnn1(output, hidden)
            output = self.linear(hidden)
            outputs.append(output)
        outputs = torch.cat(outputs, dim=1)
        return outputs, hidden

    def init_hidden(self, batch_size):
        return torch.zeros(batch_size, self.hidden_size, dtype=torch.float32)


# Entra√Ænement
criterion = nn.MSELoss()
optimizer = optim.Adam(rnn.parameters(), lr=0.01)

for epoch in range(100):  # Nombre d'√©poques
    hidden = rnn.init_hidden(train_input.size(0))  # Batch size dynamique
    optimizer.zero_grad()
    out, hidden = rnn(train_input[:,:-horizon], hidden, future=horizon)
    loss = criterion(out[:,:-horizon], train_input[:, horizon:])
    print(f"Epoch {epoch + 1}, Loss: {loss.item():.4f}")
    loss.backward()
    optimizer.step()

# √âvaluation
hidden = rnn.init_hidden(test_input.size(0))
with torch.no_grad():
    output, hidden = rnn(test_input, hidden, future=horizon)
    y = output.numpy()

target = test_input[0, horizon:]
print("MSE of the RNN prediction after training:",
      np.sqrt(mean_squared_error(target, y[0,:len(target)])))

# Visualisation
plt.figure(figsize=(12, 6))
plt.plot(target, label='Real')
plt.plot(y[0,:len(target)], label='Prediction')
plt.xlabel('Timestep')
plt.ylabel('Value')
plt.legend()
plt.show()
```


<br>
<br>

### 4. *Long Short-Term Memory* (LSTM)
Les **LSTM** sont un type de r√©seau de neurones r√©currents (RNN) particuli√®rement adapt√© pour le traitement de donn√©es s√©quentielles ou temporelles. Contrairement aux RNN classiques, ils sont capables de conserver des informations importantes sur de longues p√©riodes gr√¢ce √† leur structure unique. En effet, les RNN classiques souffrent du probl√®me de **disparition ou d'explosion des gradients**, ce qui rend difficile l'apprentissage des relations √† long terme.   
Les LSTM surmontent ce probl√®me gr√¢ce √† un m√©canisme de m√©moire contr√¥l√©e par des "portes". Voici alors quelques applications courantes des LSTM:   
- Analyse de s√©ries temporelles (ex. pr√©visions financi√®res, m√©t√©orologie).
- Traitement du langage naturel (NLP): traduction automatique, g√©n√©ration de texte, analyse de sentiment.
- Reconnaissance vocale et traitement audio.
- G√©n√©ration de musique ou d‚Äôimages bas√©es sur des s√©quences.

#### **a) Structure des LSTM**
<h3 align='center'>
    <img src='img/lstm.png'>
</h3>


Chaque cellule LSTM poss√®de trois portes principales:
- **Porte d‚Äôentr√©e (Input Gate)**: d√©cide quelles informations des donn√©es d'entr√©e doivent √™tre ajout√©es √† l'√©tat de m√©moire.
- **Porte d'oubli (Forget Gate)**: d√©termine quelles informations doivent √™tre oubli√©es dans l'√©tat pr√©c√©dent.
- **Porte de sortie (Output Gate)**: contr√¥le quelles informations sont envoy√©es comme sortie √† l'√©tape suivante.

#### √âquations des LSTM:
1. **Porte d‚Äôoubli**:  
   $$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$
2. **Porte d‚Äôentr√©e**:  
   $$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$  
   $$\tilde{C}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)$$
3. **Mise √† jour de l‚Äô√©tat de la cellule**:  
   $$C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t$$
4. **Porte de sortie**:  
   $$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$  
   $$h_t = o_t \cdot \tanh(C_t)$$  

---

#### **b) Impl√©mentation avec Keras**

Keras propose une impl√©mentation facile des LSTM √† travers le module `tensorflow.keras.layers.LSTM`.

#### Exemple 1: Pr√©diction avec des s√©ries temporelles
Pr√©dire une s√©rie temporelle comme les prix d'une action en utilisant des LSTM.

```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# G√©n√©rer des donn√©es fictives (s√©rie temporelle)
data = np.sin(np.linspace(0, 100, 1000))  # Donn√©es sinuso√Ødales
sequence_length = 50

# Pr√©parer les donn√©es en s√©quences
X = []
y = []
for i in range(len(data) - sequence_length):
    X.append(data[i:i+sequence_length])
    y.append(data[i+sequence_length])
X = np.array(X)
y = np.array(y)

# Reshape pour que chaque s√©quence ait une dimension suppl√©mentaire
X = X.reshape((X.shape[0], X.shape[1], 1))

# Cr√©ation du mod√®le LSTM
model = Sequential([
    LSTM(50, activation='relu', input_shape=(sequence_length, 1)),
    Dense(1)
])

# Compilation du mod√®le
model.compile(optimizer='adam', loss='mse')

# Entra√Ænement
model.fit(X, y, epochs=20, batch_size=32, verbose=1)
```

<br>

#### Param√®tres et Hyperparam√®tres Importants

Lors de l'utilisation des LSTM, certains param√®tres influencent directement la performance:

1. **Units**: Nombre de neurones dans chaque couche LSTM. Plus il y en a, plus la capacit√© du mod√®le est grande.
2. **Activation**: La fonction d'activation par d√©faut est `'tanh'`, mais vous pouvez tester d'autres fonctions.
3. **Recurrent Activation**: La fonction utilis√©e dans les portes r√©currentes (souvent `'sigmoid'`).
4. **Return Sequences**:
   - Si `True`, la couche retourne toute la s√©quence de sorties.
   - Si `False`, elle ne retourne que la derni√®re sortie (par d√©faut).
5. **Dropout**: Pour r√©duire le surapprentissage, une fraction des connexions peut √™tre d√©sactiv√©e al√©atoirement.
6. **Recurrent Dropout**: Applique un dropout sp√©cifique aux connexions r√©currentes.

<br>


#### c) **Impl√©mentation avec PyTorch**

PyTorch fournit une classe appel√©e `torch.nn.LSTM` pour impl√©menter les LSTM. Contrairement √† Keras, qui abstrait beaucoup de d√©tails, PyTorch offre plus de contr√¥le, ce qui est souvent utile pour des t√¢ches avanc√©es.   
Un LSTM PyTorch typique comprend:
1. Une ou plusieurs couches LSTM (`torch.nn.LSTM`).
2. Un module Fully Connected (`torch.nn.Linear`) pour transformer les sorties LSTM en pr√©dictions finales.
3. Une boucle d'entra√Ænement explicite o√π les donn√©es sont pass√©es dans le mod√®le.


#### Exemple 2: Pr√©diction avec des s√©ries temporelles

```python
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset

# G√©n√©ration de donn√©es sinuso√Ødales
data = np.sin(np.linspace(0, 100, 1000))  # Donn√©es sinuso√Ødales
sequence_length = 50

# Pr√©paration des donn√©es pour LSTM
X = []
y = []
for i in range(len(data) - sequence_length):
    X.append(data[i:i+sequence_length])
    y.append(data[i+sequence_length])

X = np.array(X)
y = np.array(y)

# Conversion en tenseurs PyTorch
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32)

# Cr√©ation d'un DataLoader pour faciliter l'entra√Ænement
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

dataset = TimeSeriesDataset(X_tensor, y_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)



import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(LSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # D√©finition de la couche LSTM
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        
        # Couche fully connected
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # Initialisation des √©tats cach√©s
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        # Passage √† travers la couche LSTM
        out, _ = self.lstm(x, (h0, c0))
        
        # On prend la derni√®re sortie (s√©quence finale)
        out = self.fc(out[:, -1,:])
        return out
```

<br>

#### Param√®tres importants:
- **`input_size`**: Nombre de caract√©ristiques dans chaque s√©quence d'entr√©e.
- **`hidden_size`**: Nombre de neurones dans la couche LSTM.
- **`num_layers`**: Nombre de couches LSTM empil√©es.
- **`batch_first=True`**: Si `True`, les dimensions de l'entr√©e seront `(batch_size, sequence_length, input_size)`.

---

#### Entra√Ænement du mod√®le

```python
# Instancier le mod√®le
input_size = 1  # Une seule caract√©ristique par point temporel
hidden_size = 50
output_size = 1
num_layers = 1

model = LSTMModel(input_size, hidden_size, output_size, num_layers)

# D√©finir la fonction de perte et l'optimiseur
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Boucle d'entra√Ænement
num_epochs = 20
for epoch in range(num_epochs):
    for X_batch, y_batch in dataloader:
        # Mettre les donn√©es sur le bon format
        X_batch = X_batch.unsqueeze(-1)  # Ajouter une dimension pour input_size
        y_batch = y_batch.unsqueeze(-1)  # Ajouter une dimension pour la sortie
        
        # Forward pass
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        
        # Backward pass et optimisation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}")
```

<br>


#### **d) Traitement de Texte avec LSTM**

#### **Exemple 3: Classification de sentiment avec des LSTM**
Voici un exemple o√π l‚Äôon utilise des LSTM pour classer des avis en "positifs" ou "n√©gatifs".

```python
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Charger les donn√©es IMDB
max_features = 10000  # Taille du vocabulaire
max_len = 200  # Longueur maximale des s√©quences

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)

# Pr√©traitement: Rendre toutes les s√©quences de m√™me longueur
X_train = pad_sequences(X_train, maxlen=max_len)
X_test = pad_sequences(X_test, maxlen=max_len)

# Cr√©ation du mod√®le
model = Sequential([
    Embedding(input_dim=max_features, output_dim=128, input_length=max_len),
    LSTM(64, dropout=0.2, recurrent_dropout=0.2),
    Dense(1, activation='sigmoid')
])

# Compilation
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Entra√Ænement
model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)

# √âvaluation
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy}")
```

---

#### **e) Utilisation Avanc√©e des LSTM**
#### **Bidirectional LSTM**
Pour capturer les relations dans les deux directions (pass√© et futur), utilisez les **LSTM bidirectionnels**:

```python
from tensorflow.keras.layers import Bidirectional

model = Sequential([
    Embedding(input_dim=max_features, output_dim=128, input_length=max_len),
    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),
    Dense(1, activation='sigmoid')
])
```

#### **Empilement de couches LSTM**
Ajoutez plusieurs couches LSTM pour accro√Ætre la complexit√© du mod√®le:

```python
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(sequence_length, 1)),
    LSTM(64, return_sequences=False),
    Dense(1)
])
```


<br>
<br>

### 5. Gated Recurrent Units (GRU)

Les **GRU** sont une variante simplifi√©e des r√©seaux de neurones r√©currents (RNN), introduite pour r√©soudre les limitations des RNN classiques, tout en √©tant plus l√©g√®re que les LSTM.   
- Comme les LSTM, les GRU sont con√ßues pour **apprendre des d√©pendances √† long terme** dans des donn√©es s√©quentielles.
- Contrairement aux LSTM, elles sont plus simples, avec moins de param√®tres, ce qui les rend plus rapides √† entra√Æner tout en maintenant une performance comp√©titive. les GRU trouvent des applications en:
- Analyse de s√©ries temporelles: pr√©visions de donn√©es financi√®res, m√©t√©orologie.
- Traitement du langage naturel (NLP): traduction, g√©n√©ration de texte, classification.
- Traitement d'audio ou vid√©o s√©quentiel.

---

#### **a) Structure des GRU**
Les GRU utilisent deux portes principales:
1. **Porte de mise √† jour (Update Gate)**: contr√¥le la quantit√© d'information provenant du pass√© qui doit √™tre conserv√©e.
2. **Porte de r√©initialisation (Reset Gate)**: d√©cide combien d'information de l'√©tat pr√©c√©dent doit √™tre oubli√©e.

#### √âquations des GRU:
1. **Porte de mise √† jour**:  
   $$z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$$
   o√π $z_t$ est le vecteur de mise √† jour.

2. **Porte de r√©initialisation**:  
   $$r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$$
   o√π $r_t$ est le vecteur de r√©initialisation.

3. **√âtat candidat**:  
   $$\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)$$

4. **√âtat final**:  
   $$h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t$$

---


#### **b) Impl√©mentation GRU avec Keras**

Keras propose une couche **`GRU`** similaire √† `LSTM`.

#### Exemple: Pr√©diction de s√©ries temporelles
```python
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense

# G√©n√©rer des donn√©es sinuso√Ødales
data = np.sin(np.linspace(0, 100, 1000))  # Donn√©es sinuso√Ødales
sequence_length = 50

# Pr√©parer les donn√©es
X = []
y = []
for i in range(len(data) - sequence_length):
    X.append(data[i:i+sequence_length])
    y.append(data[i+sequence_length])
X = np.array(X)
y = np.array(y)

# Reshape pour GRU
X = X.reshape((X.shape[0], X.shape[1], 1))

# Mod√®le avec GRU
model = Sequential([
    GRU(50, activation='relu', input_shape=(sequence_length, 1)),
    Dense(1)
])

# Compilation du mod√®le
model.compile(optimizer='adam', loss='mse')

# Entra√Ænement
model.fit(X, y, epochs=20, batch_size=32, verbose=1)
```

<br>

#### **c) Impl√©mentation GRU avec PyTorch**

PyTorch fournit √©galement une impl√©mentation native des GRU via la classe `torch.nn.GRU`.

#### Exemple: Pr√©diction de s√©ries temporelles
```python
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

# G√©n√©rer des donn√©es
data = np.sin(np.linspace(0, 100, 1000))
sequence_length = 50

# Pr√©parer les donn√©es
X = []
y = []
for i in range(len(data) - sequence_length):
    X.append(data[i:i+sequence_length])
    y.append(data[i+sequence_length])

X = np.array(X, dtype=np.float32)
y = np.array(y, dtype=np.float32)

# Convertir en tenseurs PyTorch
X_tensor = torch.tensor(X).unsqueeze(-1)  # Ajouter une dimension pour l'input_size
y_tensor = torch.tensor(y).unsqueeze(-1)

# Dataset et DataLoader
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

dataset = TimeSeriesDataset(X_tensor, y_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# Mod√®le GRU avec PyTorch
class GRUModel(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device)  # √âtat initial cach√©
        out, _ = self.gru(x, h0)
        out = self.fc(out[:, -1,:])  # Prendre la derni√®re sortie
        return out

# Hyperparam√®tres
input_size = 1
hidden_size = 50
output_size = 1

model = GRUModel(input_size, hidden_size, output_size)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Entra√Ænement
num_epochs = 20
for epoch in range(num_epochs):
    for X_batch, y_batch in dataloader:
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}")
```

<br>

#### **d) Diff√©rences GRU vs LSTM**
| **Aspect**             | **GRU**                          | **LSTM**                       |
|-------------------------|-----------------------------------|---------------------------------|
| **Complexit√©**          | Moins complexe (2 portes)         | Plus complexe (3 portes)       |
| **Performance**         | Plus rapide, moins co√ªteux       | Peut √™tre plus pr√©cis          |
| **Capacit√© de m√©moire** | Suffisante pour de nombreuses t√¢ches | Meilleure pour de longues s√©quences |
| **Param√®tres**          | Moins de param√®tres √† entra√Æner  | Plus de param√®tres             |


<br>

- Les **GRU** sont une alternative plus rapide et l√©g√®re aux **LSTM**, tout en conservant la capacit√© de mod√©liser des d√©pendances longues dans les s√©quences.
- Elles sont particuli√®rement utiles lorsque:
  - Les ressources mat√©rielles sont limit√©es.
  - Les d√©pendances √† long terme ne sont pas critiques.
- L'impl√©mentation est simple avec **Keras** ou **PyTorch**, et le choix entre les deux d√©pend du niveau de contr√¥le souhait√©.

Les **GRU** sont un excellent compromis entre performance et complexit√©, ce qui en fait un choix populaire dans de nombreuses applications pratiques.