## ğŸ§  Objectif : Pourquoi un LSTM ?

Les **rÃ©seaux de neurones rÃ©currents (RNN)** sont faits pour traiter des **donnÃ©es sÃ©quentielles** (texte, audio, signaux, sÃ©ries temporelles...). Ils **mÃ©morisent des informations** dâ€™une Ã©tape Ã  lâ€™autre.

Mais les RNN classiques ont un gros dÃ©faut : ils **oublient rapidement** (ils nâ€™arrivent pas Ã  gÃ©rer des dÃ©pendances longues). Le **LSTM** (Long Short-Term Memory) a Ã©tÃ© inventÃ© pour **corriger Ã§a**, grÃ¢ce Ã  une **mÃ©moire contrÃ´lÃ©e par des "portes"**.

---

## âš™ï¸ Structure dâ€™un LSTM : ce qui entre, ce qui sort

Ã€ chaque **pas de temps `t`** (par exemple chaque point de ta sÃ©quence radar), l'unitÃ© LSTM reÃ§oit :

* `xâ‚œ` : lâ€™entrÃ©e actuelle (ex: un vecteur \[amplitude, distance])
* `hâ‚œâ‚‹â‚` : la **sortie prÃ©cÃ©dente**
* `câ‚œâ‚‹â‚` : la **mÃ©moire prÃ©cÃ©dente** (câ€™est le cÅ“ur du systÃ¨me)

Elle produit :

* `hâ‚œ` : la **nouvelle sortie** (quâ€™on passe Ã  la couche suivante ou au pas suivant)
* `câ‚œ` : la **nouvelle mÃ©moire interne**

---

## ğŸ”‘ 1. Les **portes** de lâ€™unitÃ© LSTM

Chaque porte est un petit rÃ©seau de neurones (souvent un produit matriciel + une activation `sigmoÃ¯de` ou `tanh`). Elle contrÃ´le le **flux d'information** :

### ğŸ§½ **Porte dâ€™oubli** `fâ‚œ`

Elle dÃ©cide **quoi oublier** de lâ€™ancienne mÃ©moire `câ‚œâ‚‹â‚`.

```math
fâ‚œ = sigmoid(W_f Â· [hâ‚œâ‚‹â‚, xâ‚œ] + b_f)
```

* Si `fâ‚œ â‰ˆ 1` â†’ on garde lâ€™info dans `câ‚œâ‚‹â‚`
* Si `fâ‚œ â‰ˆ 0` â†’ on oublie cette info

---

### ğŸ§° **Porte dâ€™entrÃ©e** `iâ‚œ` + `Ä‰â‚œ`

Elle dÃ©cide **quelle nouvelle info ajouter** Ã  la mÃ©moire.

```math
iâ‚œ = sigmoid(W_i Â· [hâ‚œâ‚‹â‚, xâ‚œ] + b_i)
Ä‰â‚œ = tanh(W_c Â· [hâ‚œâ‚‹â‚, xâ‚œ] + b_c)
```

* `iâ‚œ` dit **quelles dimensions sont mises Ã  jour**
* `Ä‰â‚œ` est la **nouvelle info candidate**


#### ğŸ” Ce que Ã§a veut dire concrÃ¨tement :

`iâ‚œ` est un **vecteur de la mÃªme taille que `Ä‰â‚œ`** (la nouvelle "info candidate" Ã  ajouter Ã  la mÃ©moire). Chacune de ses composantes (entre 0 et 1 grÃ¢ce Ã  la sigmoÃ¯de) agit comme un **interrupteur doux** :

* Si `iâ‚œ[j]` est proche de **1**, cela veut dire :
  ğŸ‘‰ **â€œOui, on veut mettre Ã  jour la dimension `j` de la mÃ©moire avec `Ä‰â‚œ[j]`.â€**

* Si `iâ‚œ[j]` est proche de **0**, cela veut dire :
  ğŸ‘‰ **â€œNon, on ne touche pas Ã  cette dimension `j` de la mÃ©moire.â€**



#### ğŸ§  Exemple simple :

Imaginons que `Ä‰â‚œ = [0.4, -0.7, 0.2]` (la nouvelle info)
et que `iâ‚œ = [1.0, 0.0, 0.5]` (le filtre "quoi ajouter").

Alors `iâ‚œ * Ä‰â‚œ = [0.4, 0.0, 0.1]`

â¡ï¸ La **1Ã¨re dimension** est complÃ¨tement ajoutÃ©e
â¡ï¸ La **2Ã¨me dimension** est ignorÃ©e
â¡ï¸ La **3Ã¨me dimension** est partiellement prise en compte



#### ğŸ“Œ En rÃ©sumÃ© :

Quand on dit que `iâ‚œ` "dit quelles dimensions sont mises Ã  jour", Ã§a signifie :

> Chaque Ã©lÃ©ment de `iâ‚œ` dÃ©cide **dans quelle mesure** on ajoute la nouvelle information `Ä‰â‚œ` **dans chaque case** de la mÃ©moire `câ‚œ`.

Tu peux imaginer que la mÃ©moire a plein de petits tiroirs (une par dimension), et `iâ‚œ` choisit **quels tiroirs ouvrir plus ou moins grand** pour y glisser la nouvelle info.


---

### ğŸ§  **Mise Ã  jour de la mÃ©moire** `câ‚œ`

On combine lâ€™oubli et lâ€™ajout :

```math
câ‚œ = fâ‚œ * câ‚œâ‚‹â‚ + iâ‚œ * Ä‰â‚œ
```

* On oublie une partie du passÃ© (`fâ‚œ * câ‚œâ‚‹â‚`)
* On ajoute une partie du prÃ©sent (`iâ‚œ * Ä‰â‚œ`)

---

### ğŸ“¤ **Porte de sortie** `oâ‚œ`

Elle contrÃ´le ce quâ€™on sort Ã  ce pas de temps :

```math
oâ‚œ = sigmoid(W_o Â· [hâ‚œâ‚‹â‚, xâ‚œ] + b_o)
hâ‚œ = oâ‚œ * tanh(câ‚œ)
```

* `hâ‚œ` est la sortie visible (transmise Ã  la couche suivante)
* `câ‚œ` est stockÃ© en mÃ©moire pour lâ€™Ã©tape suivante

---

## ğŸ§¾ En rÃ©sumÃ© : ce qui se passe dans une cellule LSTM Ã  lâ€™instant `t`

```text
EntrÃ©e: xâ‚œ, hâ‚œâ‚‹â‚, câ‚œâ‚‹â‚
Sortie: hâ‚œ, câ‚œ

1. fâ‚œ = sigmoid(W_f Â· [hâ‚œâ‚‹â‚, xâ‚œ])        â† quoi oublier ?
2. iâ‚œ = sigmoid(W_i Â· [hâ‚œâ‚‹â‚, xâ‚œ])        â† quoi ajouter ?
3. Ä‰â‚œ = tanh(W_c Â· [hâ‚œâ‚‹â‚, xâ‚œ])           â† valeur Ã  ajouter
4. câ‚œ = fâ‚œ * câ‚œâ‚‹â‚ + iâ‚œ * Ä‰â‚œ              â† mÃ©moire mise Ã  jour
5. oâ‚œ = sigmoid(W_o Â· [hâ‚œâ‚‹â‚, xâ‚œ])        â† quoi sortir ?
6. hâ‚œ = oâ‚œ * tanh(câ‚œ)                    â† sortie

```

---

## ğŸ’¡ Intuition dâ€™ingÃ©nieur

On peut voir Ã§a comme un **filtre adaptatif intelligent** :

* `câ‚œ` est une **bande passante mÃ©moire** : elle conserve lâ€™essentiel.
* Les portes `fâ‚œ`, `iâ‚œ`, `oâ‚œ` sont des **circuits logiques flous** : elles apprennent Ã  ouvrir/fermer le passage Ã  l'information selon le contexte.
* GrÃ¢ce Ã  Ã§a, le LSTM peut **retenir une info longtemps** (ex: une bosse dans le signal radar) ou **lâ€™oublier vite** (ex: du bruit ou un pic isolÃ©).

---

## ğŸ”§ En pratique (PyTorch)

Tu nâ€™as pas Ã  coder tout Ã§a Ã  la main. PyTorch fournit une implÃ©mentation :

```python
lstm = nn.LSTM(input_size=2, hidden_size=64, num_layers=2, batch_first=True)
```

Mais **comprendre ce quâ€™il se passe dans chaque cellule tâ€™aide Ã  mieux configurer le modÃ¨le**, Ã  choisir le bon pooling, ou Ã  interprÃ©ter les erreurs.

---

## ğŸš€ Ã€ quoi sert la bidirectionnalitÃ© ?

Une **LSTM bidirectionnelle** lit la sÃ©quence **dans les deux sens** :

* une LSTM â€œavantâ€ lit de `t=0` Ã  `t=T`
* une LSTM â€œarriÃ¨reâ€ lit de `t=T` Ã  `t=0`

Elle capte donc :

* le **contexte passÃ©** (`â†`) comme une LSTM normale
* **et aussi le futur** (`â†’`) dans la sÃ©quence

### ğŸ‘‰ Exemple :

Dans une phrase :

> â€œIl a **glissÃ©** sur une **peau de banane**.â€

Pour prÃ©dire ou classer "**glissÃ©**", savoir que "**peau de banane**" vient **aprÃ¨s** peut aider Ã©normÃ©ment â€” mais une LSTM classique ne le sait pas encore. Une bidirectionnelle, oui.

---

## ğŸ“Š Est-ce que câ€™est toujours mieux ?

### âœ… **Oui**, si :

* Tu travailles sur une tÃ¢che oÃ¹ **tout le contexte est disponible en avance** (ex: classification de sÃ©quence, comprÃ©hension globale, NLP, sÃ©ries temporelles **non causales**).
* Tu veux **plus de contexte global** pour prendre une dÃ©cision Ã  la fin (ex: sentiment global, dÃ©tection dâ€™anomalies sur fenÃªtre glissante...).

### âŒ **Non**, si :

* Tu fais de la **prÃ©diction temps rÃ©el / sÃ©quentielle** (ex: prÃ©dire le futur en temps rÃ©el, traitement de flux, gÃ©nÃ©ration en ligne, etc.).
* Tu ne peux **pas utiliser dâ€™info du futur** (c'est interdit dans le cadre mÃ©tier, ex: finance en ligne, robotique, etc.).



Oui, et câ€™est un sujet passionnant ! ğŸ¯ Lâ€™interprÃ©tabilitÃ© des modÃ¨les LSTM (et plus gÃ©nÃ©ralement des modÃ¨les sÃ©quentiels profonds) est **un dÃ©fi**, mais il existe plusieurs **outils et mÃ©thodes** pour **comprendre ce que le modÃ¨le a appris** ou **pourquoi il prÃ©dit ce quâ€™il prÃ©dit**.

---

## ğŸ” Outils & mÃ©thodes dâ€™interprÃ©tabilitÃ© pour les LSTM

### 1. ğŸ§  **Attention (self-attention ou mÃ©canisme externe)**

* Ajoute un **poids Ã  chaque pas de temps** de la sÃ©quence.
* Tu peux **visualiser les poids dâ€™attention** pour savoir **quels moments du signal ont influencÃ© la prÃ©diction**.

> **IdÃ©al** pour les tÃ¢ches oÃ¹ certains instants clÃ©s du signal portent plus dâ€™info que dâ€™autres (ex : anomalies, pics, motifs localisÃ©s).

#### ğŸ“¦ Outils : implÃ©mentÃ© manuellement, ou via modules comme `torch.nn.MultiheadAttention`


#### ğŸ¯ **But de l'attention**

> Permettre au modÃ¨le de **se concentrer** sur les parties **importantes** dâ€™une sÃ©quence dâ€™entrÃ©e.
> PlutÃ´t que de traiter chaque Ã©lÃ©ment de la sÃ©quence Ã©galement (comme une moyenne), le modÃ¨le **apprend Ã  pondÃ©rer chaque pas de temps** selon sa pertinence pour la tÃ¢che (ex : classification).

#### âš™ï¸ **Principe de fonctionnement**

1. **Tu obtiens une sÃ©quence de vecteurs** (ex : les `hâ‚œ` de chaque pas de temps du LSTM)
2. Tu apprends un **score dâ€™importance** pour chaque `hâ‚œ`
3. Tu appliques un **softmax** pour normaliser en poids `Î±â‚œ`
4. Tu fais une **somme pondÃ©rÃ©e** de ces `hâ‚œ` â câ€™est ton **vecteur de contexte**


#### ğŸ”§ Version 1 â€” Attention simple (maison)

##### âœ… Simple, interprÃ©table, rapide Ã  entraÃ®ner

```python
import torch
import torch.nn as nn

class LSTMWithSimpleAttention(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1, bidirectional=True):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                            batch_first=True, bidirectional=bidirectional)
        
        self.attn = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, 1)
        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, output_size)

    def forward(self, x):
        # x: (batch, seq_len, input_size)
        lstm_out, _ = self.lstm(x)  # (batch, seq_len, hidden*2)
        
        # Attention scores
        attn_scores = self.attn(lstm_out)  # (batch, seq_len, 1)
        attn_weights = torch.softmax(attn_scores, dim=1)  # (batch, seq_len, 1)

        # Context vector
        context = torch.sum(attn_weights * lstm_out, dim=1)  # (batch, hidden*2)

        out = self.fc(context)  # (batch, output_size)
        return out, attn_weights  # on peut visualiser oÃ¹ le modÃ¨le regarde
```

#### ğŸ¤– Version 2 â€” Avec `torch.nn.MultiheadAttention`

##### âœ… Plus puissant, multi-perspective

##### âš ï¸ Plus complexe, mais utile sur sÃ©quences longues ou motifs imbriquÃ©s

```python
import torch
import torch.nn as nn

class LSTMWithMultiheadAttention(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1, num_heads=4):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,
                            batch_first=True, bidirectional=True)

        self.attn = nn.MultiheadAttention(embed_dim=hidden_size * 2, num_heads=num_heads, batch_first=False)
        self.fc = nn.Linear(hidden_size * 2, output_size)

    def forward(self, x):
        # x: (batch, seq_len, input_size)
        batch_size = x.size(0)
        lstm_out, _ = self.lstm(x)  # (batch, seq_len, hidden*2)

        # Convert to (seq_len, batch, embed_dim) for MultiheadAttention
        lstm_out = lstm_out.permute(1, 0, 2)

        # Self-attention (query = key = value)
        attn_output, attn_weights = self.attn(lstm_out, lstm_out, lstm_out)

        # Back to (batch, seq_len, hidden*2)
        attn_output = attn_output.permute(1, 0, 2)

        # Pool (mean over time)
        context = attn_output.mean(dim=1)

        out = self.fc(context)
        return out, attn_weights  # (attn_weights: [batch*num_heads, seq_len, seq_len])
```

#### ğŸ†š Comparaison rapide

| Aspect                        | Attention simple         | MultiheadAttention (`nn`) |
| ----------------------------- | ------------------------ | ------------------------- |
| FacilitÃ© dâ€™implÃ©mentation     | âœ… TrÃ¨s simple            | âš ï¸ Doit permuter les dims |
| InterprÃ©tabilitÃ©              | âœ… Facile (1 score par t) | âŒ Plus dur (par tÃªte)     |
| Puissance / FlexibilitÃ©       | ğŸ”¶ Moyenne               | âœ… Forte                   |
| Utilisation de plusieurs vues | âŒ Non                    | âœ… Oui (multi-perspective) |
| RecommandÃ© pour dÃ©marrer      | âœ… Oui                    | âŒ Ã€ garder pour plus tard |










---

### 2. ğŸ“Š **LIME / SHAP (adaptÃ©s aux sÃ©quences)**

* Techniques **agnostiques** au modÃ¨le (boÃ®te noire) qui mesurent lâ€™effet de perturbations locales.
* Te disent **quels Ã©lÃ©ments de la sÃ©quence ont le plus pesÃ©** dans la dÃ©cision.

> âš ï¸ Peut Ãªtre coÃ»teux en temps de calcul, et nÃ©cessite parfois d'adapter la granularitÃ© temporelle (par ex. segmenter la sÃ©quence en blocs).

#### ğŸ“¦ Outils :

* `LIME`: [`lime.lime_tabular`](https://github.com/marcotcr/lime)
* `SHAP`: [`DeepExplainer` ou `GradientExplainer`](https://github.com/slundberg/shap)

---

### 3. ğŸ” **Gradient-based methods (saliency, integrated gradients, etc.)**

* Calculent le **gradient de la sortie par rapport Ã  lâ€™entrÃ©e** : "si je bouge ce point du signal, est-ce que la sortie change beaucoup ?"
* Permettent de **visualiser les zones sensibles du signal**.

> TrÃ¨s utilisÃ© en NLP et vision, **transposable aux sÃ©ries temporelles**.

#### ğŸ“¦ Outils :

* `captum` (lib PyTorch pour lâ€™interprÃ©tabilitÃ©)
  â†’ [https://github.com/pytorch/captum](https://github.com/pytorch/captum)
  â†’ propose : saliency maps, integrated gradients, DeepLIFT, etc.

---

### 4. ğŸ› ï¸ **Hidden state analysis**

* Inspecte manuellement les `hâ‚œ` ou `câ‚œ` au fil du temps.
* Peut Ãªtre visualisÃ© comme un "Ã©lectroencÃ©phalogramme" du modÃ¨le.
* Si tu observes des pics ou des activations fortes Ã  certains instants, cela **rÃ©vÃ¨le que le modÃ¨le "rÃ©agit" Ã  certaines zones du signal.**

#### ğŸ‘‰ Pratique :

```python
out, (hn, cn) = model.lstm(x)
# out: (batch, seq_len, hidden_size)
plt.plot(out[0].detach().cpu())  # Affiche l'Ã©volution des activations
```

---

### 5. ğŸ“š **Feature occlusion / masking**

* Masquer des morceaux du signal (par ex. remplacer par du bruit ou des zÃ©ros) et observer la variation de la prÃ©diction.
* Cela montre **quelles zones sont critiques** pour la dÃ©cision.

---

## ğŸ§  En rÃ©sumÃ© : quelle mÃ©thode choisir ?

| Objectif                                        | MÃ©thode                    | FacilitÃ©            | InterprÃ©tation                |
| ----------------------------------------------- | -------------------------- | ------------------- | ----------------------------- |
| Voir **quand** le modÃ¨le s'active               | Attention                  | âœ… Facile Ã  intÃ©grer | ğŸŒŸ TrÃ¨s visuel                |
| Voir **quels Ã©lÃ©ments** impactent la prÃ©diction | LIME / SHAP                | âš ï¸ Plus lourd       | ğŸŒŸ Explicite                  |
| Voir **oÃ¹ le gradient est fort**                | Saliency / IG (Captum)     | âœ… Moyen             | ğŸ” PrÃ©cis mais parfois bruitÃ© |
| Voir **ce que le modÃ¨le "ressent"**             | Analyse des Ã©tats internes | âœ… Facile            | ğŸ”§ Diagnostic                 |
| Tester lâ€™impact dâ€™un bloc du signal             | Masquage / occlusion       | âœ… Simple            | ğŸ§ª Empirique                  |



<br>

## ğŸ§± **Architecture LSTM pour rÃ©gression**

```python
import torch
import torch.nn as nn

class DminRegressorLSTM(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=2, bidirectional=True):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=bidirectional
        )
        
        # Taille de sortie aprÃ¨s LSTM
        lstm_output_size = hidden_size * (2 if bidirectional else 1)
        
        # RÃ©duction des features pour prÃ©diction
        self.regressor = nn.Sequential(
            nn.Linear(lstm_output_size, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()       # Pour que la sortie soit dans [0, 1]
        )

    def forward(self, x):
        # x shape: (batch, seq_len, 1)
        out, _ = self.lstm(x)  # out shape: (batch, seq_len, hidden*2)
        out = out.mean(dim=1)  # Moyenne temporelle : (batch, hidden*2)
        out = self.regressor(out)  # (batch, 1)
        return out
```


*Remarque:* On fait souvent la **moyenne temporelle** (`out.mean(dim=1)`) des sorties LSTM car câ€™est une maniÃ¨re simple, efficace et **neutre** dâ€™agrÃ©ger **l'information sÃ©quentielle** en une reprÃ©sentation **fixe** pour des tÃ¢ches comme la **classification** ou la **rÃ©gression**. Voici pourquoi ğŸ‘‡

#### 1. **On veut une sortie fixe par sÃ©quence**

* Ton LSTM produit une **sortie Ã  chaque pas de temps** â†’ `(batch, seq_len, hidden_size)`
* Mais pour une tÃ¢che de **rÃ©gression**, on veut **une seule sortie par signal** â†’ `(batch, 1)`
* Moyenne = faÃ§on simple de condenser la sÃ©quence en un seul vecteur

#### 2. **Ne fait pas dâ€™hypothÃ¨se sur oÃ¹ est lâ€™info**

* Contrairement Ã  `out[:, -1, :]` (derniÃ¨re sortie), la moyenne **utilise toute la sÃ©quence**
* Si lâ€™info utile est **n'importe oÃ¹** dans le signal, elle sera prise en compte

#### 3. **Moins sensible Ã  la position**

* Utile si la **cible** varie beaucoup dans le signal
* Cela aide Ã  gÃ©nÃ©raliser en lâ€™absence dâ€™attention explicite
