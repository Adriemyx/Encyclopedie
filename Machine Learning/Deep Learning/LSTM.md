## ğŸ§  Objectif : Pourquoi un LSTM ?

Les **rÃ©seaux de neurones rÃ©currents (RNN)** sont faits pour traiter des **donnÃ©es sÃ©quentielles** (texte, audio, signaux, sÃ©ries temporelles...). Ils **mÃ©morisent des informations** dâ€™une Ã©tape Ã  lâ€™autre.

Mais les RNN classiques ont un gros dÃ©faut : ils **oublient rapidement** (ils nâ€™arrivent pas Ã  gÃ©rer des dÃ©pendances longues). Le **LSTM** (Long Short-Term Memory) a Ã©tÃ© inventÃ© pour **corriger Ã§a**, grÃ¢ce Ã  une **mÃ©moire contrÃ´lÃ©e par des "portes"**.

---

## âš™ï¸ Structure dâ€™un LSTM : ce qui entre, ce qui sort

Ã€ chaque **pas de temps `t`** (par exemple chaque point de ta sÃ©quence radar), l'unitÃ© LSTM reÃ§oit :

* `xâ‚œ` : lâ€™entrÃ©e actuelle (ex: un vecteur \[amplitude, distance])
* `hâ‚œâ‚‹â‚` : la **sortie prÃ©cÃ©dente**
* `câ‚œâ‚‹â‚` : la **mÃ©moire prÃ©cÃ©dente** (câ€™est le cÅ“ur du systÃ¨me)

Elle produit :

* `hâ‚œ` : la **nouvelle sortie** (quâ€™on passe Ã  la couche suivante ou au pas suivant)
* `câ‚œ` : la **nouvelle mÃ©moire interne**

---

## ğŸ”‘ 1. Les **portes** de lâ€™unitÃ© LSTM

Chaque porte est un petit rÃ©seau de neurones (souvent un produit matriciel + une activation `sigmoÃ¯de` ou `tanh`). Elle contrÃ´le le **flux d'information** :

### ğŸ§½ **Porte dâ€™oubli** `fâ‚œ`

Elle dÃ©cide **quoi oublier** de lâ€™ancienne mÃ©moire `câ‚œâ‚‹â‚`.

```math
fâ‚œ = sigmoid(W_f Â· [hâ‚œâ‚‹â‚, xâ‚œ] + b_f)
```

* Si `fâ‚œ â‰ˆ 1` â†’ on garde lâ€™info dans `câ‚œâ‚‹â‚`
* Si `fâ‚œ â‰ˆ 0` â†’ on oublie cette info

---

### ğŸ§° **Porte dâ€™entrÃ©e** `iâ‚œ` + `Ä‰â‚œ`

Elle dÃ©cide **quelle nouvelle info ajouter** Ã  la mÃ©moire.

```math
iâ‚œ = sigmoid(W_i Â· [hâ‚œâ‚‹â‚, xâ‚œ] + b_i)
Ä‰â‚œ = tanh(W_c Â· [hâ‚œâ‚‹â‚, xâ‚œ] + b_c)
```

* `iâ‚œ` dit **quelles dimensions sont mises Ã  jour**
* `Ä‰â‚œ` est la **nouvelle info candidate**


#### ğŸ” Ce que Ã§a veut dire concrÃ¨tement :

`iâ‚œ` est un **vecteur de la mÃªme taille que `Ä‰â‚œ`** (la nouvelle "info candidate" Ã  ajouter Ã  la mÃ©moire). Chacune de ses composantes (entre 0 et 1 grÃ¢ce Ã  la sigmoÃ¯de) agit comme un **interrupteur doux** :

* Si `iâ‚œ[j]` est proche de **1**, cela veut dire :
  ğŸ‘‰ **â€œOui, on veut mettre Ã  jour la dimension `j` de la mÃ©moire avec `Ä‰â‚œ[j]`.â€**

* Si `iâ‚œ[j]` est proche de **0**, cela veut dire :
  ğŸ‘‰ **â€œNon, on ne touche pas Ã  cette dimension `j` de la mÃ©moire.â€**



#### ğŸ§  Exemple simple :

Imaginons que `Ä‰â‚œ = [0.4, -0.7, 0.2]` (la nouvelle info)
et que `iâ‚œ = [1.0, 0.0, 0.5]` (le filtre "quoi ajouter").

Alors `iâ‚œ * Ä‰â‚œ = [0.4, 0.0, 0.1]`

â¡ï¸ La **1Ã¨re dimension** est complÃ¨tement ajoutÃ©e
â¡ï¸ La **2Ã¨me dimension** est ignorÃ©e
â¡ï¸ La **3Ã¨me dimension** est partiellement prise en compte



#### ğŸ“Œ En rÃ©sumÃ© :

Quand on dit que `iâ‚œ` "dit quelles dimensions sont mises Ã  jour", Ã§a signifie :

> Chaque Ã©lÃ©ment de `iâ‚œ` dÃ©cide **dans quelle mesure** on ajoute la nouvelle information `Ä‰â‚œ` **dans chaque case** de la mÃ©moire `câ‚œ`.

Tu peux imaginer que la mÃ©moire a plein de petits tiroirs (une par dimension), et `iâ‚œ` choisit **quels tiroirs ouvrir plus ou moins grand** pour y glisser la nouvelle info.


---

### ğŸ§  **Mise Ã  jour de la mÃ©moire** `câ‚œ`

On combine lâ€™oubli et lâ€™ajout :

```math
câ‚œ = fâ‚œ * câ‚œâ‚‹â‚ + iâ‚œ * Ä‰â‚œ
```

* On oublie une partie du passÃ© (`fâ‚œ * câ‚œâ‚‹â‚`)
* On ajoute une partie du prÃ©sent (`iâ‚œ * Ä‰â‚œ`)

---

### ğŸ“¤ **Porte de sortie** `oâ‚œ`

Elle contrÃ´le ce quâ€™on sort Ã  ce pas de temps :

```math
oâ‚œ = sigmoid(W_o Â· [hâ‚œâ‚‹â‚, xâ‚œ] + b_o)
hâ‚œ = oâ‚œ * tanh(câ‚œ)
```

* `hâ‚œ` est la sortie visible (transmise Ã  la couche suivante)
* `câ‚œ` est stockÃ© en mÃ©moire pour lâ€™Ã©tape suivante

---

## ğŸ§¾ En rÃ©sumÃ© : ce qui se passe dans une cellule LSTM Ã  lâ€™instant `t`

```text
EntrÃ©e: xâ‚œ, hâ‚œâ‚‹â‚, câ‚œâ‚‹â‚
Sortie: hâ‚œ, câ‚œ

1. fâ‚œ = sigmoid(W_f Â· [hâ‚œâ‚‹â‚, xâ‚œ])        â† quoi oublier ?
2. iâ‚œ = sigmoid(W_i Â· [hâ‚œâ‚‹â‚, xâ‚œ])        â† quoi ajouter ?
3. Ä‰â‚œ = tanh(W_c Â· [hâ‚œâ‚‹â‚, xâ‚œ])           â† valeur Ã  ajouter
4. câ‚œ = fâ‚œ * câ‚œâ‚‹â‚ + iâ‚œ * Ä‰â‚œ              â† mÃ©moire mise Ã  jour
5. oâ‚œ = sigmoid(W_o Â· [hâ‚œâ‚‹â‚, xâ‚œ])        â† quoi sortir ?
6. hâ‚œ = oâ‚œ * tanh(câ‚œ)                    â† sortie

```

---

## ğŸ’¡ Intuition dâ€™ingÃ©nieur

On peut voir Ã§a comme un **filtre adaptatif intelligent** :

* `câ‚œ` est une **bande passante mÃ©moire** : elle conserve lâ€™essentiel.
* Les portes `fâ‚œ`, `iâ‚œ`, `oâ‚œ` sont des **circuits logiques flous** : elles apprennent Ã  ouvrir/fermer le passage Ã  l'information selon le contexte.
* GrÃ¢ce Ã  Ã§a, le LSTM peut **retenir une info longtemps** (ex: une bosse dans le signal radar) ou **lâ€™oublier vite** (ex: du bruit ou un pic isolÃ©).

---

## ğŸ”§ En pratique (PyTorch)

Tu nâ€™as pas Ã  coder tout Ã§a Ã  la main. PyTorch fournit une implÃ©mentation :

```python
lstm = nn.LSTM(input_size=2, hidden_size=64, num_layers=2, batch_first=True)
```

Mais **comprendre ce quâ€™il se passe dans chaque cellule tâ€™aide Ã  mieux configurer le modÃ¨le**, Ã  choisir le bon pooling, ou Ã  interprÃ©ter les erreurs.
