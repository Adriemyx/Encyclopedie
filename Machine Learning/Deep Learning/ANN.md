<h1 align='center'> Deep learning - Artificial Neural Networks üß†</h1>


## I. R√©seaux neuronaux biologiques
Dans les ann√©es 1830, la th√©orie cellulaire est introduite, d√©montrant que les organismes vivants sont compos√©s de cellules. 
Cependant, √† cause des limites des microscopes de l'√©poque, personne n'avait encore observ√© les constituants de base des tissus nerveux, 
faisant du syst√®me nerveux une exception √† la th√©orie cellulaire.   

En 1888, *Santiago Ram√≥n y Cajal* observe une s√©paration physique entre les cellules individuelles √† la jonction axone/dendrite (chez les poulets), 
jetant ainsi les bases de la doctrine neuronale. Cette d√©couverte lui vaut le prix Nobel de physiologie ou m√©decine en 1906.   

La doctrine neuronale postule que:
- les neurones sont les unit√©s de base du syst√®me nerveux,
- les dendrites, le noyau, l'axone et les synapses sont les composantes principales des neurones,
- les impulsions √©lectriques et les neurotransmetteurs chimiques jouent un r√¥le crucial dans la transmission des informations dans le syst√®me nerveux.

<br>

## II. R√©seaux neuronaux artificiels

L'id√©e g√©n√©ral d'un r√©seau de neurones est que:
1. Chaque neurone traite une partie de l'information et la transmet √† ses "enfants".
2. Dans l'ensemble, le r√©seau transforme des informations brutes en concepts g√©n√©raux.

Est-ce alors possible d'imiter ce syst√®me de connexions dans un syst√®me d'apprentissage qui adapte ses param√®tres en fonction des donn√©es auxquelles il est expos√©?

### 1. Histroire des r√©seaux de neurones aritificiels
En s'inspirant du neurone biologique, un mod√®le simplifi√© de neurone a √©t√© construit. En supposant qu‚Äôun signal d‚Äôentr√©e soit repr√©sent√© par un vecteur binaire $x$. Les √©l√©ments de $x$ peuvent repr√©senter des √©nonc√©s binaires (ex: "il pleut", "j'ai un parapluie"...). L‚Äô**activation** d‚Äôun neurone fut mod√©lis√© comme suit:
$f(x)=\left\{\begin{array}{ll} 0 & \textrm{si }w^T x+b\leq 0 \\ 1 & \textrm{sinon}\end{array}\right.=step\left(w^Tx + b\right)$,
o√π $step$ est la fonction √©chelon de Heaviside.  

Cette fonction est un *Perceptron de Rosenblatt*. Le perceptron de Rosenblatt est la fonction $step(w_1 x_1 + w_2 x_2 + b)$.   
Essentiellement, un perceptron est une **r√®gle de s√©paration lin√©aire**.  
Intuitivement, c‚Äôest une machine qui pond√®re les √©l√©ments de preuve $x$ et les compare √† un seuil $b$ pour prendre une d√©cision $f(x)$.


<br>

Avec $(w_1, w_2, b) = (2,2,-1)$, c'est une porte **OR**.  

Avec $(w_1, w_2, b) = (2,2,-3)$, c'est une porte **AND**.  

Avec $(w_1, w_2, b) = (-2,-2,3)$, c'est une porte **NAND**.  


```python
import numpy as np

def perceptron(w,b,x):
    return np.heaviside(np.dot(x,w)+b,0)

x = np.zeros((4,2))
x[1,0] = 1.
x[2,1] = 1.
x[3,0] = 1.
x[3,1] = 1.

print("input values:\n", x)

def OR(x):
    w = np.array([2.,2.])
    b = -1.
    return perceptron(w,b,x)
print("testing OR gate:", OR(x))

def AND(x):
    w = np.array([2.,2.])
    b = -3.
    return perceptron(w,b,x)
print("testing AND gate:", AND(x))

def NAND(x):
    w = np.array([-2.,-2.])
    b = 3.
    return perceptron(w,b,x)
print("testing NAND gate:", NAND(x))
```
*<u> Remarque:</u> Un perceptron est invariant par multiplication scalaire.*   

**Mais**, il n'est pas possible de mod√©liser une porte **XOR** par un perceptron. √âtant donn√© que les perceptrons mettent en ≈ìuvre un seuil sur une combinaison lin√©aire des entr√©es, ils ne peuvent s√©parer que les classes qui sont **lin√©airement s√©parables**. Or le XOR est un exemple typique de donn√©es non lin√©airement s√©parables.   

<br>

**Cependant**, il est possible de connecter des perceptrons ensemble pour obtenir une fonction XOR (par exemple, en remarquant que: $Z=$ $x_0$ XOR $x_1$ = $[(x_0$ OR $x_1)$ AND $(x_0$ NAND $x_1)]$.   

<h3 align='center'>
    <img src="img/xor.png" width="600px"></img>
</h3>


En r√©alit√©, il est possible de proc√©der ainsi pour **toute fonction logique**.    
De telles architectures connect√©es sont appel√©es **Perceptrons Multi-Couches (MLP)**. Ce terme a ensuite √©t√© utilis√© (abusivement) pour d√©signer des r√©seaux multi-couches de neurones artificiels, quelle que soit leur fonction d'activation.  
Par cons√©quent, tout circuit logique peut √™tre repr√©sent√© sous la forme d'un MLP.



### 2. R√©seaux de neurones aritificiels

Un r√©seau neuronal est obtenu en connectant les sorties de certains neurones aux entr√©es d'autres neurones. Le but d'un tel r√©seau est g√©n√©ralement d'apprendre √† imiter une certaine fonction $f(x)$ pour laquelle on dispose de paires de donn√©es d'entra√Ænement $(x,y)$ avec $y = f(x) + \textrm{bruit}$. Ainsi, un tel r√©seau contient trois types de neurones:
- **Neurones d'entr√©e:** Correspondent aux diff√©rentes variables d'entr√©e $x_j$ d√©crivant les exemples d'entra√Ænement.
- **Neurones de sortie:** Correspondent aux cibles $y$ des exemples.
- **Neurones cach√©s:** Tout neurone qui n'est ni un neurone d'entr√©e ni un neurone de sortie.

Un r√©seau neuronal est donc un graphe de calcul, avec des entr√©es $x$ et des sorties $y$, o√π les n≈ìuds sont des neurones et les ar√™tes connectent la sortie d'un n≈ìud √† l'une des entr√©es d'un autre.

Une **couche** est un ensemble maximal de neurones non connect√©s, situ√©s √† la m√™me profondeur depuis la couche d'entr√©e.   
Un r√©seau neuronal organis√© en couches est appel√© un **r√©seau neuronal √† propagation avant** (*feedforward NN*).   
Certains r√©seaux neuronaux ne sont pas des r√©seaux √† propagation avant et contiennent des boucles. Ils sont appel√©s **r√©seaux neuronaux r√©currents** (*Recurrent NN*).   
Un r√©seau neuronal multicouche est souvent appel√© **perceptron multicouche** (pour des raisons historiques).

<h3 align='center'>
    <img src="img/nn.png" width="600px"></img>
</h3>



La sortie d'un neurone r√©sulte de l'application de la *fonction d'activation* $\sigma$ √† une combinaison lin√©aire de ses entr√©es: $z = \sigma(w^T x + b)$.   
Les param√®tres du r√©seau sont **tous les poids d'entr√©e et biais des neurones**.     
Un r√©seau neuronal est une fonction qui transforme ses entr√©es en sorties par propagation des valeurs dans le r√©seau.   
Apprendre un r√©seau neuronal consiste √† trouver les $w$ et $b$ pour que la sortie du r√©seau corresponde √† la fonction $f(x)$ qui a g√©n√©r√© les paires de donn√©es $(x, y = f(x)+\textrm{bruit})$.


#### **Th√©or√®me d'approximation universelle:**  
Si $\sigma$ est "en forme de S", alors avec suffisamment de neurones, un r√©seau neuronal √† une seule couche et √† propagation avant peut approximer n'importe quelle fonction continue avec une pr√©cision arbitraire.  


#### **Fonctions d'activation:**
Historiquement, il a √©t√© consid√©r√© une activation stricte en "0 ou 1" pour un certain neurone. Mais si l'entr√©e $x$ n'est plus binaire, lors du traitement de $x$, soit la stimulation du neurone $w^Tx$ est sup√©rieure √† $-b$, soit elle y est inf√©rieure. Cela rend la sortie d'un neurone tr√®s sensible au bruit dans l'entr√©e ou aux erreurs dans le r√©glage des poids.   
√Ä l'inverse, il faudrait d√©finir une fonction qui soit en forme de **S**, et qui passe **progressivement** de 0 √† 1.   
- **Fonction "step":**  
$$\sigma(x) = 0 \textrm{ si }x\leq0, \, 1 \textrm{ sinon}$$
- **Fonction lin√©aire:**  
$$\sigma(x) = x$$
- **Sigmo√Øde (logistique):**  
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$
- **Tangente hyperbolique:**  
$$\sigma(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$
- **Fonction de base radiale:** (utile dans des cas sp√©cifiques, comme les cartes de Kohonen)  
$$\sigma(x) = e^{-x^2}$$


<br>

## III. Propagation des valeurs √† travers un r√©seau

Soit un r√©seau neuronal avec la structure suivante:
- 2 neurones d'entr√©e.
- Une premi√®re couche cach√©e compos√©e de 4 neurones sigmo√Ødes.
- Une seconde couche cach√©e compos√©e de 3 neurones sigmo√Ødes.
- Une couche de sortie compos√©e d'un neurone d'identit√©.


<h3 align='center'>
    <img src="img/nn2.png" width="600px"></img>
</h3>

Pour l'instant les poids seront initialis√©s de mani√®re al√©atoire, en suivant une distribution $\mathcal{N}(0,1)$.

```python
sizes = [2,4,3,1]
num_layers = len(sizes)
biases = [np.random.randn(1,y) for y in sizes[1:]]
weights = [np.random.randn(out,inp) for inp,out in zip(sizes[:-1],sizes[1:])]

def sigmoid(z):
    """The sigmoid function."""
    return 1.0/(1.0+np.exp(-z))
``` 


La fonction qui calcule la propagation vers l'avant d'une entr√©e donn√©e (ex: $x=[1,2]$) √† travers le r√©seau est donn√© par:
```python
import numpy as np

input_value = np.array([[1,2]])

def forward_pass(x, verbose=False):
    z = [np.zeros((x.shape[0], sz)) for sz in sizes]
    y = [np.zeros((x.shape[0], sz)) for sz in sizes]
    z[0] = x.copy()

    for i in range(1, len(sizes)):
        if verbose:
            print("# Forward propagation to layer", i)
        y[i] = np.dot(z[i - 1],weights[i - 1].T) + biases[i - 1]

        if verbose:
            print("Neuron inputs:", y[i])

        if i == len(sizes) - 1:
            z[i] = y[i]

        else:
            z[i] = sigmoid(y[i])

        if verbose:
            print("Layer outputs:", z[i])

    return y, z

y, z = forward_pass(input_value, verbose=True)
print(y, z)
``` 

<br>

### IV. Apprentissage des poids d'un r√©seau neuronal (cas de r√©gression)
Il faut d√©sormais adapter les poids du r√©seau de mani√®re √† ce que lors de la propagation d'une entr√©e $x$, la pr√©diction soit au plus proche de $y$. 

>   *Ex: Si la valeur associ√©e √† $x = [1,2]$ dans l'exemple pr√©c√©dent soit $y_{true} = 12.3$, la pr√©diction √©tant $y_{pred} = 0.46081774$, il faut ajuster les poids du r√©seau de mani√®re √† ce que, la prochaine fois que $x$ est propag√© √† travers le r√©seau, la pr√©diction soit plus proche de $12.3$.*


Supposons tout d'abord que nos points de donn√©es $(x,y)$ sont tir√©s d'une distribution de probabilit√© $p(x,y)$.

#### 1. Minimisation du risque et fonctions de perte

Un r√©seau neuronal avec une structure de graphe fixe est une fonction param√©trique $f_\theta$, o√π $\theta$ est le **vecteur de tous les param√®tres (poids et biais).**  
Apprendre un r√©seau neuronal qui pr√©dit correctement $y$ revient √† trouver les param√®tres $\theta$ qui minimisent la fonction suivante:  
$$L(\theta) = \displaystyle \mathbb{E}_{(x,y)\sim p(x,y)} \left[ \left(f_\theta(x) - y\right)^2 \right] = \int_{x,y} \left[ \left(f_\theta(x) - y\right)^2 \right] \mathrm{d}p(x,y) $$

Cela d√©finit un probl√®me de minimisation des moindres carr√©s. 

La **minimisation du risque** revient donc √† trouver $f^{*} \in \arg\min_{f \in \mathcal{F}} L(f)$, o√π $\mathcal{F}$ est une famille de fonctions. Dans le cas pr√©sent, puisque la structure du r√©seau est fixe, cette famille correspond aux fonctions g√©n√©r√©es en faisant varier les param√®tres $\theta$.  

Cependant, en pratique, calculer le risque est impossible car $p(x,y)$ est inconnue. En apprentissage supervis√©, on utilise des ensembles d'entra√Ænement $\{(x_i,y_i)\}_{i\in [1,N]}$ o√π les points sont ind√©pendants et tir√©s selon $p(x,y)$. L'ensemble d'entra√Ænement permet de d√©finir une mesure empirique $\bar{p}(x,y)$, et on peut approximer le risque par le **risque empirique**:  
$$\bar{L}(f) = \mathbb{E}_{(x,y)\sim \bar{p}(x,y)} \left[ \ell(f(x),y) \right] = \frac{1}{N} \sum_{i=1}^N \ell(f(x_i),y_i).$$

Le risque empirique est une estimation de Monte Carlo du risque, bas√©e sur l'ensemble d'entra√Ænement. Cette g√©n√©ralisation est le principe de base de la **minimisation du risque empirique**, qui sous-tend l'optimisation des r√©seaux neuronaux.

```python
ypred, zpred = forward_pass(input_X, verbose=True)
pred = zpred[-1]
err = np.mean((pred - output_y)**2)
print("Empirical risk estimate:", err)
```


#### 2. Descente de gradient stochastique

Soit une estimation initiale $\theta_0$ pour les param√®tres de $f_\theta$. Comment ajuster cette estimation pour minimiser $L(\theta)$? La descente de gradient directe indique de se d√©placer dans la direction oppos√©e du gradient de $L(\theta)$ par rapport √† $\theta$. √âcrivons ce gradient:

$$\nabla_\theta L(\theta) = \nabla_\theta \left[ \mathbb{E}_{(x,y)\sim p(x,y)} \left[ \left(f_\theta(x) - y\right)^2 \right] \right]$$


$$\nabla_\theta L(\theta) = \mathbb{E}_{(x,y)\sim p(x,y)} \left[ \nabla_\theta \left[ \left(f_\theta(x) - y\right)^2 \right] \right]$$


$$\nabla_\theta L(\theta) = \mathbb{E}_{(x,y)\sim p(x,y)} \left[ 2 \left(f_\theta(x) - y\right) \nabla_\theta f_\theta(x) \right]$$

Ainsi, le gradient de $L(\theta)$ est l'esp√©rance de $2 \left(f_\theta(x) - y\right) \nabla_\theta f_\theta(x)$. En d'autres termes:

$$\nabla_\theta L(\theta) = \int_{x,y} 2 \left(f_\theta(x) - y\right) \nabla_\theta f_\theta(x) \mathrm{d}p(x,y)$$

Le probl√®me avec cette expression est qu'elle n√©cessite la connaissance de $p(x,y)$ pour tous les couples $(x,y)$ possibles (comme pour le calcul du risque). Cela n√©cessiterait une quantit√© infinie de donn√©es. Cependant, il est possible d'essayer de substituer le risque par le risque empirique et approximer ce gradient avec un ensemble de donn√©es fini $\left\{\left(x_i,y_i\right)\right\}_{i\in [1,N]}$ tir√© ind√©pendamment selon $p$:  
$$\nabla_\theta L(\theta) \approx \nabla_\theta \bar{L}(\theta) = \frac{1}{N}\sum_{i=1}^N 2 \left(f_\theta(x_i) - y_i\right) \nabla_\theta f_\theta(x_i)$$

Ceci est en fait une *estimation bruyante du gradient* (qui converge vers le vrai gradient dans le cas d'un √©chantillonnage infini).  
La th√©orie de la *descente de gradient stochastique* indique que si $g(\theta)$ est une estimation bruyante du gradient $\nabla_\theta L(\theta)$, alors la s√©quence $\theta_k$ converge vers un minimum local de $L(\theta)$:  
$$\theta_{k+1} = \theta_k - \alpha_k g(\theta_k)$$  
sous la condition que $\sum \alpha_k = \infty$ et $\sum \alpha_k^2 < \infty$ (conditions de Robbins-Monro).

La premi√®re condition $\sum \alpha_k = \infty$ garantit que, quel que soit le point de d√©part des param√®tres $\theta_0$, peu importe la distance du minimum, cette proc√©dure peut l'atteindre.  

La seconde condition $\sum \alpha_k^2 < \infty$ force les pas d'apprentissage √† √™tre une s√©quence d√©croissante et √©vite les oscillations autour du minimum.
 
$$g(\theta) = \frac{1}{N} \sum_{i=1}^N 2 \left(f_\theta(x_i) - y_i\right) \nabla_\theta f_\theta(x_i).$$


Un passage complet (pour calculer le $\sum_{i=1}^N$) sur l'ensemble de l'entra√Ænement sera appel√© une *√©poque d'entra√Ænement* (**epoch**).


#### 3. Mini-batch

Clarification de la notation: 

Il est possible de calculer:
$$g_{j}(\theta) = \frac{1}{N} \sum_{i=1}^N 2 \left(f_{\theta}(x_i) - y_i\right) \frac{\partial f_{\theta}}{\partial\theta_j}(x_i)$$
Et ainsi, il est possible de mettre √† jour $\theta_j$ avec:
$$\theta_j \leftarrow \theta_j - \alpha_k g_{j}(\theta)$$

*<u>Remarque:</u> $\theta_k$ fait r√©f√©rence au $k$-√®me vecteur de param√®tres dans la s√©quence ci-dessus, $\theta_j$ fait r√©f√©rence au $j$-√®me composant du vecteur $\theta$.*

Ainsi, le calcul de tous les composants de $g(\theta)$ et la mise √† jour de chaque √©l√©ment de $\theta$ peuvent √™tre effectu√©s en *parall√®le*.

Cependant, pour de grands ensembles de donn√©es, la sommation sur les $N$ √©l√©ments est co√ªteuse sur le plan computationnel.  

L'√©cart type de la moyenne empirique sur $n$ √©chantillons i.i.d. d'une variable al√©atoire $X$ est $\frac{\sigma}{\sqrt{n}}$, o√π $\sigma$ est l'√©cart type de la loi de $X$.
</details>

Ainsi, il est possible de d√©finir une version moins co√ªteuse (mais plus bruyante) de l'estimateur du gradient en sommant seulement sur un sous-ensemble al√©atoire de $n$ points d'entra√Ænement ($n \ll N$): 
$$\nabla_\theta L(\theta) \approx g(\theta) = \frac{1}{n} \sum_{i=1}^n 2 \left(f_\theta(x_i) - y_i\right) \nabla_\theta f_\theta(x_i) $$

Un tel sous-ensemble est appel√© un *minibatch*. Lorsque $n=1$, l'estimateur du gradient est bas√© sur un seul exemple et est donc tr√®s bruyant et la convergence peut √™tre tr√®s lente et instable. Lorsque $n\rightarrow N$, le niveau de bruit diminue au prix d'un co√ªt computationnel plus √©lev√©. En pratique, le niveau de bruit diminue suffisamment rapidement pour qu'il soit soit possible de prendre $n \in [50;1000]$ dans la plupart des cas.

En g√©n√©ral, des mini-batchs de taille fixe sont pris et le terme $\frac{1}{n}$ est omis dans l'estimation du gradient: il se fond avec le pas de l'apprentissage $\alpha_k$, appel√© √©galement **taux d'apprentissage**.



#### 4. Calcul de gradient r√©cursif

Pour que la mise √† jour d√©crite soit faisable, il est n√©cessaire d'avoir une fonction $f_\theta$ diff√©rentiable. Soit $\nabla_\theta f_\theta(x)$:
$$\nabla_\theta f_\theta(x) = \left[ \begin{array}{c} \vdots \\ \frac{\partial f_\theta}{\partial \theta_j}(x) \\ \vdots \end{array} \right]$$

Soit le neurone $j$ et $w_{ij}$ ses poids d'entr√©e (avec la convention que $i=0$ correspond au biais):
- $x_{ij}$ l'entr√©e $i$-√®me de ce neurone
- $y_j = \sum_i w_{ij} x_{ij}$ l'entr√©e scalaire √† la fonction d'activation
- $z_j = \sigma(y_j)$ la sortie du neurone

Ces trois quantit√©s ont √©t√© calcul√©es lors du *passage avant*, lorsque $x$ a √©t√© propag√© √† travers le r√©seau pour obtenir $f_\theta(x)$.

En utilisant la r√®gle de la cha√Æne pour √©crire $\frac{\partial f_\theta}{\partial w_{ij}}(x)$ comme une expression des d√©riv√©es partielles de $f_\theta$ par rapport √† $z_j$ et $y_j$:
$$\frac{\partial f_\theta}{\partial w_{ij}}(x) = \frac{\partial f_\theta}{\partial z_j}(x) \frac{\partial z_j}{\partial y_j}(x) \frac{\partial y_j}{\partial w_{ij}}(x)$$

Or $y_j = \sum_i w_{ij} x_{ij}$, donc:
$$\frac{\partial y_j}{\partial w_{ij}}(x) = x_{ij}$$

$z_j = \sigma(y_j)$, donc:
$$\frac{\partial z_j}{\partial y_j}(x) = \sigma'(y_j)$$

Et donc:
$$\boxed{\frac{\partial f_\theta}{\partial w_{ij}}(x) = \frac{\partial f_\theta}{\partial z_j}(x) \sigma'(y_j) x_{ij}}$$

Il reste √† calculer le premier terme dans l'expression ci-dessus.   
**<u>Cas 1:</u>** $j$ est un neurone de sortie, alors $z_j$ est le $j$-√®me composant de $f_\theta(x)$, et donc:
$$\frac{\partial f_\theta}{\partial z_j}(x) = 1$$

En cons√©quence, pour les neurones de la couche de sortie:
$$\boxed{\frac{\partial f_\theta}{\partial w_{ij}}(x) = \sigma'(y_j) x_{ij}}$$

Et donc, la mise √† jour des poids d'entr√©e $w_{ij}$ pour le neurone de sortie $j$ (dans la couche de sortie) est:
$$w_{ij} \leftarrow w_{ij} - \alpha \left(f_\theta(x) - y\right)\sigma'(y_j) x_{ij}$$


**<u>Cas 2:</u>**  $j$ est un neurone de la couche juste avant la couche de sortie.   
Soit $L_j$ l'ensemble des indices des neurones qui alimentent directement la sortie du neurone $j$. $z_j$ correspond √† la variable $x_{jl}$ pour ces neurones et $y_l$ est l'entr√©e scalaire du neurone $l$.

En prenant la d√©riv√©e totale:
$$\frac{\partial f_\theta}{\partial z_j}(x) = \sum_{l \in L_j} \frac{\partial f_\theta}{\partial y_l}(x) \frac{\partial y_l}{\partial z_j}(x)$$

Alors:
$$\frac{\partial f_\theta}{\partial z_j}(x) = \sum_{l \in L_j} \frac{\partial f_\theta}{\partial z_l}(x) \frac{\partial z_l}{\partial y_l}(x) \frac{\partial y_l}{\partial z_j}(x)$$

Comme pr√©c√©demment:
$$\frac{\partial z_l}{\partial y_l}(x) = \sigma'(y_l) \text{ et } \frac{\partial y_l}{\partial z_j}(x) = w_{jl}$$

Donc cette d√©riv√©e totale devient:
$$\boxed{\frac{\partial f_\theta}{\partial z_j}(x) = \sum_{l \in L_j} \frac{\partial f_\theta}{\partial z_l}(x) \sigma'(y_l) w_{jl}}$$

Cela fournit une relation de r√©currence entre $\frac{\partial f_\theta}{\partial z_j}(x)$ et $\frac{\partial f_\theta}{\partial z_l}(x)$ pour $l \in L_j$.

Soit $\delta_j = \frac{\partial f_\theta}{\partial z_j}(x) \sigma'(y_j)$:
$$\frac{\partial f_\theta}{\partial w_{ij}}(x) = \frac{\partial f_\theta}{\partial z_j}(x) \frac{\partial z_j}{\partial y_j}(x) \frac{\partial y_j}{\partial w_{ij}}(x) = \frac{\partial f_\theta}{\partial z_j}(x) \sigma'(y_j) x_{ij} = \delta_j x_{ij},$$
$$\boxed{\frac{\partial f_\theta}{\partial w_{ij}}(x) = \delta_j x_{ij}}$$

Gr√¢ce √† l'√©quation de r√©currence entre le neurone $j$ et ses fr√®res dans $L_j$:
$$\delta_j = \frac{\partial f_\theta}{\partial z_j}(x) \sigma'(y_j) = \sigma'(y_j) \sum_{l \in L_j} \delta_l w_{jl}$$

Et, en particulier, pour les neurones de sortie:
$$\delta_j = \frac{\partial f_\theta}{\partial z_j}(x) \sigma'(y_j) = \sigma'(y_j)$$

En r√©sum√©:
$$\boxed{\delta_j = \left\{ \begin{array}{ll}
\sigma'(y_j) & \text{pour les neurones de sortie,} \\
\sigma'(y_j) \sum_{l \in L_j} \delta_l w_{jl} & \text{pour les autres neurones.}
\end{array} \right.}$$


#### 5. Backpropagation

La cl√© de la r√©tropropagation est de remarquer que dans tous les cas:
$$\frac{\partial f_\theta}{\partial w_{ij}}(x) = \delta_j x_{ij}$$

Si le neurone $j$ est un neurone de sortie, alors $z_j$ est le $j$-√®me composant de $f_\theta(x)$. Ainsi, $\frac{\partial f_\theta}{\partial z_j}(x) = 1$. En cons√©quence, pour ces neurones:
$$\delta_j = \frac{\partial f_\theta}{\partial z_j}(x) \sigma'(y_j) = \sigma'(y_j)$$

R√©cursivement, une fois que tous les $\delta_j$ pour la couche de sortie ont √©t√© calcul√©s, il est possible de calculer les $\delta_j$ pour la derni√®re couche cach√©e:
$$\delta_j = \sigma'(y_j) \sum_{l \in L_j} \delta_l w_{jl}$$

Et les poids d'entr√©e du neurone $j$ peuvent √™tre mis √† jour:
$$w_{ij} \leftarrow w_{ij} - \alpha \left(f_\theta(x) - y\right) \delta_j x_{ij}$$

Une fois que tous ces poids ont √©t√© mis √† jour et que tous les $\delta_j$ ont √©t√© calcul√©s pour les neurones correspondants, il est possible d'avancer une couche en arri√®re dans le r√©seau, et ainsi de suite jusqu'√† atteindre la couche d'entr√©e.

Cet algorithme est appel√© *r√©tropropagation* (**backpropagation**) du gradient. La r√©tropropagation permet au r√©seau d'apprendre comment ajuster les poids en fonction de l'erreur, optimisant ainsi la performance du mod√®le sur les donn√©es d'entra√Ænement.




<br>

L'algorithme de r√©tropropagation permet de mettre √† jour les poids du r√©seau.

**Propagation avant:**
<ol style="list-style-type:none">

- Entr√©e $x$
- $\lambda =$ couche d'entr√©e
- Tant que $\lambda \neq$ couche de sortie:
  <ol style="list-style-type:none">

    - Pour $j$ dans $\lambda$:   
      -> Calculer $y_j = \sum w_{ij} x_{ij}$ et $z_j = \sigma(y_j)$
    - $\lambda \leftarrow$ couche suivante
    - $x \leftarrow z$
  </ol>

- Sortie $f_\theta(x)$
</ol>

**R√©tropropagation:**
<ol style="list-style-type:none">

- Diff√©rence de la sortie $\Delta = f_\theta(x) - y$
- Pour $j$ dans la couche de sortie, $\delta_j = \sigma'(y_j)$
- $\lambda =$ couche de sortie
- Tant que $\lambda \neq$ couche d'entr√©e:
  <ol style="list-style-type:none">

    - Pour $j$ dans $\lambda$:
      <ol style="list-style-type:none">

        - Calculer $\delta_j = \sigma'(y_j) \sum_{l \in L} \delta_l w_{jl}$ (sauf pour la couche de sortie)
        - Mettre √† jour $w_{ij} \leftarrow w_{ij} - \alpha \Delta \delta_j x_{ij}$
      </ol>

    - $\lambda =$ couche pr√©c√©dente
  </ol>
</ol>

<br>

Un certain nombre de remarques peuvent √™tre faites pour rendre cette computation plus fluide et efficace.

1. $\sigma'(x) = \sigma(x) \left(1 - \sigma(x)\right)$ permet d'obtenir $\sigma'$ gratuitement lors de la propagation avant et de le stocker.
2. Toutes les op√©rations de la propagation arri√®re peuvent √™tre √©crites sous forme matricielle (tout comme pour la propagation avant).
3. Dans les notations ci-dessus, $x_{0j} = 1$ car il s'agit du terme qui sera multipli√© par le biais.
4. Pour un donn√© $j$, tous les $x_{ij}$ dans les notations ci-dessus sont vraiment la valeur $z$ de la couche avant le neurone $j$.

On peut facilement r√©√©crire la propagation avant et la r√©tropropagation comme des op√©rations matricielles/vecteurs.
Soient $\lambda$ le num√©ro de la couche, en commen√ßant √† 0 pour la couche d'entr√©e. Soit $w_{\lambda-1}$ la matrice de poids $p \times q$ avant la couche $\lambda$, o√π $p$ est la taille de la couche $\lambda$ et $q$ est la taille de la couche $\lambda-1$ (plus un pour les biais). Enfin, $\circ$ d√©note le produit √©l√©ment par √©l√©ment (produit Hadamard) de deux matrices.

**Propagation avant:**
<ol style="list-style-type:none">

- Entr√©e $x$
- $\lambda = 1$
- Tant que $\lambda \neq$ index de la couche de sortie:
  <ol style="list-style-type:none">

    - Calculer $y_\lambda = w_{\lambda-1}^T x$,
    - Calculer $z_\lambda = \sigma(y_\lambda)$ et $s_\lambda = \sigma'(y_\lambda)$
    - $\lambda \leftarrow \lambda + 1$
    - $x \leftarrow z_\lambda$
  </ol>
- Sortie $f_\theta(x)$
</ol>

**R√©tropropagation:**
<ol style="list-style-type:none">

- Diff√©rence de la sortie $\Delta = f_\theta(x) - y$
- $\lambda =$ index de la couche de sortie
- $\delta_\lambda = s_\lambda$
- $w_{\lambda-1} \leftarrow w_{\lambda-1} - \alpha \Delta (\delta_\lambda \circ z_{\lambda-1}^T)$
- $\lambda \leftarrow \lambda - 1$
- Tant que $\lambda \neq 0$:
  <ol style="list-style-type:none">

    - $\delta_\lambda = s_\lambda \circ (\delta_{\lambda + 1} \cdot w_\lambda)$
    - $w_{\lambda-1} \leftarrow w_{\lambda-1} - \alpha \Delta (\delta_\lambda \circ z_{\lambda-1}^T)$
    - $\lambda \leftarrow \lambda - 1$
  </ol>
</ol>


### V. MLP avec `scikit-learn`

```python
# Import necessary libraries
from sklearn.neural_network import MLPRegressor

# Initialize the MLPRegressor (Multi-layer Perceptron) model with specific parameters
NN = MLPRegressor(
    hidden_layer_sizes=(100, 10),   # Tuple specifying the number and size of hidden layers
    activation='tanh',               # The activation function to be used for hidden layers ('tanh' or 'relu' are common choices)
    solver='lbfgs',                   # The optimization algorithm to use ('lbfgs', 'sgd', 'adam')
    max_iter=5000,                    # Maximum number of iterations allowed for convergence
    learning_rate_init=0.1            # Initial learning rate for weight updates
)

# Fit the model to the data
NN.fit(X, Y)

# Make predictions
y_predict = NN.predict(X)

# Plot the graph
fig=plt.figure(figsize=(22, 8), dpi=80, facecolor='w', edgecolor='k')
plt.plot(X.ravel(), Y.ravel(), 'r.', markersize=10, label=u'Observations')
plt.plot(x, y_real, 'b', label=u'$f(x)$')
plt.plot(X, y_predict, 'g', label=u'$NN(x)$')
plt.xlabel('$x$')
plt.ylabel('$f(x)$')
plt.legend(loc='upper left')
```


### VI. R√©seaux de neurones pour la classification

La d√©rivation √©crite ci-dessus peut √™tre r√©p√©t√©e pour d'autres fonctions de co√ªt. En particulier, pour les t√¢ches de classification, si l'on a $K$ classes avec $p_k$ les probabilit√©s des classes cibles pour l'entr√©e $x$, la fonction de co√ªt √† *entropie crois√©e* (**cross-entropy**) est couramment utilis√©e en classification:
$$L(\theta) = \sum_{k=1}^K p_k \log f_\theta(x)$$

Scikit-learn offre une API facile √† utiliser pour la classification, comme illustr√© ci-dessous, mais sa flexibilit√© reste limit√©e et PyTorch offre une meilleure API.


```python
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split

Xtrain, ytrain, Xtest, ytest = data.split(2000)

NN = MLPClassifier(
    hidden_layer_sizes=(250),
    learning_rate_init=0.01,
    activation='relu')

NN.fit(Xtrain, ytrain)
print(f"NN score: {NN.score(Xtest, ytest)}")
```