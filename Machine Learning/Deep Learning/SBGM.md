<h1 align='center'> Deep learning - Score-Based Generative Modeling üéØ</h1>

Les mod√®les bas√©s sur le score, √©galement appel√©s mod√®les de diffusion, sont une famille d'approches bas√©es sur [l'estimation des gradients de la distribution des donn√©es](https://arxiv.org/abs/1907.05600). Ces m√©thodes g√©n√®rent des √©chantillons en √©chantillonnant √† partir d'une distribution al√©atoire, puis en suivant une estimation du gradient de la distribution des donn√©es pour construire des √©chantillons √† partir de la distribution apprise. 

<img src="img/diffusion_models.png">



## I. Introduction

### 1. **Id√©es cl√©s**
- **Score de densit√©**: 
   - Le score d'une distribution $p(x)$ est d√©fini comme le gradient du logarithme de la densit√©:
     $$\nabla_x \log p(x)$$
   - Il s'agit d'une direction qui indique o√π la probabilit√© $p(x)$ augmente le plus rapidement dans l'espace des donn√©es.

- **Objectif**:
   - L'objectif du SBGM est de mod√©liser le **score** (et non directement la densit√©) pour des distributions complexes. Une fois le score appris, on peut g√©n√©rer des √©chantillons en suivant une dynamique stochastique appropri√©e.

- **Diffusion et Bruit**:
   - Pour simplifier l'apprentissage des scores, les donn√©es sont progressivement bruit√©es jusqu'√† une distribution simple (comme une gaussienne). Ce processus est appel√© **processus de diffusion avant**.
   - Le **processus inverse** consiste √† enlever le bruit de mani√®re progressive, en suivant les gradients appris, pour revenir aux donn√©es d'origine.

<br>

### 2. **Composants fondamentaux**
- **Processus de Bruitage**:
   - On ajoute du bruit aux donn√©es pour obtenir une s√©quence de distributions $p_t(x)$, o√π $t$ indique un niveau de bruit.
   - Par exemple, dans un processus gaussien, les donn√©es sont progressivement transform√©es en bruit pur $\mathcal{N}(0, I)$:
     $$p_t(x) = \mathcal{N}(x; \sqrt{\alpha_t}x_0, (1 - \alpha_t)I)$$

- **Apprentissage du Score**:
   - Le r√©seau neuronal $s_\theta(x, t)$ est entra√Æn√© pour approximer le score $\nabla_x \log p_t(x)$ pour chaque niveau $t$.
   - La perte est g√©n√©ralement bas√©e sur une mesure comme:
     $$\mathcal{L} = \mathbb{E}_{p_t(x)} \left[ \| s_\theta(x, t) - \nabla_x \log p_t(x) \|^2 \right]$$

- **Processus Inverse**:
   - Une fois que $s_\theta(x, t)$ est appris, un processus stochastique inverse est utilis√© pour transformer une gaussienne en donn√©es r√©alistes. Cela repose sur des dynamiques telles que les √©quations de Langevin ou les √©quations diff√©rentielles stochastiques (SDEs).

<br>

### 3. **√âtapes d'un SBGM**

- **Ajout de bruit progressif**:
   - Transformez les donn√©es d'origine $p(x)$ en bruit gaussien $p_T(x)$ via un processus de diffusion.

- **Entra√Ænement d'un mod√®le de score**:
   - Entra√Ænez un mod√®le $s_\theta(x, t)$ √† pr√©dire les gradients de densit√© $\nabla_x \log p_t(x)$.

- **G√©n√©ration de donn√©es**:
   - Partant d'une distribution simple (comme $\mathcal{N}(0, I)$), utilisez les gradients $s_\theta(x, t)$ pour remonter vers la distribution des donn√©es via un processus stochastique.

<br>

L'une des applications les plus populaires du SBGM est la g√©n√©ration d'images r√©alistes. Par exemple, **Denoising Diffusion Probabilistic Models (DDPM)** utilise cette m√©thode pour g√©n√©rer des images √† partir de bruit pur.

---

### **Code Python pour un SBGM avec PyTorch**

Voici un exemple simple de diffusion et de g√©n√©ration avec un processus de diffusion gaussienne:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Hyperparam√®tres
timesteps = 1000
batch_size = 64
image_size = 28 * 28  # MNIST images
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Processus de bruitage
def noise_schedule(t, beta_1=0.0001, beta_T=0.02):
    return beta_1 + t * (beta_T - beta_1) / (timesteps - 1)

betas = torch.tensor([noise_schedule(t) for t in range(timesteps)], device=device)
alphas = 1 - betas
alpha_cumprod = torch.cumprod(alphas, dim=0)

# Diffusion: Bruit progressif
def forward_diffusion(x_0, t, noise):
    sqrt_alpha_cumprod_t = alpha_cumprod[t].sqrt()
    sqrt_one_minus_alpha_t = (1 - alpha_cumprod[t]).sqrt()
    return sqrt_alpha_cumprod_t * x_0 + sqrt_one_minus_alpha_t * noise

# Mod√®le de score
class ScoreNetwork(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(ScoreNetwork, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim)
        )
        
    def forward(self, x, t):
        return self.model(x)

score_model = ScoreNetwork(image_size, 256).to(device)
optimizer = optim.Adam(score_model.parameters(), lr=1e-4)

# Entra√Ænement
def train_score_model(data_loader, epochs):
    for epoch in range(epochs):
        for x_0 in data_loader:
            x_0 = x_0.view(-1, image_size).to(device)
            t = torch.randint(0, timesteps, (batch_size,), device=device)
            noise = torch.randn_like(x_0)
            x_t = forward_diffusion(x_0, t, noise)
            
            predicted_noise = score_model(x_t, t)
            loss = nn.MSELoss()(predicted_noise, noise)
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
        print(f"Epoch {epoch + 1}: Loss = {loss.item()}")

# G√©n√©ration
def sample(score_model, steps=timesteps):
    x = torch.randn(batch_size, image_size, device=device)
    for t in reversed(range(steps)):
        beta_t = betas[t]
        alpha_t = alphas[t]
        sqrt_one_minus_alpha_t = (1 - alpha_t).sqrt()
        
        score = score_model(x, t)
        noise = torch.randn_like(x) if t > 0 else 0
        x = (1 / (1 - beta_t).sqrt()) * (x - beta_t * score) + sqrt_one_minus_alpha_t * noise
    
    return x

# Exemple de donn√©es MNIST
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
mnist = datasets.MNIST('./data', train=True, download=True, transform=transform)
data_loader = DataLoader(mnist, batch_size=batch_size, shuffle=True)

# Entra√Æner et g√©n√©rer
train_score_model(data_loader, epochs=5)
generated_images = sample(score_model)
```



<br>
<br>


## II. Perturbation des donn√©es par un processus de diffusion

Un [processus de diffusion](https://en.wikipedia.org/wiki/Diffusion_process) corrompt progressivement les donn√©es en ajoutant du bruit al√©atoire, jusqu'√† ce que les donn√©es ressemblent √† du bruit pur. Un processus de diffusion est un exemple de [processus stochastique](https://en.wikipedia.org/wiki/Stochastic_process#:~:text=A%20stochastic%20or%20random%20process%20can%20be%20defined%20as%20a,an%20element%20in%20the%20set.), tel que le [mouvement brownien](https://en.wikipedia.org/wiki/Brownian_motion). La trajectoire d‚Äôun processus de diffusion peut √™tre mod√©lis√©e par une √©quation diff√©rentielle stochastique (EDS):

$$d \mathbf{x} = \mathbf{f}(\mathbf{x}, t) d t + g(t) d \mathbf{w}$$

### Variables et fonctions:
- $\mathbf{f}(\mathbf{x}, t)$: coefficient de d√©rive d√©terministe.
- $g(t)$: coefficient de diffusion, modulant l'intensit√© du bruit.
- $\mathbf{w}$: mouvement brownien standard.

Pour la mod√©lisation g√©n√©rative, un processus de diffusion est choisi tel que $\mathbf{x}(0) \sim p_0$ (distribution des donn√©es d'origine) et $\mathbf{x}(T) \sim p_T$ (distribution gaussienne simple). 

### Exemple de processus de diffusion
Une EDS simple est d√©finie par:
$$d \mathbf{x} = \sigma^t d\mathbf{w}, \quad t \in [0, 1]$$

Dans ce cas:
$$p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0)) = \mathcal{N}\bigg(\mathbf{x}(t) ; \mathbf{x}(0), \frac{1}{2\log \sigma}(\sigma^{2t} - 1) \mathbf{I}\bigg)$$

Lorsque $\sigma$ est grand, la distribution pr√©alable, $p_{t=1}$ est 
$$\int p_0(\mathbf{y})\mathcal{N}\bigg(\mathbf{x} ; \mathbf{y}, \frac{1}{2 \log \sigma}(\sigma^2 - 1)\mathbf{I}\bigg) d \mathbf{y} \approx \mathbf{N}\bigg(\mathbf{x} ; \mathbf{0}, \frac{1}{2 \log \sigma}(\sigma^2 - 1)\mathbf{I}\bigg),$$
qui est approximativement ind√©pendante de la distribution des donn√©es et dont l'√©chantillonnage est facile √† r√©aliser.

Intuitivement, cette EDD capture un continuum de perturbations gaussiennes avec une fonction de variance $\frac{1}{2 \log \sigma}(\sigma^{2t} - 1)$. Ce continuum de perturbations permet de transf√©rer progressivement des √©chantillons d'une distribution de donn√©es $p_0$ √† une distribution gaussienne simple $p_1$.


<br>

### Impl√©mentation en Python
Voici une illustration d'un processus de diffusion gaussien avec une simulation:

```python
import numpy as np
import matplotlib.pyplot as plt

def diffusion_process(x0, sigma, timesteps):
    t_vals = np.linspace(0, 1, timesteps)
    x_vals = [x0]
    for t in t_vals[1:]:
        noise = np.random.normal(0, sigma**t, size=x0.shape)
        x_vals.append(x_vals[-1] + noise)
    return np.array(x_vals)

# Param√®tres
x0 = np.array([0.0])  # Position initiale
sigma = 1.2  # Param√®tre de diffusion
timesteps = 100

# Simulation
trajectory = diffusion_process(x0, sigma, timesteps)
plt.plot(np.linspace(0, 1, timesteps), trajectory)
plt.title("Processus de diffusion")
plt.xlabel("Temps")
plt.ylabel("Position")
plt.show()
```

<br>
<br>



## III. Estimation du score

Le score d√©pendant du temps, $\nabla_\mathbf{x} \log p_t(\mathbf{x})$, est utilis√© pour inverser le processus de diffusion. L'objectif est de former un mod√®le bas√© sur le score, $s_\theta(\mathbf{x}, t)$, afin d'approximer ce gradient. La fonction de perte utilis√©e pour entra√Æner $s_\theta$ est:

$$\min_\theta \mathbb{E}_{t\sim \mathcal{U}(0, T)} [\lambda(t) \mathbb{E}_{\mathbf{x}(0) \sim p_0(\mathbf{x})}\mathbf{E}_{\mathbf{x}(t) \sim p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))}[ \|s_\theta(\mathbf{x}(t), t) - \nabla_{\mathbf{x}(t)}\log p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))\|_2^2]]$$

o√π $\mathcal{U}(0,T)$ est une distribution uniforme sur $[0, T]$, $p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))$ repr√©sente la probabilit√© de transition de $\mathbf{x}(0)$ √† $\mathbf{x}(t)$, et $\lambda(t) \in \mathbb{R}_{>0}$ d√©signe une fonction de pond√©ration positive.   

Pour l'EDD d√©finie, il est possible d'utiliser la fonction de pond√©ration $\lambda(t) = \frac{1}{2 \log \sigma}(\sigma^{2t} - 1)$.



Dans l'objectif, l'esp√©rance sur $\mathbf{x}(0)$ peut √™tre estim√©e avec des moyennes empiriques sur des √©chantillons de donn√©es de $p_0$. L'esp√©rance sur $\mathbf{x}(t)$ peut √™tre estim√©e par √©chantillonnage √† partir de $p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0))$, ce qui est efficace lorsque le coefficient de d√©rive $\mathbf{f}(\mathbf{x}, t)$ est affine. La fonction de poids $\lambda(t)$ est g√©n√©ralement choisie pour √™tre inversement proportionnelle √† $\mathbb{E}[\|\nabla_{\mathbf{x}}\log p_{0t}(\mathbf{x}(t) \mid \mathbf{x}(0)) \|_2^2]$.


<br>

### Impl√©mentation en Python
L'estimation du score peut √™tre simul√©e en utilisant des √©chantillons bruit√©s. Voici un exemple:

```python
import torch
import torch.nn as nn
import torch.optim as optim

# R√©seau de score
class ScoreModel(nn.Module):
    def __init__(self):
        super(ScoreModel, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(2, 128),
            nn.ReLU(),
            nn.Linear(128, 2)
        )

    def forward(self, x, t):
        t_embed = torch.cat([torch.sin(2 * np.pi * t), torch.cos(2 * np.pi * t)], dim=-1)
        x_t = torch.cat([x, t_embed], dim=-1)
        return self.net(x_t)

# Entra√Ænement
model = ScoreModel()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.MSELoss()

for epoch in range(100):
    x_t = torch.randn(32, 2)  # √âchantillons bruit√©s
    t = torch.rand(32, 1)     # Temps al√©atoire
    score_true = -x_t         # Approximation simplifi√©e du vrai gradient
    score_pred = model(x_t, t)
    loss = criterion(score_pred, score_true)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

<br>
<br>


## IV. √âchantillonnage avec des solveurs num√©riques d'EDD

Pour √©chantillonner la distribution initiale $\mathbf{x}(0)$, l'EDS en temps inverse est r√©solue. Pour l'EDS directe:

$$d \mathbf{x} = \sigma^t d\mathbf{w}$$

L'EDS inverse devient:

$$d\mathbf{x} = -\sigma^{2t} \nabla_\mathbf{x} \log p_t(\mathbf{x}) dt + \sigma^t d \bar{\mathbf{w}}$$


Car pour toute EDD de la forme

$$d \mathbf{x} = \mathbf{f}(\mathbf{x}, t) dt + g(t) d\mathbf{w}$$

l'EDD en temps inverse est donn√©e par

$$d \mathbf{x} = [\mathbf{f}(\mathbf{x}, t) - g(t)^2 \nabla_\mathbf{x} \log p_t(\mathbf{x})] dt + g(t) d \bar{\mathbf{w}}$$


### M√©thode d'Euler-Maruyama en Python
Une impl√©mentation simple pour simuler l'inversion de l'EDS est donn√©e ci-dessous:

```python
def euler_maruyama(x_t, model, sigma, timesteps, delta_t):
    for t in reversed(range(1, timesteps)):
        t_scaled = t / timesteps
        score = model(torch.tensor(x_t, dtype=torch.float32), torch.tensor([[t_scaled]]))
        noise = np.random.normal(0, sigma**t_scaled, size=x_t.shape)
        x_t = x_t - sigma**(2 * t_scaled) * score.detach().numpy() * delta_t + sigma**t_scaled * np.sqrt(delta_t) * noise
    return x_t

# Param√®tres
x_t = np.random.normal(0, 1, size=(1, 2))  # Distribution pr√©alable
delta_t = 0.01
timesteps = 100
sigma = 1.2

# √âchantillonnage
x_0 = euler_maruyama(x_t, model, sigma, timesteps, delta_t)
print("√âchantillon g√©n√©r√©:", x_0)
```

<br>
<br>

## V. Entra√Ænement et √©valuation du mod√®le

La qualit√© des √©chantillons est √©valu√©e en mesurant la similarit√© entre les distributions g√©n√©r√©es et les donn√©es d'origine. Les techniques d'√©valuation incluent:
- Le calcul du score FID (Frechet Inception Distance) pour des √©chantillons g√©n√©r√©s.
- La comparaison visuelle pour des ensembles de donn√©es d'images.



Voici une impl√©mentation qui d√©taille comment entra√Æner un mod√®le bas√© sur le score et g√©n√©rer des √©chantillons en utilisant un processus de diffusion:

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np

# Configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
sigma = 25.0  # Coefficient de diffusion
T = 1.0  # Temps final
num_steps = 1000  # Nombre de pas pour la discr√©tisation
delta_t = T / num_steps  # Pas de temps
lr = 1e-4  # Taux d'apprentissage
batch_size = 64  # Taille de batch
image_size = 28  # Taille des images (MNIST)
channels = 1  # Canaux d'entr√©e (1 pour MNIST)

# D√©finition du mod√®le bas√© sur le score
class ScoreNetwork(nn.Module):
    def __init__(self):
        super(ScoreNetwork, self).__init__()
        self.net = nn.Sequential(
            nn.Conv2d(channels, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(128, 64, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, channels, 3, padding=1)
        )

    def forward(self, x, t):
        # Encode le temps `t` comme une caract√©ristique sinusoidale et concat√®ne
        t_embed = torch.cat([torch.sin(2 * np.pi * t), torch.cos(2 * np.pi * t)], dim=-1)
        t_embed = t_embed.unsqueeze(-1).unsqueeze(-1)  # Adapter les dimensions pour l'ajouter √† `x`
        t_embed = t_embed.expand_as(x)
        return self.net(x + t_embed)

# Processus de bruit
def perturb_data(x, t, sigma):
    noise = torch.randn_like(x)
    scale = (1 / (2 * np.log(sigma)) * (sigma ** (2 * t) - 1)) ** 0.5
    return x + scale * noise

# Calcul du score analytique pour l'entra√Ænement
def compute_true_score(x, x0, t, sigma):
    scale = (1 / (2 * np.log(sigma)) * (sigma ** (2 * t) - 1)) ** 0.5
    return -(x - x0) / scale ** 2

# Pr√©parer les donn√©es MNIST
transform = transforms.Compose([
    transforms.Resize(image_size),
    transforms.ToTensor(),
    lambda x: x * 2 - 1  # Normalisation √† [-1, 1]
])
dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Initialisation du mod√®le et de l'optimiseur
model = ScoreNetwork().to(device)
optimizer = optim.Adam(model.parameters(), lr=lr)
loss_fn = nn.MSELoss()

# Entra√Ænement
for epoch in range(10):  # Nombre d'√©poques
    for x, _ in dataloader:
        x = x.to(device)
        t = torch.rand((x.size(0),), device=device)  # √âchantillons de `t ~ U(0, T)`
        x_t = perturb_data(x, t, sigma)  # Donn√©es bruit√©es
        true_score = compute_true_score(x_t, x, t, sigma)  # Score analytique

        # Pr√©diction du mod√®le
        pred_score = model(x_t, t)

        # Calcul et r√©tropropagation de la perte
        loss = loss_fn(pred_score, true_score)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch + 1}, Loss: {loss.item()}")

# √âchantillonnage avec la m√©thode Euler-Maruyama
def sample(model, num_samples, num_steps, sigma, device):
    x = torch.randn((num_samples, channels, image_size, image_size), device=device)  # √âchantillons initiaux
    for step in reversed(range(num_steps)):
        t = torch.tensor([step / num_steps], device=device)
        score = model(x, t)
        noise = torch.randn_like(x) if step > 0 else 0
        x = x + sigma ** (2 * t) * score * delta_t + sigma ** t * (delta_t ** 0.5) * noise
    return x

# G√©n√©ration d'√©chantillons
samples = sample(model, num_samples=16, num_steps=num_steps, sigma=sigma, device=device)

# Affichage des √©chantillons g√©n√©r√©s
import matplotlib.pyplot as plt

samples = samples.detach().cpu().numpy()
fig, axes = plt.subplots(4, 4, figsize=(8, 8))
for i, ax in enumerate(axes.flat):
    ax.imshow(samples[i, 0], cmap='gray')
    ax.axis('off')
plt.show()
```

### R√©sum√© des √©tapes du code
1. **D√©finition du mod√®le** : Un r√©seau convolutif est d√©fini pour approximer le score conditionnel au temps.
2. **Perturbation des donn√©es** : Les donn√©es sont bruit√©es selon le processus de diffusion d√©fini.
3. **Entra√Ænement** : Le mod√®le est entra√Æn√© pour minimiser la diff√©rence entre le score pr√©dit et le score analytique.
4. **√âchantillonnage** : La m√©thode Euler-Maruyama est utilis√©e pour inverser le processus de diffusion et g√©n√©rer des donn√©es.

Ce code peut √™tre ex√©cut√© sur des images MNIST pour voir les √©chantillons g√©n√©r√©s √† partir d'une distribution gaussienne bruit√©e.