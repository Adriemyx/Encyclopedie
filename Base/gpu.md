<h1 align='center'> GPU üñ•</h1>


L'utilisation des GPU (unit√©s de traitement graphique) pour acc√©l√©rer les calculs en Python est devenue courante gr√¢ce √† l'int√©gration de CUDA (Compute Unified Device Architecture) de NVIDIA. CUDA permet de tirer parti de la puissance de calcul massive des GPU pour effectuer des t√¢ches qui seraient autrement lentes avec des CPU seuls. Cette technologie est particuli√®rement utile dans le contexte de l'apprentissage automatique, du traitement d'image, de la vision par ordinateur, du traitement du langage naturel et d'autres domaines n√©cessitant des calculs intensifs. Il y a donc plusieurs avantages √† utiliser des GPU:

- **Performance accrue**: Les GPU offrent des performances bien sup√©rieures aux CPU pour de nombreux types de calculs, notamment les multiplications de matrices, les r√©seaux de neurones profonds et les autres op√©rations math√©matiques lourdes.
- **Parall√©lisation**: Les GPU poss√®dent de nombreux cores de traitement capables d'effectuer des op√©rations en parall√®le, ce qui est id√©al pour les calculs vectoriels et matriciels massifs utilis√©s dans le deep learning.
- **R√©duction de la latence**: Utiliser un GPU peut consid√©rablement r√©duire le temps de calcul, acc√©l√©rant ainsi le processus d'entra√Ænement et d'inf√©rence pour des mod√®les complexes.

<br>
<br>

## I. **Pr√©requis pour l'utilisation de CUDA avec Python**
Avant d'utiliser CUDA avec Python, il faut s'assurer que:
- **CUDA**: Avoir install√© CUDA sur votre machine. CUDA est une technologie de NVIDIA permettant aux d√©veloppeurs d'utiliser le GPU comme unit√© de calcul parall√®le.
- **CuDNN**: Pour le deep learning, il faut avoir √©galement besoin de CuDNN (CUDA Deep Neural Network), qui est une librairie optimis√©e pour les r√©seaux neuronaux profonds avec CUDA.
- **Pip**: il faut s'assurer d'avoir `pip` install√© pour installer les librairies Python n√©cessaires.

<br>
<br>

## II. **Installation des librairies Python pour CUDA**
Pour utiliser CUDA avec Python, les principales librairies √† installer sont:
- `torch` pour PyTorch
- `tensorflow-gpu` pour TensorFlow
- `cupy` pour des calculs numpy-like acc√©l√©r√©s par CUDA
- `pycuda` pour le calcul GPU direct √† travers CUDA

Voici un exemple pour installer PyTorch avec CUDA support:
```bash
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu11.0
```

<br>
<br>

## III. **Code Exemple avec PyTorch**
L'exemple suivant montre comment utiliser PyTorch avec CUDA pour entra√Æner un simple r√©seau de neurones sur un GPU.

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

# G√©n√©ration de donn√©es fictives
X = np.random.randn(1000, 20)  # 1000 √©chantillons, 20 features
y = (np.random.randn(1000) > 0).astype(int)  # Classes binaires

# Conversion en tensreurs PyTorch
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.int64)

# Cr√©ation du jeu de donn√©es et dataloader
dataset = TensorDataset(X_tensor, y_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# D√©finition du mod√®le
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(20, 50)
        self.fc2 = nn.Linear(50, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x

# Envoi du mod√®le sur le GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SimpleNN().to(device)

# Optimiseur et fonction de perte
criterion = nn.BCELoss()  # Binary Cross Entropy Loss
optimizer = optim.SGD(model.parameters(), lr=0.01)

# Entra√Ænement du mod√®le
num_epochs = 100
for epoch in range(num_epochs):
    for batch in dataloader:
        X_batch, y_batch = batch
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        
        # Forward pass
        outputs = model(X_batch)
        loss = criterion(outputs.squeeze(), y_batch.float())
        
        # Backward pass et optimisation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
    if (epoch + 1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Pr√©diction sur le GPU
model.eval()
with torch.no_grad():
    outputs = model(X_tensor.to(device))
    predicted = (outputs.squeeze() > 0.5).int()
    accuracy = (predicted == y_tensor).float().mean()
    print(f'Accuracy on GPU: {accuracy:.4f}')
```

## IV. **Interop√©rabilit√© avec CPU/GPU**
- PyTorch, TensorFlow et d'autres librairies modernes permettent une transparence totale entre CPU et GPU. Vous pouvez facilement transf√©rer des tensors entre le CPU et le GPU √† l'aide de fonctions comme `.to('cuda')` pour transf√©rer vers GPU et `.to('cpu')` pour transf√©rer vers CPU.
- Cela permet d'utiliser un GPU pour les op√©rations intensives et de basculer vers le CPU pour les t√¢ches n√©cessitant moins de puissance de calcul.

## V. **Utilisation de `torch.cuda` pour les op√©rations sp√©cifiques au GPU**
- `torch.cuda.is_available()`: V√©rifie si CUDA est disponible sur le syst√®me.
- `torch.cuda.get_device_name(0)`: Retourne le nom du GPU disponible.
- `torch.cuda.memory_allocated(device)`: Retourne la m√©moire GPU actuellement allou√©e.
- `torch.cuda.empty_cache()`: Lib√®re la m√©moire GPU.